%% LyX 2.4.3 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[british]{article}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[LGR,T1]{fontenc}
\usepackage[utf8]{luainputenc}
\usepackage{color}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{graphicx}
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\newcommand*\LyXbar{\rule[0.585ex]{1.2em}{0.25pt}}
\DeclareRobustCommand{\greektext}{%
  \fontencoding{LGR}\selectfont\def\encodingdefault{LGR}}
\DeclareRobustCommand{\textgreek}[1]{\leavevmode{\greektext #1}}

%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxcode}
	{\par\begin{list}{}{
		\setlength{\rightmargin}{\leftmargin}
		\setlength{\listparindent}{0pt}% needed for AMS classes
		\raggedright
		\setlength{\itemsep}{0pt}
		\setlength{\parsep}{0pt}
		\normalfont\ttfamily}%
	 \item[]}
	{\end{list}}

\makeatother

\usepackage{babel}
\begin{document}
\title{Mathematical Analysis 1}
\date{10 June 2025}
\author{Giacomo}

\maketitle
\includegraphics[scale=0.5]{/home/guc/Pictures/Cover}

\newpage{}

\part*{Pre-Derivatives}

\section*{Theorems, functions and axioms}

``Calvin: You know, I don’t think math is a science, I think it’s
a religion.

Hobbes: A religion?

Calvin: Yeah. All these equations are like miracles. You take two
numbers and when you add them, they magically become one NEW number!
No one can say how it happens. You either believe it or you don’t.
{[}Pointing at his math book{]} This whole book is full of things
that have to be accepted on faith! It’s a religion!''

\subsection*{Logic}

\subsubsection*{Propositional Logic}

A proposition is a statement. Statements in math are either true or
false, if you combined multiple propositions there are multiple outcomes
depending on the validity of each proposition in the statement. I
call them compound statements (but I dont think its the actual name).

\paragraph*{Negation (Negazione):}

Represented by a $\lnot$. Inverts the value of a statement, e.g (Bob
went to the store), the opposite can be represented as $\lnot$(Bob
went to the store).

\paragraph*{Disjunction (Disgiunzione):}

Represented by a $\vee$. Statement is true if at least one of the
propositions is also true.

\paragraph*{Conjunction (Congiunzione):}

Represented by a $\wedge.$ Statement is true only if both propositions
are also true.

\paragraph*{Implication (Implicazione):}

Represented by a $\rightarrow$. Its formally defined as : $\neg P\vee Q$.

\paragraph*{Biconditional (Bicondizionale):}

Represented by a $\leftrightarrow$. Statement is true if both of
the propositions share the same value.

\subsubsection*{Predicate Logic}

\paragraph*{Universal:}

It is represented by $\forall$. ``For every ...''

\paragraph*{Existence:}

It is represented by $\exists$. ``There exists ...''

\subsubsection*{Axioms}

\paragraph*{Axioms (Assiomi):}

An axiom is a postulate, more commonly known as assumption. It is
a statement that is held as always true in regards to the problem
or proof needed to solve. There are different types of axioms which
are briefly stated below:

\subparagraph*{Logical Axioms:}

A universal truth in all of Mathematics applicable in both the Physics
Notes and the Linear Algebra Notes.

\subparagraph*{Non Logical Axioms:}

Domain specific assumptions, such as Axioms only applicable in $\mathbb{R}$(Gross
oversimplification)

\subsection*{Group Theory (Theoria degli insiemi)}

\subsubsection*{Definition of a Group}

A group is a set, that has the following requirements:

\paragraph*{Closure:}

For all $a,b\in G,a*b\in G$

\paragraph*{Associativity:}

$a,b,c\in G,(a*b)*c=a*(c*b)$

\paragraph*{Identity:}

Lets say e exists $e\in G$ such that $e*a=a*e=a$ for all $a\in G$ 

\paragraph*{Inverse:}

If there exists a a, such that $a\in G$ therefore there exists a
inverses

\subsection*{Proofs (Dimostrazione)}

In Math, every theorem and formula needs to be able to be proven in
a proof. There are multiple types of proofs which are used to show
that a theorem and or formula is valid. In these notes, very few proofs
will be done, however in the written blue book. Each proof I have
written in there has a classification. If a proof is referenced it
will have a corresponding code written next to it referencing the
written proof in the blue notebook. 

\paragraph*{Direct Proof (Dimostrazione Diretta).}

By using known definitions, axioms and theorems, a sequence of logical
steps can be used to directly demonstrate whether or not the statement
is correct.

\paragraph{Proof by Contradiction (Dimostrazione per Assurdo).}

Assume that a statement is false and connect it to a logical contradiction. 

\paragraph*{Induction (Induzione).}

Used for statements involving natural numbers

\subparagraph*{Base Case: }

Verify the statement holds for the initial value

\subparagraph*{Inductive Step: }

Assume it holds for a n=k, then prove it holds for n=k+1.

\paragraph*{Constructive Proof (Dimostrazione Costruttiva).}

Make a identity with the exact desired property. More formally `'Demonstrates
the existence of an object by explicitly constructing it'\/'

\subsection*{Functions}

A \textbf{function} (\textit{funzione}) is a very common conecept in math that formalizes the relationship between two sets by assigning each element of the first set to exactly one element of the second set. Formally, a function \( f: A \to B \) consists of:
\begin{itemize}
    \item A \textbf{domain} (\textit{dominio}) \( A \), the set of all possible inputs.
    \item A \textbf{codomain} (\textit{codominio}) \( B \), the set into which all outputs are mapped.
    \item A rule or correspondence that links each element \( x \in A \) to a unique element \( f(x) \in B \).
\end{itemize}

\paragraph*{Formal Definition}
A function \( f \) is a subset of the Cartesian product \( A \times B \) such that for every \( x \in A \), there exists exactly one \( y \in B \) where \( (x, y) \in f \). This is denoted as \( y = f(x) \).

\paragraph*{Key Properties}
\begin{enumerate}
    \item \textbf{Injectivity} (\textit{iniettiva}): A function is injective if distinct inputs map to distinct outputs:
    \[
        \forall x_1, x_2 \in A, \quad f(x_1) = f(x_2) \implies x_1 = x_2.
    \]
    \item \textbf{Surjectivity} (\textit{suriettiva}): A function is surjective if every element in \( B \) is an output for some input:
    \[
        \forall y \in B, \quad \exists x \in A \text{ such that } y = f(x).
    \]
    \item \textbf{Bijectivity} (\textit{biiettiva}): A function is bijective if it is both injective and surjective, establishing a one-to-one correspondence between \( A \) and \( B \).
\end{enumerate}

\subsection*{Natural Numbers}



The \textbf{natural numbers} (\textit{numeri naturali}) are the standard version of numbers in maths, used for counting and ordering. Formally, the set of natural numbers \( \mathbb{N} \) is defined as:
\[
\mathbb{N} = \{1, 2, 3, \ldots\} \quad \text{(sometimes including } 0 \text{ depending on context)}.
\]
They are characterized by their discrete, non-negative integer values and form the foundation for number theory and arithmetic.

\paragraph*{Formal Definition (Peano Axioms)}
The properties of natural numbers are axiomatically defined by the \textbf{Peano axioms} (\textit{assiomi di Peano}):
\begin{enumerate}
    \item \( 1 \) (or \( 0 \)) is a natural number.
    \item Every natural number \( n \) has a unique successor \( S(n) \), which is also a natural number.
    \item \( 1 \) (or \( 0 \)) is not the successor of any natural number.
    \item Distinct natural numbers have distinct successors: \( S(m) = S(n) \implies m = n \).
    \item \textbf{Induction}: If a property holds for \( 1 \) (or \( 0 \)) and holds for \( S(n) \) whenever it holds for \( n \), then it holds for all natural numbers.
\end{enumerate}

\paragraph*{Key Properties}
\begin{itemize}
    \item \textbf{Closure under addition and multiplication}: For all \( a, b \in \mathbb{N} \), \( a + b \in \mathbb{N} \) and \( a \cdot b \in \mathbb{N} \).
    \item \textbf{Non-closure under subtraction and division}: Subtraction \( a - b \) or division \( a / b \) may not result in a natural number.
    \item \textbf{Well-ordering principle} (\textit{principio del buon ordinamento}): Every non-empty subset of \( \mathbb{N} \) has a least element.
    \item \textbf{Infinite cardinality}: \( \mathbb{N} \) is countably infinite.
\end{itemize}

\paragraph*{Number Theory}
Natural numbers are central to number theory, which studies:
\begin{enumerate}
    \item \textbf{Prime numbers} (\textit{numeri primi}): Natural numbers \( > 1 \) with no divisors other than \( 1 \) and themselves:
    \[
        \mathbb{P} = \{2, 3, 5, 7, 11, \ldots\}.
    \]
    \item \textbf{Divisibility}: A number \( a \) divides \( b \) (\( a \mid b \)) if \( \exists k \in \mathbb{N} \) such that \( b = a \cdot k \).
    \item \textbf{Mathematical induction} (\textit{induzione matematica}): A proof technique leveraging the Peano axioms.
    \item \textbf{Modular arithmetic} (\textit{aritmetica modulare}): Operations on residues modulo \( n \), e.g., \( 7 \equiv 2 \mod 5 \).
\end{enumerate}


\subsection*{Whole Numbers}

\textbf{Whole Numbers} (\textit{numeri interi non negativi}) are an extension of the natural numbers that include zero, forming the set \( \mathbb{W} = \{0, 1, 2, 3, \ldots\} \). They are used for counting discrete objects and represent non-negative integers without fractions or decimals. 

\paragraph*{\textbf{Formal Definition}}  
The set \( \mathbb{W} \) satisfies the \textbf{Peano axioms} (\textit{assiomi di Peano}) with zero as the base element:
\begin{itemize}
    \item \( 0 \) is a whole number.
    \item Every whole number \( n \) has a unique successor \( S(n) \in \mathbb{W} \).
    \item \( 0 \) is not the successor of any whole number.
    \item Distinct numbers have distinct successors: \( S(a) = S(b) \implies a = b \).
    \item \textbf{Induction}: If a property holds for \( 0 \) and for \( S(n) \) whenever it holds for \( n \), it holds for all \( \mathbb{W} \).
\end{itemize}

\paragraph*{\textbf{Key Properties}}  
\begin{itemize}
    \item \textbf{Closure under addition and multiplication}: For \( a, b \in \mathbb{W} \), \( a + b \in \mathbb{W} \) and \( a \cdot b \in \mathbb{W} \).
    \item \textbf{Non-closure under subtraction}: \( a - b \in \mathbb{W} \) only if \( a \geq b \).
    \item \textbf{Additive identity}: \( 0 + a = a \) for all \( a \in \mathbb{W} \).
    \item \textbf{Well-ordering principle} (\textit{principio del buon ordinamento}): Every non-empty subset of \( \mathbb{W} \) has a least element.
\end{itemize}

\paragraph*{\textbf{Representation}}  
Whole numbers are represented in numeral systems such as:
\begin{itemize}
    \item \textbf{Decimal}: \( 0, 1, 2, \ldots \)
    \item \textbf{Binary}: \( 0_2 = 0_{10}, 1_2 = 1_{10}, 10_2 = 2_{10} \)
    \item \textbf{Unary}: \( 0 \) (often represented as an absence of marks), \( | = 1, || = 2 \).
\end{itemize}

\paragraph*{\textbf{Differences from Natural Numbers}}  
Unlike natural numbers (\textit{numeri naturali}), which sometimes exclude zero, whole numbers explicitly include \( 0 \). This makes \( \mathbb{W} \) the set \( \mathbb{N} \cup \{0\} \) in contexts where natural numbers start at \( 1 \).


\paragraph*{Below,}

is a image of all the relevant groups of numbers and how they are
related. Not all of them have been stated in this point in the notes,
but they are all relevant for analysis 1

\includegraphics[scale=0.2]{/home/guc/Pictures/NumberSetinC\lyxdot svg}

\subsection*{Exponential Properties}

\subsubsection*{Exponential Functions}
For $a > 0$ and $x \in \mathbb{R}$, the \emph{exponential function} with base $a$ is defined as the function $f(x) = a^x$. This represents continuous growth (when $a > 1$) or decay (when $0 < a < 1$) processes. The fundamental identity $a^x = e^{x \ln a}$ relates all exponentials to the natural base $e \approx 2.71828$, where $\exp(x) = e^x$ is the unique solution to $y' = y$ with $y(0)=1$. Exponentials map additive changes to multiplicative scaling: $a^{x+y} = a^x \cdot a^y$.

\subsubsection*{Exponential Properties}
For $a > 0$, $b > 0$, and $x,y \in \mathbb{R}$:
\begin{itemize}
\renewcommand{\labelitemi}{}
\setlength\itemsep{-0.5em}
\setlength\parsep{-0.5em}
    \item $a^x a^y = a^{x+y}$
    \item $\frac{a^x}{a^y} = a^{x-y}$
    \item $(a^x)^y = a^{xy}$
    \item $a^0 = 1$
    \item $a^{-x} = \frac{1}{a^x}$
    \item $(ab)^x = a^x b^x$
    \item $\left(\frac{a}{b}\right)^x = \frac{a^x}{b^x}$
\end{itemize}

\subsubsection*{Def of Logarithms}
For $a > 0$ ($a \neq 1$), $b > 0$:
\begin{itemize}
\renewcommand{\labelitemi}{}
    \item $\log_a b = x \iff a^x = b$
    \item $\log_a 1 = 0$
    \item $\log_a a = 1$
    \item $a^{\log_a b} = b$
\end{itemize}

\subsubsection*{Logarithm Properties}
For $a > 0$ ($a \neq 1$), $x,y > 0$, $k \in \mathbb{R}$:
\begin{itemize}
\renewcommand{\labelitemi}{}
    \item $\log_a(xy) = \log_a x + \log_a y$
    \item $\log_a\left(\frac{x}{y}\right) = \log_a x - \log_a y$
    \item $\log_a(x^k) = k\log_a x$
    \item $\log_a a^x = x$
\end{itemize}

\subsubsection*{Change of Base}
For $a,b > 0$ ($a,b \neq 1$), $c > 0$:
\[
\log_a c = \frac{\log_b c}{\log_b a}
\]
Special case: $\log_a b = \frac{1}{\log_b a}$

\subsubsection*{Natural Exponential and Logarithm}
\begin{itemize}
\renewcommand{\labelitemi}{}
    \item $e^x = \exp(x)$
    \item $\ln x = \log_e x$
    \item $e^{\ln x} = x$ for $x > 0$
    \item $\ln(e^x) = x$ for $x \in \mathbb{R}$
\end{itemize}

\subsubsection*{Exponential-Logarithmic Equations}
Key solving techniques:
\begin{itemize}
\renewcommand{\labelitemi}{}
    \item If $a^x = a^y$ then $x = y$
    \item If $\log_a x = \log_a y$ then $x = y$
    \item To solve $a^{f(x)} = b$: take logarithms of both sides
    \item To solve $\log_a f(x) = b$: rewrite as $f(x) = a^b$
\end{itemize}

\subsection*{Trigonometric Functions}

\textbf{Trigonometric Functions} (\textit{funzioni trigonometriche}) are periodic functions that relate angles in a right triangle or on the unit circle to ratios of side lengths. The primary trigonometric functions are sine (\(\sin\)), cosine (\(\cos\)), tangent (\(\tan\)), and their reciprocals: cosecant (\(\csc\)), secant (\(\sec\)), and cotangent (\(\cot\)).

\paragraph*{\textbf{Formal Definition (Unit Circle)}}  
For an angle \(\theta\) measured counterclockwise from the positive \(x\)-axis on the unit circle (\(x^2 + y^2 = 1\)):
\[
\sin\theta = y, \quad \cos\theta = x, \quad \tan\theta = \frac{y}{x} \quad (x \neq 0).
\]
The reciprocals are defined as:
\[
\csc\theta = \frac{1}{\sin\theta}, \quad \sec\theta = \frac{1}{\cos\theta}, \quad \cot\theta = \frac{x}{y} \quad (y \neq 0).
\]

\paragraph*{\textbf{Key Properties}}  
\begin{itemize}
    \item \textbf{Periodicity} (\textit{periodicità}): \(\sin\theta\) and \(\cos\theta\) have period \(2\pi\); \(\tan\theta\) and \(\cot\theta\) have period \(\pi\).
    \item \textbf{Range}: 
    \[
    \sin\theta, \cos\theta \in [-1, 1]; \quad \tan\theta \in \mathbb{R} \text{ (excluding asymptotes)}.
    \]
    \item \textbf{Parity}: \(\sin\theta\) and \(\tan\theta\) are odd functions; \(\cos\theta\) is even:
    \[
    \sin(-\theta) = -\sin\theta, \quad \cos(-\theta) = \cos\theta.
    \]
    \item \textbf{Pythagorean Identity} (\textit{identità pitagorica}):
    \[
    \sin^2\theta + \cos^2\theta = 1.
    \]
\end{itemize}

\paragraph*{\textbf{Differences from Other Functions}}  
Unlike polynomial or exponential functions, trigonometric functions:
\begin{itemize}
    \item Are periodic and bounded (except \(\tan\theta\) and \(\cot\theta\)).
    \item Model oscillatory behavior (e.g., waves, circular motion).
    \item Require angular input (radians or degrees) rather than purely scalar quantities.
\end{itemize}

\paragraph*{Unit Circle For Reference:}

\includegraphics[scale=0.3]{pasted1}

\paragraph*{\textbf{Fundamental Identities (\textit{identità fondamentali})}}
\begin{itemize}
    \item \textbf{Reciprocal Relations}:
    \[
    \tan\theta = \frac{\sin\theta}{\cos\theta}, \quad \cot\theta = \frac{\cos\theta}{\sin\theta}, \quad \sec\theta = \frac{1}{\cos\theta}, \quad \csc\theta = \frac{1}{\sin\theta}.
    \]
    \item \textbf{Extended Pythagorean Identities}:
    \[
    1 + \tan^2\theta = \sec^2\theta, \quad 1 + \cot^2\theta = \csc^2\theta.
    \]
    \item \textbf{Co-Function Identities} (\textit{identità complementari}):
    \[
    \sin\left(\frac{\pi}{2} - \theta\right) = \cos\theta, \quad \cos\left(\frac{\pi}{2} - \theta\right) = \sin\theta, \quad \tan\left(\frac{\pi}{2} - \theta\right) = \cot\theta.
    \]
\end{itemize}

\paragraph*{\textbf{Angle Addition \& Subtraction}}  
For any angles \(\alpha\) and \(\beta\):
\begin{itemize}
    \item \(\sin(\alpha \pm \beta) = \sin\alpha\cos\beta \pm \cos\alpha\sin\beta\)
    \item \(\cos(\alpha \pm \beta) = \cos\alpha\cos\beta \mp \sin\alpha\sin\beta\)
    \item \(\tan(\alpha \pm \beta) = \frac{\tan\alpha \pm \tan\beta}{1 \mp \tan\alpha\tan\beta}\)
\end{itemize}

\paragraph*{\textbf{Multiple-Angle \& Half-Angle Identities}}  
\begin{itemize}
    \item \textbf{Double-Angle}:
    \[
    \sin(2\theta) = 2\sin\theta\cos\theta, \quad \cos(2\theta) = \cos^2\theta - \sin^2\theta = 2\cos^2\theta - 1 = 1 - 2\sin^2\theta.
    \]
    \item \textbf{Triple-Angle}:
    \[
    \sin(3\theta) = 3\sin\theta - 4\sin^3\theta, \quad \cos(3\theta) = 4\cos^3\theta - 3\cos\theta.
    \]
    \item \textbf{Half-Angle} (sign depends on quadrant):
    \[
    \sin\left(\frac{\theta}{2}\right) = \pm\sqrt{\frac{1 - \cos\theta}{2}}, \quad \cos\left(\frac{\theta}{2}\right) = \pm\sqrt{\frac{1 + \cos\theta}{2}}.
    \]
\end{itemize}

\paragraph*{\textbf{Product-to-Sum \& Sum-to-Product}}  
\begin{itemize}
    \item \textbf{Product-to-Sum}:
    \[
    \sin\alpha\sin\beta = \frac{1}{2}[\cos(\alpha - \beta) - \cos(\alpha + \beta)], \quad \cos\alpha\cos\beta = \frac{1}{2}[\cos(\alpha - \beta) + \cos(\alpha + \beta)].
    \]
    \item \textbf{Sum-to-Product}:
    \[
    \sin\alpha \pm \sin\beta = 2\sin\left(\frac{\alpha \pm \beta}{2}\right)\cos\left(\frac{\alpha \mp \beta}{2}\right), \quad \cos\alpha + \cos\beta = 2\cos\left(\frac{\alpha + \beta}{2}\right)\cos\left(\frac{\alpha - \beta}{2}\right).
    \]
\end{itemize}

\paragraph*{\textbf{Triangle Relations (Laws)}}  
For any triangle with sides \(a, b, c\) opposite angles \(A, B, C\):
\begin{itemize}
    \item \textbf{Law of Sines}:
    \[
    \frac{a}{\sin A} = \frac{b}{\sin B} = \frac{c}{\sin C} = 2R \quad (R = \text{circumradius}).
    \]
    \item \textbf{Law of Cosines}:
    \[
    c^2 = a^2 + b^2 - 2ab\cos C.
    \]
    \item \textbf{Law of Tangents}:
    \[
    \frac{a - b}{a + b} = \frac{\tan\left(\frac{A - B}{2}\right)}{\tan\left(\frac{A + B}{2}\right)}.
    \]
\end{itemize}

\subsection*{Complex Numbers introduction}

The largest domain covered in these notes: $\mathbb{C}$. Complex
numbers are an extension of real numbers, defined by the imaginary
number i With this strange property where $i^{2}=-1$. They are used
to resolve polynomial equations unsolvable in real numbers, as shown
in the fundamental theorem of algebra (Might need a section on this).
Below is a complex number with iy as the imaginary component and x
as the real component.

\begin{equation}
z=x+iy
\end{equation}
 Imagine $\mathbb{R}$ covering the whole x axis, and $\mathbb{C}$
covering the whole y axis, that's the complex plane.

\subsubsection*{Operations}

Addition

\begin{equation}
z_{1}+z_{2}=(x_{1}+x_{2})+i(y_{1}+y_{2})
\end{equation}
 Subtraction

\begin{equation}
z_{1}-z_{2}=(x_{1}-x_{2})+i(y_{1}-y_{2})
\end{equation}
 Multiplication

\begin{equation}
z_{1}z_{2}=(x_{1}+iy_{1})(x_{2}+iy_{2})=(x_{1}x_{2}-y_{1}y_{2})+i(x_{1}y_{2}+x_{2}y_{1})
\end{equation}
 Complex Conjugate

\begin{equation}
\bar{z}=x-iy
\end{equation}
 Modulus

\begin{equation}
|z|=\sqrt{x^{2}+y^{2}}
\end{equation}
 Inverse

\begin{equation}
z^{-1}=\frac{\bar{z}}{|z|^{2}}=\frac{x-iy}{x^{2}+y^{2}}
\end{equation}

A image of the $\mathbb{C}$ plane for reference, showing the inverse
modulus as well:

\includegraphics[scale=0.2]{/home/guc/Pictures/Complex_conjugate_picture\lyxdot svg}

\subsection*{Polar Coordinates}

Polar coordinates are a alternative to the commonly used Cartesian
coordinate system (x,y). It is mesured using two metrics:

\subparagraph*{Radial Distance (r):}

Which is the distance from the origin which is also the hypotenuse
of the triangle.

\subparagraph*{Angular Coordinate ($\theta$):}

Which is the angle between the radial distance and the +x axis. Which
increases counterclockwise. 

\paragraph*{Basic Conversion between Cartesian and Polar}

For Cartesian to Polar it is:

\[
r=\sqrt{x^{2}+y^{2}},\theta=arctan\left(\frac{y}{x}\right)
\]
 And for Polar to Cartesian it is:

\[
x=rcos\theta,y=rsin\theta
\]


\paragraph*{Basis vectors}

Basis vectors represented in polar coordinates:

\[
\mathbf{\hat{\mathbf{r}}=}cos\theta\hat{\mathbf{i}}+sin\theta\hat{\mathbf{j}},\hat{\mathbf{\theta}}=-sin\theta\hat{\mathbf{i}}+cos\theta\hat{\mathbf{j}}
\]


\paragraph*{Polar Functions}

A polar function is represented as:

\[
r=f(\theta)
\]
 Much like a regular function it has a domain and a range, expressed
below:

Domain: $\theta\in[0,2\pi)$

Range: $r\in\mathbb{R}$

\paragraph*{Common set of Polar Functions:}

-

\includegraphics[scale=0.3]{pasted3}

\includegraphics[scale=0.3]{pasted4}

\includegraphics[scale=0.3]{pasted2}

\pagebreak{}

\section*{Sums and Sequences}

\textquotedbl La situazione è grave ma non è seria.\textquotedbl{}

\subsection*{Limits}

\paragraph*{How its represented.}

Any sequence that converges to a limit is represented as:

\[
lim_{n\rightarrow\infty}a_{n}=L
\]

Where $\left\{ a_{n}\right\} $is the sequence.

\paragraph*{Formal Definition.}

\begin{equation*}
    \lim_{n \to \infty} a_n = L \quad \iff \quad
    \forall \varepsilon > 0,\; \exists N \in \mathbb{N},\; \forall n \geq N,\; |a_n - L| < \varepsilon
\end{equation*}

``For every positive number, there exists a natural number such that,
for all integers, the distance between and L is less than \textgreek{ε}.''

\subsection*{Sequence}

\paragraph{Definition:}

A sequence/sucession is simply a list of $\mathbb{N}$ while following
a set of rules defined by a function. The function then maps each
$\mathbb{N}$ to a corresponding $\mathbb{R}$ following the rules
defined by the function. 

\paragraph*{Representation:}

It is often shown as a random letter (this case a) $a_{n}$ with n
representing the number of the term.

\subparagraph*{The first term, }

$a_{1}$ is called the initial term (termine iniziale)

\subparagraph*{The terms after the first,}

$a_{1+n}$ is called the recursive formula (formula ricorsiva)

\paragraph*{Types:}

There are 2 specific categories which will be covered in more detail.
1. Whether its bounded (limitata) or unbounded (illimitata). 2. Whether
its convergent (convergente), divergent (divergente) or oscillatory
(oscillante).

\subsection*{Bounded Successions (Successioni Limitate)}

\paragraph*{Definition:}

A bounded sequence (successione limitata), is a sequence $a_{n}$
that exists within a range such that if $b\in\mathbb{R}$, b is greater
than $a_{n}$, and there exists $c\in\mathbb{R}$ that is less than
$a_{n}$, it is a bounded sequence. A bounded sequence is may suggest
convergences and can be proven by using the Bolzano-Weierstrass Theorem.

\paragraph*{Intervals:}

\subparagraph*{Open Interval}

\[
(a,b)\coloneqq\left\{ x\in\mathbb{R}|a<x<b\right\} 
\]

A open interval includes all $\mathbb{R}$ numbers between a and b,
excluding the endpoints. Represented by a () and <

\subparagraph*{Closed Interval }

\[
[a,b]\coloneqq\left\{ x\in\mathbb{R}|a\leq x\leq b\right\} 
\]

A closed interval includes all $\mathbb{R}$ numbers between a and
b, including the endpoints. Represented by a {[}{]} and $\leq$. 

\subparagraph*{Empty Interval}

Denoted as $\emptyset$, it contains no numbers.

\subparagraph*{Degenerate Interval}

A single point, $[a,a]=\left\{ a\right\} $. By technicality it is
always closed.

\subparagraph*{Half Intervals}

It is possible to have intervals which are open at one end and closed
at the other, and vice versa. e.g

\[
(a,b]\coloneqq\left\{ x\in\mathbb{R}|a<x\leq b\right\} 
\]


\subparagraph*{Infinite Intervals}

There are also intervals which are infinite on one side and open or
closed on the other. e.g

\[
(a,+\infty)\coloneqq\left\{ x\in\mathbb{R}|a<x\right\} 
\]

The infinite can be negative as well on the other side. (Not sure
if the infinite has to be in a open interval, because I have not seen
any which are not in a open interval)

\paragraph*{Types of bounds: }

\subparagraph*{Upper bound:}

If a upper bound exists we call it bounded from above.

\subparagraph*{Lower bound:}

If a lower bound exists we call it bounded from below.

If both upper and lower bound exist the set is bounded.

\subparagraph*{Bound properties:}

There can be multiple upper and lower bound (As clearly shown in the
diagram below). 

\paragraph*{Supremum and Infimum}

The supremum and infimum can only exist for a interval with at least
one open point. Its the smallest possible upper bound (If its a supremum)
or the largest possible lower bound (If its a infimum). It can NEVER
reach the interval. The Supermum can be written as $supM$ and the
Infium can be written as $infM$

\paragraph*{Minimum and Maximum}

For a Minimum or a Maximum you must have at least one closed interval
point. (as shown in the diagram below). The minimum or maximum is
the point a or b that hold the interval. 

\paragraph*{Diagram of open and closed sequences:}

\includegraphics[scale=0.4]{/home/guc/Pictures/boundedsequences}

\subsection*{Monotone Sequences }

\paragraph*{Definition:}

A sequence that is monotone is either non-decreasing or non-increasing.
Therefore, by extention a constant sequence is simultaneously non-decreasing
and non-increasing. Therefore it is monotone. The strictly increasing/decreasing
are simply subsets.

\paragraph*{Growth of Sequences:}

\subparagraph*{Non-decreasing sequence (successioni crescenti):}

Is a sequence that increases if each term is greater than or equal
to the previous term:
\[
\forall n\in\mathbb{N},a_{n+1}\geq a_{n}
\]


\subparagraph*{Non-increasing sequences (successioni decrescenti):}

Is a sequence that it decreases if each term is less than or equal
to the previous term.
\[
\forall n\in\mathbb{N},a_{n+1}\leq a_{n}
\]


\subparagraph*{Strictly increasing sequences (successioni strettamente crescenti):}

Is a sequence that strictly increasing if each term is strictly greater
than the previous term.
\[
\forall n\in\mathbb{N},a_{n+1}>a_{n}
\]


\subparagraph*{Strictly decreasing sequences (successioni strettamente decrescenti):}

Is a sequence that strictly decreasing if each term is strictly less
than the previous term.
\[
\forall n\in\mathbb{N},a_{n+1}<a_{n}
\]


\subsection*{Convergent and Divergent Sequences}

\subsubsection*{Convergent}

A sequence is convergent if it approaches a finite limit (L) as the
n approaches infinity. Formally this can be defined:

A sequence $\left\{ a_{n}\right\} $ converges to L if 

\[
\forall\varepsilon>0,\exists N\in\mathbb{N}|\forall n\geq N,|a_{n}-L|<\varepsilon
\]


\paragraph*{Accumulation Points Link}

A convergent sequence has exactly one accumulation point which is
its limit. This is different from the divergent sequence that will
have multiple accumulation values or many improper ones.

\paragraph*{Limit sup and inf}

For convergent sequences:

\[
lim_{n\rightarrow\infty}supa_{n}=lim_{n\rightarrow\infty}infa_{n}=L
\]


\paragraph*{Note on Bounded Sequences:}

\uline{While boundedness is a requirement for a sequence to be convergent
it is not the only requirement.}

\paragraph*{Monotone Convergence Theorem:}

Every bounded monotonic sequence converges

\paragraph*{Limit properties for convergence:}

If b is a sequence that converges to M and a is a sequence that converges
to L then
\[
lim_{n\rightarrow\infty}(a_{n}\pm b_{n})=L\pm M
\]
\[
lim_{n\rightarrow\infty}(a_{n}\times b_{n})=L\times M
\]
\[
lim_{n\rightarrow\infty}\frac{a_{n}}{b_{n}}=\frac{L}{M}(M\neq0)
\]


\subsubsection*{Divergent}

A sequence is divergent if it does not converge to any limit L. There
are two primary times of divergence, divergence to infinity(may also
be refered to improper divergence) or oscillatory divergence. The
formal definitions for divergence to infinity is listed below:

\[
\forall M\in\mathbb{R},\exists N\in\mathbb{N}
\]

Such that $\forall n\geq N,a_{n}>M$, and if it diverging to negative
infinity it is $\forall n\geq N,a_{n}<M$

And for Oscillations:

\subparagraph*{Bounded Oscillations:}

Terms change between bounds with the lim sum not equalling the lim
inf

\subparagraph*{Unbounded oscillation:}

Terms grow wile changing and slowly diverging in absolute value but
don't have a directional convergence.

\paragraph*{Unbounded}

While it is not guaranteed that every bounded sequence is convergent,
it is guaranteed that every unbounded sequence is divergent by definition.
However not all divergent sequences are unbounded. 

\paragraph*{Sub sequences}

If a sub sequence of a sequence diverges to infinity then the also
sequence will diverge.

If two sub sequences converge to different limits, the original sequence
diverges.

\paragraph*{Limit properties}

If $lim_{n\rightarrow\infty}a_{n}=+\infty$ and the sequence b is
bounded below then

\[
lim_{n\rightarrow\infty}(a_{n}+b_{n})=+\infty
\]
If $lim_{n\rightarrow\infty}a_{n}=+\infty$ and $lim_{n\rightarrow\infty}b_{n}=c>0$
then

\[
lim_{n\rightarrow\infty}(a_{n}\times b_{n})=+\infty
\]


\subsection*{Cauchy Sequence}

A Cauchy sequence is a sequence whose terms become arbitrarily close
to one another as the sequence progresses, regardless of whether the
sequence converges to a specific limit.

\paragraph{Formal Definition}

Every sequence with this definition is a Cauchy Sequence

\[
\forall\epsilon>0,\exists N\in\mathbb{N}\Rightarrow\forall m,n\geq N,|a_{m}-a_{n}|<\epsilon
\]


\paragraph*{Properties}

\subparagraph{Convergence}

In R every Cauchy sequence by definition must be convergent, due to
the completeness property. 

\subparagraph*{Boundedness}

Every Cauchy Sequence is bounded.

\subparagraph*{Subsequence}

If a Cauchy sequence has a convergent subsequence, the entire sequence
converges to the same limit. 

\subparagraph*{Notes on convergence}

All convergent sequences are Cauchy, not all Cauchy sequences are
convergent IN INCOMPLETE SPACES

\subsection*{Napier's Constant (Costante di Nepero)}

This may also be known as e or exponential or exp. It has 3 separate
definitions: Limit, Series, and integral. However I will only do the
explanation for limit definition here. The integral explanation will
be done in the integral chapter.

\paragraph*{Limit Definition:}

\begin{equation}
e=lim_{n\rightarrow\infty}\left(1+\frac{1}{n}\right)^{n}
\end{equation}

\subparagraph*{Base of the Natural Logarithm} 
\( e \) is the base of the natural logarithm, denoted as \( \ln(x) \).

\subparagraph*{Approximate Value} 
\( e \approx 2.71828 \).

\subparagraph*{Irrational Number} 
\( e \) cannot be expressed as a ratio of two integers.

\subparagraph*{Transcendental Number} 
\( e \) is not a root of any non-zero polynomial with rational coefficients.

\subparagraph*{Limit Definition} 
\[
e = \lim_{n \to \infty} \left(1 + \frac{1}{n}\right)^n
\]

\subparagraph*{Euler's Identity} 
\[
e^{i\pi} + 1 = 0
\]
Connecting \( e \), imaginary numbers, and \( \pi \)

\subsubsection*{Complex Exponential (l'esponenziale Complesso)}

The complex exponential function denoted as $e^{z}$ or exp(z). The
vast majority of definitions of complex exponential are the exact
same as the regular exponential function. Including the proof of their
equivalence. There are several ways to define a complex exponential
function, and I will list them below.

\subparagraph*{Power series expansion}

\paragraph*{
\[
e^{z}=\sum_{k=0}^{\infty}\frac{z^{k}}{k!}
\]
}

\subparagraph*{Limit Definition}

\[
e^{z}=lim_{n\rightarrow\infty}\left(1+\frac{z}{n}\right)^{n}
\]


\subparagraph*{Additive Property (Kinda definition)}

\[
e^{w+z}=e^{w}e^{z}
\]


\subparagraph*{Complex log}
\begin{lyxcode}
\[
log(e^{z})=\left\{ z+2\pi ik|k\in\mathbb{Z}\right\} 
\]
\end{lyxcode}

\paragraph*{Key Properties}

\subparagraph*{Non zero}

$\frac{1}{e^{z}}=e^{-z}$ and e\textasciicircum z does not equal
zero for all C

\subparagraph*{Periodic
\[
e^{z+2\pi}=e^{z}
\]
}

\subparagraph*{Identita di Euler}

For $e^{i\theta}=cos\theta+i*sin\theta$

\[
e^{i\pi}=-1
\]


\subparagraph*{Conjugate}

$\overline{e^{z}}=e^{\overline{z}}$

\subparagraph*{Modulus}

$|e^{z}|=e^{\mathfrak{\mathbb{R}}(z)}$

\subsection*{\textcolor{black}{Handling Infinity}}

\subsubsection*{\textcolor{black}{Limit Superior}}

\textcolor{black}{For a sequence $(a_{n})$, the limit superior which
is written as }

\textcolor{black}{
\[
limsup_{n\rightarrow\infty}a_{n}
\]
 is the supremum of all the accumulation points (cluster points) of
the sequence. For example if a sequence diverges to infinity its only
accumulation point is infinity therefore }

\textcolor{black}{
\[
limsup_{n\rightarrow\infty}a_{n}=\infty
\]
 The main diffrence between the supremum is that the supremum }

\textcolor{black}{Lets say we have a sequence $(a_{n})$ which is
given by $a_{n}=n$, the limit superior of $(a_{n})$ is the largest
accumulation value of $(a_{n})$. Whether it is improper or proper
accumulation value. It is represented as:}

\textcolor{black}{
\[
a=limsup_{n\rightarrow\infty}a_{n}
\]
}

\textcolor{black}{The difference between the limit superior and superior
is that the limit superior is ALWAYS smaller than the }

\paragraph*{\textcolor{black}{Limit Inferior }}

\textcolor{black}{The limit inferior of $(a_{n})$ is the smallest
improper accumulation value of $(a_{n})$, and its represented by
the notation}

\textcolor{black}{
\[
a=liminf_{n\rightarrow\infty}a_{n}
\]
}

\paragraph*{\textcolor{black}{Accumulation Values and Divergence}}

\textcolor{black}{A value $a\in\mathbb{R}\cup\left\{ +\infty,-\infty\right\} $
is called a accumulation value of $a_{n}$, if there exists a sub
sequence $a_{n_{k}}$ of $a_{n}$ such that the limit $k\rightarrow\infty|a_{n_{k}}=a$}

\textcolor{black}{For improper cases:}

\textcolor{black}{+$\infty$ is a accumilation value if the sequences
is unbounded above, meaning for every M$\in$R infinit}

\paragraph*{\textcolor{black}{Improper accumulation values}}

\textcolor{black}{Any sequence that has no accumulation values has
at least one improper accumulation value}

\subsection*{Bolzano-Weierstrass Theorem (Teorema di Bolzano-Weierstrass)}

% BOLZANO-WEIERSTRASS THEOREM
\begin{itemize}
    \item \textbf{Statement (Teorema di Bolzano-Weierstrass):} Every bounded sequence has a convergent subsequence. Formally:
    \[
    \left( \exists K > 0 \text{ such that } \forall n \in \mathbb{N},\, |a_n| \leq K \right) \implies \exists\, \{a_{n_k}\} \subseteq \{a_n\} \text{ and } \exists L \in \mathbb{R} \text{ such that } \lim_{k \to \infty} a_{n_k} = L
    \]
    
    \item \textbf{Key requirements:}
    \begin{itemize}
        \item Bounded sequence ($\exists K > 0 : \forall n \in \mathbb{N},\, |a_n| \leq K$)
    \end{itemize}
    
    \item \textbf{Consequence:} Applies in $\mathbb{R}^n$ (via metodo degli intervalli incapsulati or Heine-Borel)
\end{itemize}

\subsection*{Theorem of Zero}

\subsection*{Trigonometric Functions and $\pi$}

\subsection*{Complex Polynomials}

\subsubsection*{Fundamental Theorem of Algebra}

\pagebreak{}

\section*{Series}

`'Its better to ask for forgiveness than to ask for permission'\/'
- Wise person

\subsection*{Series (Serie)}

A series is the sum of the all the terms in a sequence. Formally,
if a is a sequence and we want the infinite sum of the series:

\[
\sum_{n=1}^{\infty}a_{n}=a_{1}+a_{2}+a_{3}+...
\]


\subsubsection*{Partial Sum (Somma Parziale)}

A series does not have to necessarily have to be the sum of infinite
terms. A partial sum, allows just the sum of the k terms:

\[
S_{k}=\sum_{n=1}^{k}a_{n}
\]

If $\sum a_{n}$ converges, then the lim

This series converges to S if lim$_{k\rightarrow\infty}S_{k}=S$ otherwise,
as mentioned in previous sections it diverges.

\subsubsection*{Necessary Condition for Convergence (Divergence Test)}
If \(\sum a_n\) converges, then \(\lim_{n \to \infty} a_n = 0\). \\
Equivalently, if \(\lim_{n \to \infty} a_n \neq 0\) (or the limit does not exist), the series diverges.

\subsection*{Types of Series}

\subsubsection*{Geometric (Serie Geometrica)}

It is a series with the standard form $\sum_{n=0}^{\infty}ar^{n}$
where r is the common ratio. There are 3 primary types of geometric
series:

\subparagraph*{Partial sum (Finite geometric series)}

For a finite number k, the partial sum 
\[
S_{k}=\sum_{n=0}^{n-1}ar^{n}=a+ar+ar^{2}+...+ar^{n-1}
\]

Formula
\[
S_{k}=a\times\frac{1-r^{n}}{1-r}
\]

valid only if $r\cancel{=}1$. If r=1 then

\[
S_{n}=a\times n
\]

There is also a alternate notation for the partial sum formula that
is pretty commonly used.

\[
S_{n+1}=\sum_{k=0}^{n}ar^{k}=\frac{a(1-r^{n+1})}{1-r}
\]


\subparagraph*{Infinite geometric series}

Theorem:

if $\sum_{n=0}^{\infty}ar^{n}$ converges then $|r|<1$ and
\[
\sum_{n=0}^{\infty}ar^{n}=\frac{a}{1-r}
\]


\subparagraph*{Divergence}

The series is divergent if $|r|\geq1,$ and the terms do not approach
zero

\subsubsection*{Telescoping (Serie Telescopica)}

A telescopic series is a series where a lot of the terms cancel out
when written in partial sums
\[
\sum_{n=1}^{\infty}(a_{n}-a_{n+1})
\]

whose partial sum will telescope to
\[
S_{k}=\sum_{n=1}^{k}(a_{n}-a_{n+1})=a_{1}-a_{k+1}
\]

When handling a limit to L
\[
\sum_{n=1}^{\infty}(a_{n}-a_{n+1})=a_{1}-L
\]

\textbf{Note:} This requires \(\lim_{n \to \infty} a_n = L\). \\
\textbf{Example:} \(\sum_{n=1}^{\infty} \frac{1}{n(n+1)} = \sum_{n=1}^{\infty} \left( \frac{1}{n} - \frac{1}{n+1} \right)\) converges to 1.

\subsubsection*{Harmonic Series (Serie armonica)}

A harmonic series is a series that diverges despite its terms tending
to zero, it is represented as:

\[
\sum_{n=1}^{\infty}\frac{1}{n}
\]

A small proof is listed below:

\[
1+\frac{1}{2}+\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+...+\frac{1}{8}\right)+...\geq1+\frac{1}{2}+2\left(\frac{1}{4}\right)+4\left(\frac{1}{8}\right)+...=\sum_{k=0}^{\infty}\frac{1}{2}=\infty
\]

This shows that $a_{n}\rightarrow0$ is necessary for convergence,
but not sufficient to prove it.

\paragraph*{P series}

Harmonic series are a type of p series. P series is often used in
comparison to check if series converge or not. It is represented as

\[
\sum_{n=1}^{\infty}\frac{1}{n^{p}}
\]
 where it is said that if \(p \leq 1\) the series diverges, and if \(p > 1\) the series \textbf{converges}.

\subsubsection*{Associativity of series (Associatività della somma di serie)}

The associativity of series, or the property that grouping terms in
different ways does not affect the sum, holds for convergent series
but not necessarily for divergent ones. 

\subsubsection*{Series with varying signs (Serie a segno variabile)}

% POWER SERIES
\paragraph*{\textbf{Power Series (Serie di potenze)}}
\begin{itemize}
    \item \textbf{Definition:} A series of the form:
    \[
    \sum_{n=0}^{\infty} c_n (x - a)^n = c_0 + c_1(x-a) + c_2(x-a)^2 + \cdots
    \]
    where $c_n$ are coefficients, $a$ is the center, and $x$ is the variable.
    
    \item \textbf{Radius of Convergence (R):} Determined by:
    \[
    R = \frac{1}{L} \quad \text{where} \quad L = \limsup_{n \to \infty} \sqrt[n]{|c_n|}
    \]
    (Cauchy-Hadamard formula) or when it exists:
    \[
    R = \lim_{n \to \infty} \left| \frac{c_n}{c_{n+1}} \right|
    \]
    
    \item \textbf{Interval of Convergence:} The set $x \in (a - R, a + R)$ where:
    \begin{itemize}
        \item Converges absolutely for $|x - a| < R$
        \item May converge or diverge at endpoints $x = a \pm R$
        \item Diverges for $|x - a| > R$
    \end{itemize}
    
    \item \textbf{Properties:}
    \begin{itemize}
        \item \textbf{Continuity:} Continuous on $(a - R, a + R)$
        \item \textbf{Differentiation:} Can differentiate term-by-term:
        \[
        f'(x) = \sum_{n=1}^{\infty} n c_n (x - a)^{n-1}
        \]
        with same radius $R$
        \item \textbf{Integration:} Can integrate term-by-term:
        \[
        \int f(x)  dx = C + \sum_{n=0}^{\infty} \frac{c_n}{n+1} (x - a)^{n+1}
        \]
        with same radius $R$
    \end{itemize}
    
    \item \textbf{Examples:}
    \begin{itemize}
        \item Exponential: $\displaystyle e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} \quad (R = \infty)$
        \item Geometric: $\displaystyle \frac{1}{1-x} = \sum_{n=0}^{\infty} x^n \quad (R = 1)$
        \item Arctangent: $\displaystyle \arctan x = \sum_{n=0}^{\infty} (-1)^n \frac{x^{2n+1}}{2n+1} \quad (R = 1)$
    \end{itemize}
    
    \item \textbf{Important Theorem:} If $\sum c_n (x-a)^n = 0$ for all $x$ in some interval, then $c_n = 0$ for all $n$.
\end{itemize}

% LINEARITY OF SERIES SUM
\paragraph*{\textbf{Linearity of Series Sum (Teorema linearità somma)}}
\begin{itemize}
    \item \textbf{Statement:} Given two convergent series $\sum_{n=1}^{\infty} a_n = A$ and $\sum_{n=1}^{\infty} b_n = B$, and constants $\alpha, \beta \in \mathbb{R}$, then:
    \[
    \sum_{n=1}^{\infty} (\alpha a_n + \beta b_n) = \alpha A + \beta B
    \]
    
    \item \textbf{Proof Outline:}
    \begin{itemize}
        \item Let $s_N = \sum_{n=1}^{N} a_n \to A$ and $t_N = \sum_{n=1}^{N} b_n \to B$
        \item The partial sum of the combined series:
        \[
        \sigma_N = \sum_{n=1}^{N} (\alpha a_n + \beta b_n) = \alpha s_N + \beta t_N
        \]
        \item By limit laws:
        \[
        \lim_{N \to \infty} \sigma_N = \alpha \lim_{N \to \infty} s_N + \beta \lim_{N \to \infty} t_N = \alpha A + \beta B
        \]
    \end{itemize}
    
    \item \textbf{Key Requirements:}
    \begin{itemize}
        \item Both series must converge individually
        \item Linearity fails for divergent series (counterexample: $a_n = (-1)^n$, $b_n = (-1)^{n+1}$)
    \end{itemize}
    
    \item \textbf{Extension:} For finitely many convergent series $\sum a_n^{(k)} = S_k$ and constants $c_k$:
    \[
    \sum_{n=1}^{\infty} \left( \sum_{k=1}^{m} c_k a_n^{(k)} \right) = \sum_{k=1}^{m} c_k S_k
    \]
    
    \item \textbf{Warning:} Does \emph{not} apply to conditionally convergent series rearrangements (Riemann series theorem).
\end{itemize}

\paragraph*{\textbf{Cesàro Summability (Convergenza alla Cesàro)}}
\begin{itemize}
    \item \textbf{Definition:} A sequence $(a_n)$ is Cesàro summable to limit $L$ if the arithmetic mean of its first $n$ partial sums converges to $L$:
    \[
    \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n s_k = L
    \]
    where $s_k = a_1 + a_2 + \cdots + a_k$ are the partial sums.
    
    \item \textbf{Equivalent Form:} For the sequence $(a_n)$ itself:
    \[
    \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n a_k = L
    \]
    \item \textbf{Key Properties:}
    \begin{itemize}
        \item \textbf{Regularity:} If $\lim_{n \to \infty} a_n = L$ exists, then $(a_n)$ is Cesàro summable to $L$
        \item \textbf{Strictly Weaker:} Cesàro summability doesn't imply convergence (e.g., $a_n = (-1)^n$)
        \item \textbf{Linearity:} If $a_n \to_{\text{C}} A$ and $b_n \to_{\text{C}} B$, then:
        \[
        \alpha a_n + \beta b_n \to_{\text{C}} \alpha A + \beta B
        \]
    \end{itemize}
    
    \item \textbf{Example (Divergent Sequence):} Consider $a_n = (-1)^{n+1}$:
    \[
    \text{Sequence: } 1, -1, 1, -1, 1, -1, \dots
    \]
    \[
    \text{Partial sums: } s_n = \begin{cases} 
    1 & n \text{ odd} \\
    0 & n \text{ even}
    \end{cases}
    \]
    \[
    \text{Cesàro mean: } \sigma_n = \frac{1}{n} \sum_{k=1}^n s_k \to \frac{1}{2}
    \]
    Thus $(a_n)$ is Cesàro summable to $1/2$ despite divergence.
\end{itemize}

% WEIERSTRASS M-TEST FOR SERIES
\paragraph*{\textbf{Weierstrass M-Test (Criterio M di Weierstrass)}}
\begin{itemize}
    \item \textbf{Statement:} Let $\{f_n\}$ be a sequence of functions on $E \subseteq \mathbb{R}$. If $\exists \{M_n\} \subset \mathbb{R}$ such that:
    \[
    \text{(i) } |f_n(x)| \leq M_n \quad \forall x \in E,  \quad \forall n \in \mathbb{N}
    \]
    \[
    \text{(ii) } \sum_{n=1}^{\infty} M_n < \infty
    \]
    Then $\sum_{n=1}^{\infty} f_n$ converges uniformly on $E$.
    
    \item \textbf{Key requirements:}
    \begin{itemize}
        \item Dominating series $\sum M_n$ must converge (absolutely)
        \item $M_n$ independent of $x \in E$
        \item $M_n \geq 0$ (non-negative majorants)
    \end{itemize}
    
    \item \textbf{Consequences:}
    \begin{itemize}
        \item Preserves continuity: If $f_n \in \mathcal{C}(E)$, then $\sum f_n \in \mathcal{C}(E)$
        \item Term-by-term integration:
        \[
        \int_a^b \sum_{n=1}^{\infty} f_n(x)  dx = \sum_{n=1}^{\infty} \int_a^b f_n(x)  dx
        \]
        \item Term-by-term differentiation (requires additional convergence of $\sum f_n'$)
    \end{itemize}
\end{itemize}

\paragraph*{\textbf{Special Case: Weierstrass Factorization}}
\begin{itemize}
    \item \textbf{Statement:} Every entire function $f$ can be represented as:
    \[
    f(z) = z^m e^{g(z)} \prod_{n=1}^{\infty} E_p\left(\frac{z}{a_n}\right)
    \]
    where $a_n$ are non-zero zeros, $m$ is zero multiplicity at origin, $g$ entire, and $E_p$ are elementary factors.
\end{itemize}

% LINEARITY OF SERIES SUM
\paragraph*{\textbf{Linearity of Series Sum (Teorema linearità somma)}}
\begin{itemize}
    \item \textbf{Statement:} Given two convergent series $\sum_{n=1}^{\infty} a_n = A$ and $\sum_{n=1}^{\infty} b_n = B$, and constants $\alpha, \beta \in \mathbb{R}$, then:
    \[
    \sum_{n=1}^{\infty} (\alpha a_n + \beta b_n) = \alpha A + \beta B
    \]
    
    \item \textbf{Proof Outline:}
    \begin{itemize}
        \item Let $s_N = \sum_{n=1}^{N} a_n \to A$ and $t_N = \sum_{n=1}^{N} b_n \to B$
        \item The partial sum of the combined series:
        \[
        \sigma_N = \sum_{n=1}^{N} (\alpha a_n + \beta b_n) = \alpha s_N + \beta t_N
        \]
        \item By limit laws:
        \[
        \lim_{N \to \infty} \sigma_N = \alpha \lim_{N \to \infty} s_N + \beta \lim_{N \to \infty} t_N = \alpha A + \beta B
        \]
    \end{itemize}
    
    \item \textbf{Key Requirements:}
    \begin{itemize}
        \item Both series must converge individually
        \item Linearity fails for divergent series (counterexample: $a_n = (-1)^n$, $b_n = (-1)^{n+1}$)
    \end{itemize}
    
    \item \textbf{Extension:} For finitely many convergent series $\sum a_n^{(k)} = S_k$ and constants $c_k$:
    \[
    \sum_{n=1}^{\infty} \left( \sum_{k=1}^{m} c_k a_n^{(k)} \right) = \sum_{k=1}^{m} c_k S_k
    \]
    
    \item \textbf{Warning:} Does \emph{not} apply to conditionally convergent series rearrangements (Riemann series theorem).
\end{itemize}

\subsection*{\sout{Cauchy Criterion}}

\sout{A series a converges if and and only if for every epsilon that
is grater than zero there exists a N such that for all \mbox{$m>n\geq N$}}

\subsubsection*{\\
\sout{\parbox{\linewidth}{
\[
|\sum_{k=n+1}^{m}a_{k}|<\epsilon
\]
}\\
}}

\subsubsection*{\sout{Absolute and Conditional Convergence}}

\subsubsection*{\sout{Using Cauchy criterion in problems}}

\paragraph*{\sout{For a Sequence}}

\sout{There are 4 main steps:}

\sout{1. Assign a epsilon that is grater than zero}

\sout{2. Find a integer N such that for all m, n > N, the inequality
\mbox{$|a_{m}-a_{n}|<\epsilon$} holds.}

\paragraph*{\sout{For a Series}}

\sout{The steps are very similar to a sequence}

\sout{1. Assign a epsilon that is grater than zero}

\sout{2. Find a integer N such that for all m>n>N, the inequality
\mbox{$|\sum_{k=n+1}^{m}a_{k}|<\epsilon$}}

\subsection*{Handling Convergence in Problems}

\subsubsection*{Comparison (Criterio del confronto)}

To find convergence using comparison, there are two primary methods
used. Direct comparison test and the Limit comparison test. 

\subparagraph*{Direct Comparison Test}

For two series $\sum a_{n}$ and $\sum b_{n}$ with $0\leq a_{n}\leq b_{n}$
for all $n\geq N$

If $\sum b_{n}$ converges then $\sum a_{n}$ will converge

If $\sum a_{n}$ diverges then $\sum b_{n}$ will diverge

\subparagraph*{Limit Comparison Test}

For two series $\sum a_{n}$ and $\sum b_{n}$ with $a_{n}>0,b_{n}>0$
\[
L=lim_{n\rightarrow\infty}\frac{a_{n}}{b_{n}}
\]

If L>0 $\sum a_{n}$ and $\sum b_{n}$ share the same convergence
type

If L=0 and $\sum b_{n}$ converges then $\sum a_{n}$ also converges

If L=$\infty$ and $\sum b_{n}$ diverges, then $\sum a_{n}$ also
diverges

\subsubsection*{Ratio (Criterio del rapporto)}

A series $\sum a_{n}$, calculate the limit
\[
L=lim_{n\rightarrow\infty}|\frac{a_{n+1}}{a_{n}}|
\]

If L < 1 the series converges

If L > 1 the series diverges

if L = 1 this test doesn't give enough info do determine a result

\subsubsection*{Root (Criterio della radice)}

A series $\sum a_{n}$, calculate the limit
\[
L=lim_{n\rightarrow\infty}\sqrt[n]{|a_{n}|}
\]

If L < 1 the series converges

If L > 1 the series diverges

if L = 1 this test doesn't give enough info do determine a result

\subsubsection*{Integral (Criterio del integrale)}

A series $\sum_{n=N}^{\infty}a_{n}$ with continuous, positive and
decreasing terms, $a_{n}=f(n)$ for $n\geq N$

If the integral $\int_{N}^{\infty}f(x)dx$ converges, then the series
$\sum a_{n}$ converges

If the integral diverges then $\sum a_{n}$ diverge

\subsection*{Cauchy Condensation Test (it is another way to say the same thing as above, I may remove this)}
If \(a_n \geq 0\) and \(a_n\) is decreasing, then \(\sum_{n=1}^{\infty} a_n\) converges iff \(\sum_{k=1}^{\infty} 2^k a_{2^k}\) converges.

\subsubsection*{\subsubsection*{Alternating Series Test (Leibniz)}
An alternating series \(\sum (-1)^n b_n\) or \(\sum (-1)^{n+1} b_n\) converges if:
\begin{enumerate}
    \item \(b_n \geq 0\) and \textbf{monotonically decreasing}: \(b_{n+1} \leq b_n\)
    \item \(\lim_{n \to \infty} b_n = 0\)
\end{enumerate}
This implies \textbf{conditional convergence} (converges but not absolutely).}

\subsection*{L'Hôpital's rule}

Is a theorem, which is primarily used to check limits by comparing
the derivatives of the limits. 

\paragraph*{Definition}

f and g are functions which are differentiable on a open interval.
Assuming that,

$lim_{x\rightarrow a}f(x)=lim_{x\rightarrow a}g(x)=0$, $g'(x)\neq0$
and $lim_{x\rightarrow a}\frac{f'(x)}{g'(x)}$

Then:

\[
lim_{x\rightarrow a}\frac{f(x)}{g(x)}=lim_{x\rightarrow a}\frac{f'(x)}{g'(x)}
\]


\subsubsection*{THIS WILL GO HERE BECAUSE I DONT KNOW WHERE ELSE TO PUT IT}

\paragraph*{\textbf{Domain Analysis (Analisi del Dominio)}}
\begin{itemize}
    \item Determine where $f(x)$ is defined
    \item Identify interval boundaries and discontinuities
\end{itemize}

\paragraph*{\textbf{Closed Interval Check (Intervalli Chiusi)}}
\begin{itemize}
    \item For $[a,b]$, apply Extreme Value Theorem (Teorema dei Valori Estremi):
    \[
    \exists\ x_{\text{max}},x_{\text{min}} \in [a,b] \quad \text{where}\quad f(x_{\text{max}}) \geq f(x) \geq f(x_{\text{min}})
    \]
\end{itemize}

\paragraph*{\textbf{First Derivative (Derivata Prima)}}
\begin{itemize}
    \item Compute $f'(x)$
    \item Find critical points (punti critici):
    \[
    \begin{cases}
    f'(x) = 0 \quad \text{(stationary points)}\\
    f'(x)\ \text{undefined} \quad \text{(cusps/corners)}
    \end{cases}
    \]
\end{itemize}

\paragraph*{\textbf{Classification (Classificazione)}}
\paragraph*{\textbf{Second Derivative Test (Test della Derivata Seconda)}}
\begin{itemize}
    \item Compute $f''(x)$
    \[
    \begin{cases}
    f''(x_0) > 0 \Rightarrow \text{local min (minimo locale)}\\
    f''(x_0) < 0 \Rightarrow \text{local max (massimo locale)}\\
    f''(x_0) = 0 \Rightarrow \text{inconclusive}
    \end{cases}
    \]
\end{itemize}

\paragraph*{\textbf{First Derivative Test (Test della Derivata Prima)}}
\begin{itemize}
    \item Analyze sign changes:
    \[
    \begin{array}{ll}
    + \to - & \Rightarrow \text{local max} \\
    - \to + & \Rightarrow \text{local min} \\
    \text{No change} & \Rightarrow \text{not extremum}
    \end{array}
    \]
\end{itemize}

\paragraph*{\textbf{Endpoint Evaluation (Valutazione agli Estremi)}}
\begin{itemize}
    \item For closed intervals:
    \[
    \text{Compare}\quad f(a),\ f(b),\ f(x_{\text{crit}})
    \]
\end{itemize}

\newpage{}

\part*{Post-Derivatives}

\section*{Derivatives}

``The Difference Between the Almost Right Word and the Right Word
Is Really a Large Matter---’Tis the Difference Between the Lightning
Bug and the Lightning'' - Mark Twain

\subsection*{Derivatives (Derivate)}

\subsubsection*{Fundamental Concept}
The derivative of a function \(f\) at a point \(a\) is the instantaneous rate of change of the function with respect to its input. Geometrically, it represents the slope of the tangent line to the graph of \(f\) at the point \((a, f(a))\), providing the best linear approximation of the function near \(a\).

\subsubsection*{Notation (Notazione)}
Two primary notations exist for derivatives:
\begin{itemize}
    \item Leibniz notation: \(\frac{df}{dx}\) or \(\frac{dy}{dx}\)
    \item Prime notation: \(f'(x)\)
\end{itemize}
Higher-order derivatives (derivate di ordine superiore) extend this notation with additional differentials or prime marks, representing repeated differentiation.

\subsubsection*{Limit Definition (Definizione di Limite)}
A function \(f: \mathbb{R} \to \mathbb{R}\) is differentiable (derivabile) at \(a \in \mathbb{R}\) if either of the following equivalent limits exists:
\[
f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h} = \lim_{x \to a} \frac{f(x) - f(a)}{x - a}
\]
This definition requires \(f\) to be defined on an open interval containing \(a\). In real analysis, differentiability signifies that \(f\) admits a linear approximation at \(a\):  
\[
f(x) = f(a) + f'(a)(x - a) + r(x)
\]
where the remainder term \(r(x)\) satisfies \(\lim_{x \to a} \frac{r(x)}{|x - a|} = 0\). The \(\varepsilon\)-\(\delta\) formalization states:
\[
\forall \varepsilon > 0,\  \exists \delta > 0\ :\ 0 < |x - a| < \delta \implies \left| \frac{f(x) - f(a)}{x - a} - f'(a) \right| < \varepsilon
\]

\subsubsection*{Geometric Interpretation (Interpretazione Geometrica)}
The derivative \(f'(a)\) is the limit of slopes of secant lines through points \((a, f(a))\) and \((x, f(x))\) as \(x \to a\). When this limit exists, it defines the slope of the unique tangent line (retta tangente) to \(f\) at \(a\).

\subsubsection*{Continuity and Differentiability}
Differentiability implies continuity (continuità): \(f\) differentiable at \(a\) \(\implies\) \(f\) continuous at \(a\). The converse does not hold, as demonstrated by functions with discontinuities in their rate of change, such as those containing corners (punti angolosi) or vertical tangents (tangenti verticali). In real analysis, this is formalized through the linear approximation condition: A function fails to be differentiable at points where the remainder \(r(x)\) does not vanish faster than \(|x-a|\).

\subsubsection*{Methods for Differentiation}

\paragraph*{Product rule}

\begin{equation}
\frac{d}{dx}\left[f(x)g(x)\right]=f'(x)g(x)+f(x)g'(x)
\end{equation}


\paragraph{Quotient Rule}

\begin{equation}
\frac{d}{dx}\left(\frac{f(x)}{g(x)}\right)=\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^{2}}
\end{equation}


\paragraph*{Chain Rule}

\begin{equation}
\frac{d}{dx}f(g(x))=f'(g(x))g'(x)
\end{equation}

\subsection*{Higher-Order Derivatives}
The $n$-th derivative is defined recursively:
\[
f^{(n)}(x) = \frac{d}{dx}\left[f^{(n-1)}(x)\right]
\]

\subsection*{Additional Differentiation Rules}

\subsubsection*{Basic Rules}
\begin{itemize}
    \item Constant Rule: $\frac{d}{dx}[c] = 0$
    \item Power Rule: $\frac{d}{dx}[x^n] = nx^{n-1}$
    \item Constant Multiple: $\frac{d}{dx}[cf(x)] = c f'(x)$
    \item Sum/Difference: $\frac{d}{dx}[f(x) \pm g(x)] = f'(x) \pm g'(x)$
\end{itemize}

\subsubsection*{Higher-Order Derivatives}
Second derivative: $f''(x)$ or $\frac{d^2y}{dx^2}$. Recursive formula:
\[
f^{(n)}(x) = \frac{d}{dx}\left[f^{(n-1)}(x)\right]
\]

\subsubsection*{Exponential/Logarithmic Derivatives}
\begin{itemize}
    \item $\frac{d}{dx}[e^x] = e^x$
    \item $\frac{d}{dx}[a^x] = a^x \ln a$
    \item $\frac{d}{dx}[\ln x] = \frac{1}{x}$
    \item $\frac{d}{dx}[\log_a x] = \frac{1}{x \ln a}$
\end{itemize}

\subsubsection*{Trigonometric Derivatives}
\begin{itemize}
    \item $\frac{d}{dx}[\sin x] = \cos x$
    \item $\frac{d}{dx}[\cos x] = -\sin x$
    \item $\frac{d}{dx}[\tan x] = \sec^2 x$
    \item $\frac{d}{dx}[\cot x] = -\csc^2 x$
    \item $\frac{d}{dx}[\sec x] = \sec x \tan x$
    \item $\frac{d}{dx}[\csc x] = -\csc x \cot x$
\end{itemize}

\subsubsection*{Inverse Trig Derivatives}
\begin{itemize}
    \item $\frac{d}{dx}[\sin^{-1} x] = \frac{1}{\sqrt{1 - x^2}}$
    \item $\frac{d}{dx}[\cos^{-1} x] = -\frac{1}{\sqrt{1 - x^2}}$
    \item $\frac{d}{dx}[\tan^{-1} x] = \frac{1}{1 + x^2}$
    \item $\frac{d}{dx}[\cot^{-1} x] = -\frac{1}{1 + x^2}$
    \item $\frac{d}{dx}[\sec^{-1} x] = \frac{1}{|x|\sqrt{x^2 - 1}}$
    \item $\frac{d}{dx}[\csc^{-1} x] = -\frac{1}{|x|\sqrt{x^2 - 1}}$
\end{itemize}

\subsubsection*{Implicit Differentiation}
Example for $x^2 + y^2 = 1$:
\[
2x + 2y \frac{dy}{dx} = 0 \Rightarrow \frac{dy}{dx} = -\frac{x}{y}
\]

\subsubsection*{Parametric Derivatives}
For $x = x(t)$, $y = y(t)$:
\[
\frac{dy}{dx} = \frac{dy/dt}{dx/dt}, \quad \frac{d^2y}{dx^2} = \frac{d/dt(dy/dx)}{dx/dt}
\]

\subsubsection*{Hyperbolic Derivatives}
\begin{itemize}
    \item $\frac{d}{dx}[\sinh x] = \cosh x$
    \item $\frac{d}{dx}[\cosh x] = \sinh x$
    \item $\frac{d}{dx}[\tanh x] = \text{sech}^2 x$
\end{itemize}

\subsubsection*{Inverse Function Derivative}
If $f$ and $g$ are inverses:
\[
g'(x) = \frac{1}{f'(g(x))}
\]

\paragraph*{\textbf{Taylor Series (Serie di Taylor)}}
The Taylor series of a smooth function \( f: \mathbb{R} \to \mathbb{R} \) expanded about \( a \in \mathbb{R} \) is:
\[
T_f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x - a)^n,
\]
where \( f^{(n)}(a) \) is the \( n \)-th derivative of \( f \) at \( a \).

\paragraph*{\textbf{Taylor's Theorem (Teorema di Taylor)}}
For \( N \geq 0 \), the function decomposes as:
\[
f(x) = \sum_{n=0}^{N} \frac{f^{(n)}(a)}{n!} (x - a)^n + R_N(x),
\]
with the Lagrange remainder:
\[
R_N(x) = \frac{f^{(N+1)}(c)}{(N+1)!} (x - a)^{N+1} \quad \text{for some } c \in (a, x).
\]

\paragraph*{\textbf{Analyticity (Analiticità)}}
\begin{itemize}
    \item The series converges to \( f(x) \) near \( a \) if \( \lim_{N \to \infty} R_N(x) = 0 \). 
    \item The \textbf{radius of convergence} \( R \) satisfies:
    \[
    \frac{1}{R} = \limsup_{n \to \infty} \left| \frac{f^{(n)}(a)}{n!} \right|^{1/n}.
    \]
    \item Smoothness \(\not\!\!\!\implies\) analyticity: The series may diverge or converge to a different function.
\end{itemize}

\paragraph*{\textbf{Multi-Variable Case (Caso Multivariato)}}
For \( f: \mathbb{R}^k \to \mathbb{R} \), the series generalizes using multi-indices \( \alpha = (\alpha_1, \ldots, \alpha_k) \):
\[
T_f(\mathbf{x}) = \sum_{|\alpha| \geq 0} \frac{\partial^{|\alpha|} f(a)}{\alpha!} (\mathbf{x} - \mathbf{a})^\alpha,
\]
with \( \alpha! = \alpha_1! \cdots \alpha_k! \) and \( \partial^{|\alpha|} = \frac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1} \cdots \partial x_k^{\alpha_k}} \).

\paragraph*{\textbf{Key Distinctions (Differenze Fondamentali)}}
\begin{itemize}
    \item \textbf{Analytic \( \iff \)} Taylor series converges to \( f \) locally.
    \item \textbf{Uniqueness:} Analytic functions have unique power series representations.
    \item \textbf{Global vs. Local:} Analyticity extends local derivative data to a neighborhood.
\end{itemize}

\paragraph*{\textbf{General Taylor Series Application (Applicazione Generale della Serie di Taylor)}}  
Consider a smooth function \( f: \mathbb{R} \to \mathbb{R} \) and a point \( a \in \mathbb{R} \).  

\paragraph*{\textbf{Expansion Process (Processo di Sviluppo)}}  
\begin{itemize}  
    \item Compute derivatives \( f^{(n)}(a) \) for \( n \geq 0 \).  
    \item Construct the Taylor series:  
    \[  
    T_f(x) = f(a) + f'(a)(x - a) + \frac{f''(a)}{2}(x - a)^2 + \cdots = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x - a)^n.  
    \]  
\end{itemize}  

\paragraph*{\textbf{Approximation and Error (Approssimazione ed Errore)}}  
For a fixed \( N \), the \( N \)-th Taylor polynomial approximates \( f \):  
\[  
P_N(x) = \sum_{n=0}^{N} \frac{f^{(n)}(a)}{n!}(x - a)^n,  
\]  
with error bounded by the Lagrange remainder:  
\[  
|R_N(x)| \leq \frac{\max_{c \in [a, x]} |f^{(N+1)}(c)|}{(N+1)!} |x - a|^{N+1}.  
\]  

\paragraph*{\textbf{Convergence Analysis (Analisi della Convergenza)}}  
\begin{itemize}  
    \item Calculate the radius of convergence \( R \) using:  
    \[  
    \frac{1}{R} = \limsup_{n \to \infty} \left| \frac{f^{(n)}(a)}{n!} \right|^{1/n}.  
    \]  
    \item If \( \lim_{N \to \infty} R_N(x) = 0 \) for \( |x - a| < R \), then \( T_f(x) = f(x) \) in this interval.  
\end{itemize}  

\paragraph*{\textbf{Interpretation (Interpretazione)}}  
This constructs a polynomial sequence \( \{P_N(x)\} \) that:  
\begin{itemize}  
    \item Matches \( f \)’s derivatives at \( a \) up to order \( N \).  
    \item Converges to \( f(x) \) locally if \( f \) is analytic at \( a \).  
\end{itemize}  

\pagebreak{}

\section*{Functions (Additional)}

\paragraph*{\textbf{Sign Function (Funzione Segno)}}
The sign function, denoted \(\operatorname{sign}(x)\) or \(\operatorname{sgn}(x)\), 
returns the sign of a real number. It is defined as:
\[
\operatorname{sign}(x) = 
\begin{cases} 
-1 & \text{if } x < 0 \\
\phantom{-}0 & \text{if } x = 0 \\
\phantom{-}1 & \text{if } x > 0 
\end{cases}
\]

\paragraph*{\textbf{Key Properties (Proprietà Fondamentali)}}
\begin{itemize}
    \item \textbf{Zero equivalence}: 
        \(\operatorname{sign}(x) = 0 \quad \Longleftrightarrow \quad x = 0\)
    \item \textbf{Odd symmetry}: 
        \(\operatorname{sign}(-x) = -\operatorname{sign}(x)\)
    \item \textbf{Multiplicative identity}: 
        \(x = |x| \cdot \operatorname{sign}(x) \quad \forall x \in \mathbb{R}\)
    \item \textbf{Absolute value relationship}: 
        \(|x| = x \cdot \operatorname{sign}(x) \quad \text{for } x \neq 0\)
\end{itemize}

\paragraph*{\textbf{Algebraic Identities (Identità Algebriche)}}
\begin{itemize}
    \item \textbf{Product rule}: 
        \(\operatorname{sign}(x \cdot y) = \operatorname{sign}(x) \cdot \operatorname{sign}(y)\)
    \item \textbf{Quotient rule}: 
        \(\operatorname{sign}\left(\frac{x}{y}\right) = \frac{\operatorname{sign}(x)}{\operatorname{sign}(y)} \quad (y \neq 0)\)
    \item \textbf{Square equivalence}: 
        \(\left[\operatorname{sign}(x)\right]^2 = 
        \begin{cases} 
        1 & x \neq 0 \\
        0 & x = 0 
        \end{cases}\)
\end{itemize}

\paragraph*{\textbf{Calculus Properties (Proprietà di Calcolo)}}
\begin{itemize}
    \item \textbf{Derivative}: The derivative exists in the distributional sense:
        \[
        \frac{d}{dx}\operatorname{sign}(x) = 2\delta(x)
        \]
        where \(\delta(x)\) is the Dirac delta function.
    \item \textbf{Integral representation}: 
        \[
        \operatorname{sign}(x) = \lim_{a \to 0^+} \frac{2}{\pi} \int_0^\infty \frac{\sin(xt)}{t} e^{-at}  dt
        \]
    \item \textbf{Step function connection}: 
        \(\operatorname{sign}(x) = 2H(x) - 1\) where \(H(x)\) is the Heaviside step function.
\end{itemize}

\paragraph*{\textbf{Special Values (Valori Speciali)}}
\begin{itemize}
    \item \(\operatorname{sign}(0) = 0\)
    \item \(\lim_{x \to \infty} \operatorname{sign}(x) = 1\)
    \item \(\lim_{x \to -\infty} \operatorname{sign}(x) = -1\)
    \item \(\operatorname{sign}(x) \cdot \operatorname{sign}(x) = 1 \quad (x \neq 0)\)
\end{itemize}

\pagebreak{}

\section*{Integrals}

“If you're gonna shoot an elephant Mr. Schneider, you better be prepared
to finish the job.”

\LyXbar{} Gary Larson, The Far Side 

\subsubsection*{Riemann Integral}

A Riemann integral is the the limit $f:[a,b]\rightarrow\mathbb{R}$
of all the Riemann sums between the points a and b. Defined as:

\[
\int_{a}^{b}f(x)dx=lim_{||P||\rightarrow0}\sum_{i=1}^{n}f(c_{i})\Delta x_{i}
\]
 Where:

n is the sub-intervals

$\Delta x_{i}$is the width of the i sub-interval

$c_{i}$is a sample point

{*}this exact definition will likely never be used in 

\paragraph*{Properties,}

\begin{gather*}
    \text{Linearity:} \quad \int_a^b \left( \alpha f(x) + \beta g(x) \right) dx = \alpha \int_a^b f(x) \, dx + \beta \int_a^b g(x) \, dx \\
    \text{Additivity:} \quad \int_a^b f(x) \, dx = \int_a^c f(x) \, dx + \int_c^b f(x) \, dx \quad \text{for } c \in (a, b) \\
    \text{Monotonicity:} \quad f(x) \leq g(x) \ \forall x \in [a, b] \implies \int_a^b f(x) \, dx \leq \int_a^b g(x) \, dx \\
    \text{Bounds:} \quad m(b - a) \leq \int_a^b f(x) \, dx \leq M(b - a) \quad \text{where } m = \inf_{[a,b]} f,\ M = \sup_{[a,b]} f \\
    \text{Integrability:} \quad f \text{ is Riemann integrable } \iff f \text{ bounded } \land \ f \text{ continuous a.e. on } [a, b] \\
    \text{Fundamental Theorem:} \quad F' = f \implies \int_a^b f(x) \, dx = F(b) - F(a)
\end{gather*}

\subsubsection*{Fundamental theorem of calculus}

\subsubsection*{Methods for integration}

\paragraph{Substitution,}

if $u=f(x)$, then $du=g'(x)dx$

\begin{equation}
\int f(g(x))g'(x)dx=\int f(u)du
\end{equation}


\paragraph{Integration by parts}

\begin{equation}
\int udv=uv-\int vdu
\end{equation}


\paragraph{Leibniz rule for differentiation.}

If f(t) is continuous and g(x) is differentiable
\[
f(x)=\int_{a}^{g(x)}f(t)dt
\]

If this is true, then:

\begin{equation}
F(x)'=f\left(g(x)\right)\cdot g'(x)
\end{equation}

If both the limits are dependent on x:

\[
I(x)=\int_{g_{1}(x)}^{g_{2}(x)}f(t)dt
\]

then:

\begin{equation}
I'(x)=f\left(g_{2}(x)\right)g_{2}'(x)-f\left(g_{1}(x)\right)g_{1}'(x)
\end{equation}

\subsubsection*{Improper Integral Convergence Criteria}
For $\int_a^\infty f(x)  dx$ or $\int_a^b f(x)  dx$ with singularity at $b$:
\begin{center}
\begin{tabular}{p{0.3\textwidth}p{0.65\textwidth}}
\textbf{Type} & \textbf{Condition} \\
\hline
$\int_1^\infty x^{-p}  dx$ & Converges iff $p > 1$ \\
$\int_0^1 x^{-p}  dx$ & Converges iff $p < 1$ \\
$\int_a^b |x-c|^{-p}  dx$ & Converges iff $p < 1$ \\
Oscillatory $\int_1^\infty g(x)  dx$ & Converges if $\int_1^\infty |g(x)|  dx < \infty$ (absolute conv.) \\
& or by Dirichlet test (bounded antiderivative, monotone $\to 0$)
\end{tabular}
\end{center}

\subsubsection*{Limit Comparison Test Framework}
Given $f(x) \sim g(x)$ as $x \to c$ (singularity or $\infty$):
\begin{enumerate}
\item Compute $L = \lim_{x \to c} \frac{f(x)}{g(x)}$
\item If $0 < L < \infty$, then $\int f$ and $\int g$ converge/diverge together
\item For $L=0$: $\int g < \infty \Rightarrow \int f < \infty$
\item For $L=\infty$: $\int g = \infty \Rightarrow \int f = \infty$
\end{enumerate}

\subsubsection*{Singularity Analysis Toolkit}
Near singular point $c$:
\begin{align*}
|\sin x| &\sim |x - k\pi| \quad \text{near } x = k\pi \\
|\ln x| &\sim |x - 1| \quad \text{near } x = 1 \\
\sqrt{x^2 + a} - x &\sim \frac{a}{2x} \quad \text{as } x \to \infty \\
e^x - 1 - x &\sim \frac{x^2}{2} \quad \text{near } x = 0
\end{align*}

\subsubsection*{Parameter-Dependent Integrals Strategy}
For $\int_a^b h(x,\alpha,\beta)  dx$ with $\alpha,\beta > 0$:
\begin{center}
\begin{tabular}{p{0.4\textwidth}p{0.55\textwidth}}
\textbf{Problem Type} & \textbf{Solution Approach} \\
\hline
$\int_0^1 x^{-\alpha}|\ln x|^\beta  dx$ & 1. Near 0: compare to $x^{-\alpha + \epsilon}$ \\
& 2. Near 1: compare to $|x-1|^\beta$ \\
& \fbox{Converges iff $\alpha < 1$} \\
\hline
$\int_0^\infty \frac{dx}{x^\alpha |\sin x|^\beta}$ & 1. Near $k\pi$: $|\sin x|^{-\beta} \sim |x-k\pi|^{-\beta}$ \\
& 2. At $\infty$: periodic singularities $\Rightarrow$ sum test \\
& \fbox{Finite interval: $\beta < 1$} \\
& \fbox{Infinite interval: $\beta < 1$ and $\alpha > 1$}
\end{tabular}
\end{center}

\subsubsection*{Advanced Convergence Techniques}
\begin{enumerate}
\item \textbf{Integration by Parts:} For $\int f'g$, requires $\lim_{x \to c} f(x)g(x)$ exists and $\int f g' < \infty$
\item \textbf{Series Comparison:} For $\int_1^\infty$, decompose into $\sum \int_{n}^{n+1}$ and compare to series
\item \textbf{Exponential Domination:} $x^p e^{-ax} \to 0$ for all $p$ when $a > 0$
\end{enumerate}

\subsubsection*{Parameter Limit Computations}
For $F(a) = \int_0^\infty g(x,a)  dx$:
\begin{enumerate}
\item Justify limit-interchange via Dominated Convergence or uniform bounds
\item For $F_a(x) = \int_0^{x^a} h(t)  dt$:
\begin{align*}
\lim_{x \to 0^+} F_a(x) &= \begin{cases} 
0 & a > 0 \\
\int_0^\infty h(t)  dt & a < 0 
\end{cases} \\
F_a'(0) &= \begin{cases} 
0 & a > 1 \\
\text{DNE} & a \leq 1 
\end{cases}
\end{align*}
\end{enumerate}

\subsubsection*{Theoretical Result Template}
Given $f \in C^1((0,1])$ with $\int_0^1 \sqrt{x} |f'(x)|  dx < \infty$:
\begin{enumerate}
\item $\sqrt{\epsilon} f(\epsilon) \to 0$ as $\epsilon \to 0^+$ by:
\[
\left| \sqrt{\epsilon} f(\epsilon) \right| \leq \sqrt{\epsilon} |f(1)| + \int_\epsilon^1 \sqrt{t} |f'(t)| \frac{\sqrt{\epsilon}}{\sqrt{t}}  dt
\]
\item $\int_0^1 \frac{f(x)}{\sqrt{x}}  dx < \infty$ via integration by parts:
\[
\int_\epsilon^1 \frac{f(x)}{\sqrt{x}}  dx = \left[2\sqrt{x}f(x)\right]_\epsilon^1 - 2\int_\epsilon^1 \sqrt{x} f'(x)  dx
\]
\end{enumerate}

\pagebreak{}


\section*{Differential Equations}

\textquotedbl Would you tell me, please, which way I ought to go
from here?\textquotedbl{}

\textquotedbl That depends a good deal on where you want to get to,\textquotedbl{}
said the Cat.

\subsubsection*{ODEs and PDEs}

Ordinary Differential Equations (ODEs) are a differential equation
which has a single variable. ODEs have a general form:

\begin{equation}
F\left(x,y,\frac{dy}{dx},\frac{d^{2}y}{dx^{2}},...,\frac{d^{n}y}{dx^{n}}\right)=0
\end{equation}

where 

- x independent

- y dependent

Partial Differential Equations (PDEs) are a Differential equation
which has multiple independent variables. Instead of using the standard
d, they use partial derivatives (\ensuremath{\partial}) to show the
change with respect for multiple variables. The general form is:

\begin{equation}
F\left(x,y,u,\frac{\partial u}{\partial x},\frac{\partial u}{\partial y},\frac{\partial^{2}u}{\partial x^{2}},....\right)=0
\end{equation}

where

- x,y independent

- u(x,y) dependent

Fundamentally image a ODE as a means to track a single car, while
PDE track all the traffic in the city.

\subsubsection*{Types of ODEs}

ODEs are usually classified by 2 primary things. their order, aka
the degree of their derivative, and by whether they are linear or
non-linear. 

\paragraph*{First order ODEs}

are pretty self explanatory, they involve only the 1st derivative.
Here is a basic first order ODE:

\paragraph*{
\begin{equation}
\frac{dy}{dx}+y=x
\end{equation}
Second order ODEs}

involve UP to the 2nd derivative. Here is a example:

\paragraph*{
\begin{equation}
\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}+y=0
\end{equation}
Higher order ODEs}

involve everything 3rd derivative or higher. It is unlikely to ever
appear in a 1st year analysis exam, but you never know. 

\paragraph*{Liniar and Non-liniear ODEs.}

An ODE is linear if the dependent variable and the derivatives are
in a linear form. Basically: they are not multiplied together. Anything
else is considered non-liniear. A linear ODE can be written in the
form:

\begin{equation}
a_{n}(x)\frac{d^{n}y}{dx^{n}}+a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}}+...+a_{1}(x)\frac{dy}{dx}+a_{0}(x)y=f(x)
\end{equation}

- a(x) is a function of x

Here are some basic examples. We will go in much more detail when
solving ODEs.

$\frac{dy}{dx}+3y=x$, and $\frac{d^{2}y}{dx^{2}}+x\frac{dy}{dx}+y=sinx$

\[
Ineedtolearnbetterformating:)
\]
 A non-linear ODE is any ordinary differential equation that cannot
be written in the linear form shown earlier. This is because the dependent
variable or its derivatives are not linear. I will cover this more
later on in the chapter.

\subsubsection*{The weird classifications of ODEs}

\paragraph{A Homogeneous ODE,}

is a differential equation where L is a linear differential operator.

\paragraph{
\[
L[y]=0
\]
 A Non-Homogeneous ODE,}

is a differential equation where f(x) \ensuremath{\neq} 0

\paragraph{
\[
L[y]=f(x)
\]
 A Autonomous ODE,}

is a differential equation where the independent variable (usually
x or t) does not appear in the equation.

\paragraph*{
\[
\frac{d^{n}y}{dx^{n}}=F\left(y,y',...,y^{(n-1)}\right)
\]
A Non-Autonomous ODE,}

is a differential equation if the independent variable appears eq
(1).

{*}You can use multiple types at once, just use common sense to make
sure its right

\subsubsection*{Basic existence}

\subsection*{Dealing with ODEs}

\paragraph*{Separable ODE,}

can be written as 
\[
\frac{dy}{dx}=f(x)g(y)
\]
 Divide both sides by g(y), and multiply both sides by dx
\[
\frac{dy}{g(y)}=f(x)dx
\]
 Integrate both sides, make sure to keep the constant on the RHS
\[
\int\frac{dy}{g(y)}=\int f(x)dx
\]


\paragraph*{Integrating Method.}

Format the equation to fit the following before using the method
\[
\frac{dy}{dx}+P(x)y=Q(x)
\]
 Find the function $\mu(x)$ that will help simplify the problem.
(Symplify as much as possible here it will help a lot later on)
\[
\mu(x)=e^{\int P(x)dx}
\]
Multiply every term by the function $\mu(x)$ and using the product
rule calculate the derivative of the LHS
\[
\frac{d}{dx}(\mu(x)y)=\mu(x)Q(x)
\]
 Integrate both sides, remember the constant!
\[
y=\frac{1}{\mu(x)}\int\mu(x)Q(x)dx+C
\]

Quick note on integrating factor. A integrating factor is the method
used to solve derivative equation above. The \textgreek{μ}(x) also
called the integrating factor, works for any, first-order linear differential
equations. A function is derived by multiplying the equation with
\textgreek{μ}(x), which makes the left-hand side a derivative of \textgreek{μ}(x)y.

\paragraph*{Exact Method}

If a DE is exact, which can be found if it is in this form

\[
M(x,y)dx+N(x,y)dy=0
\]
 After this, calculate the partial derivative of M in respect to y
and the partial derivative of N with respect to x. If these are equivalent
the DE is exact.

\[
\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}
\]

Lets make a function that we will call $\Psi$ such that, $\varPsi_{x}=M(x,y)$
and $\Psi_{y}=N(x,y)$

Therefore we can write this now as

\[
\Psi_{x}+\Psi_{y}\frac{dy}{dx}=0
\]
we can start to find this function $\Psi$. So we will start to integrate
M with respect to x. (h(y) is a function of y)

\[
\Psi(x,y)=\int Mdx+h(y)
\]
 We can now differentiate $\Psi$ with respect to y

\[
\frac{\partial\Psi}{\partial y}=\frac{\partial}{\partial y}\left(\int Mdx\right)+h'(y)=N
\]
 Solve the integral for h(y)

\subsection*{Types of Problems}

\subsubsection*{Initial Value Problem}

Generally, IVPs are a DE and a \uline{initial condition} or condition's
which when used in unison they can be used to solve a function, that
will also fit the DE. The steps are pretty straight forward. 

1. Solve the DE

\[
y(x)=\int f(x)dx+C
\]

2. Use the initial condition, lets say that $y(x_{0})=y_{0}$

\[
y_{0}=\int f(x_{0})dx+C
\]

Where C is:

\[
C=y_{0}-V
\]
 {*}V is the value of the integral at $x_{0}$, therefore if we replace
C, the final answer is

\paragraph*{
\[
y(x)=\protect\int f(x)dx+(y_{0}-V)
\]
}

\paragraph*{Proving existence and uniqueness}

\subparagraph*{Theorem Definition:}

If f(x,y) and the partial derivative $\frac{\partial f}{\partial y}$
are continuous in:

\[
D=\left\{ (x,y)||x-x_{0}|\leq a,|y-y_{0}|\leq b\right\} 
\]
around the point $(x_{0},y_{0})$ therefore, there exits a interval
between x and $x_{0}$where there is at least one solution of y(x).
And it proves that this solution is unique on the interval.

\subparagraph*{Steps:}

Write the ODE in standard form, aka:

\[
\frac{dy}{dx}=f(x,y)
\]

use intuition to check that the function f is continuous between the
points you want. Then if there are any points where the function is
non continuous, then be sure to mark it such that it is clear, using
$\leq\geq><$. This is the first rule.

Now, do a partial derivative of y such that:

\[
\frac{\partial f}{\partial y}=f(x,y)
\]

Remember to treat x as a constant in this case!!!

If the result is continuous in D between x and $x_{0}$ then this
proves that there is at least a solution. 

\subsection*{Interpreting answers}

\paragraph{Interval of validity for Linear DE.}

The interval of validity for a Linear DE is largest around $x_{0}$where
$p(x)$ and $q(x)$ are continuous. Make sure to exclude discontinuities.

\paragraph{Interval of validity for Non-Linear DE,}

Solutions may be exponential to infinity or become undefined despite
$f(x,y)$ being smooth. Therefore check all the points where the solution
becomes undefined. Extend the interval on both sides of $x_{0}$ till
it is no longer possible.

\subsubsection*{Interpreting answers}

(this whole section is a bit useless, I may remove it if I don't find
any use for it son)

\paragraph*{Explicit Solution,}

is when the dependent variable is isolated and in terms of the independent
variable. e.g

\[
y=x^{2}+C
\]

(soluzione esplicita)

\paragraph*{Implicit Solution,}

is when the dependent variable is not explicitly isolated from the
independent. e.g

\[
x^{2}+y^{2}=C
\]

(soluzione implicita)

\paragraph{General Solution,}

is the solution containing all the possible solutions for the differential
equation, ie it keeps the constants, its the trivial form. (soluzione
generale)

\paragraph*{Particular Solution,}

is the solution which is a specific solution by locking the constants
by using the initial conditions, ie the C has a fixed value. (soluzione
particolare)

\paragraph*{Equilibrium Solution,}

is a solution which is constant because the dependent variable does
not change and therefore the derivative is zero. (soluzione di equilibrio)

\paragraph*{Parametric Solution,}

is a solution represented using a parameter like (t,u,z..) instead
of using x and y. eg.

\[
y(t)=\sqrt{t^{2}+C}
\]

(soluzione parametrica)

\subsubsection*{Second Order Equations-- temporary latex code which isnt my own
for the exam}



\subsubsection*{Solving Second-Order Differential Equations}
A second-order differential equation has the general form:
\[
F(y'', y', y, x) = 0
\]
Below are methods for solving linear and nonlinear cases.

%-------------------------------------------------
\paragraph{1. Linear Homogeneous Equations with Constant Coefficients}
\textbf{General form:}
\[
a y'' + b y' + c y = 0 \quad (a \neq 0)
\]
\textbf{Solution procedure:}
\begin{enumerate}
    \item Solve the \textbf{characteristic equation}:
    \[
    a r^2 + b r + c = 0
    \]
    
    \item \textbf{Case analysis for roots \( r_1, r_2 \):}
    \begin{itemize}
        \item \textbf{Distinct real roots:}  
        If \( r_1 \neq r_2 \),
        \[
        y(x) = C_1 e^{r_1 x} + C_2 e^{r_2 x}
        \]
        
        \item \textbf{Repeated real root:}  
        If \( r_1 = r_2 = r \),
        \[
        y(x) = (C_1 + C_2 x)e^{r x}
        \]
        
        \item \textbf{Complex conjugate roots:}  
        If \( r = \alpha \pm i\beta \),
        \[
        y(x) = e^{\alpha x}\left[C_1 \cos(\beta x) + C_2 \sin(\beta x)\right]
        \]
    \end{itemize}
\end{enumerate}

%-------------------------------------------------
\paragraph{2. Linear Nonhomogeneous Equations}
\textbf{General form:}
\[
y'' + p(x)y' + q(x)y = g(x)
\]
\textbf{Method 1: Undetermined Coefficients}
\begin{enumerate}
    \item Find the complementary solution \( y_c \) (solve the homogeneous equation).
    \item Assume a particular solution \( y_p \) based on \( g(x) \) (e.g., \( g(x) = e^{kx} \Rightarrow y_p = Ae^{kx} \)).
    \item If \( g(x) \) matches part of \( y_c \), multiply \( y_p \) by \( x \) (or \( x^n \) for repeated roots).
    \item Substitute \( y_p \) into the DE and solve for coefficients.
    \item General solution: \( y = y_c + y_p \).
\end{enumerate}

\textbf{Method 2: Variation of Parameters}
\begin{enumerate}
    \item Find \( y_c = C_1 y_1 + C_2 y_2 \).
    \item Compute the Wronskian: 
    \[
    W = y_1 y_2' - y_1' y_2
    \]
    \item Particular solution:
    \[
    y_p = -y_1 \int \frac{y_2 g(x)}{W} \, dx + y_2 \int \frac{y_1 g(x)}{W} \, dx
    \]
    \item General solution: \( y = y_c + y_p \).
\end{enumerate}

%-------------------------------------------------
\paragraph{3. Cauchy-Euler Equations}
\textbf{General form:}
\[
x^2 y'' + b x y' + c y = 0
\]
\textbf{Solution steps:}
\begin{enumerate}
    \item Assume \( y = x^r \). Substitute to get:
    \[
    r^2 + (b - 1)r + c = 0
    \]
    \item Solve for \( r \). The solution mirrors constant-coefficient cases:
    \begin{itemize}
        \item Real distinct roots: \( y = C_1 x^{r_1} + C_2 x^{r_2} \)
        \item Repeated root: \( y = x^{r}(C_1 + C_2 \ln x) \)
        \item Complex roots \( r = \alpha \pm i\beta \):  
        \( y = x^{\alpha}\left[C_1 \cos(\beta \ln x) + C_2 \sin(\beta \ln x)\right] \)
    \end{itemize}
\end{enumerate}

%-------------------------------------------------
\paragraph{4. Reduction of Order}
\textbf{When one solution \( y_1 \) is known:}
\begin{enumerate}
    \item Let \( y_2 = v(x) y_1 \).
    \item Substitute \( y_2 \) into the DE and solve for \( v(x) \).
    \item The second solution is \( y_2 = y_1 \int \frac{e^{-\int p(x) \, dx}}{y_1^2} \, dx \).
\end{enumerate}

\textbf{Special cases:}
\begin{itemize}
    \item \textbf{Equation missing \( y \):} Let \( p = y' \), reducing to \( p' = f(x, p) \).
    \item \textbf{Equation missing \( x \):} Let \( p = y' \), then \( y'' = p \frac{dp}{dy} \), reducing to \( p \frac{dp}{dy} = f(y, p) \).
\end{itemize}

%-------------------------------------------------
\paragraph{5. Series Solutions Near Ordinary Points}
For \( P(x)y'' + Q(x)y' + R(x)y = 0 \) with ordinary point at \( x_0 \):
\begin{enumerate}
    \item Assume \( y = \sum_{n=0}^\infty a_n (x - x_0)^n \).
    \item Substitute into DE and equate coefficients of like powers.
    \item Derive a recurrence relation for \( a_n \).
\end{enumerate}

%-------------------------------------------------
\paragraph{6. Laplace Transform for Initial Value Problems}
\textbf{Procedure:}
\begin{enumerate}
    \item Take Laplace transform of the DE:
    \[
    \mathcal{L}\{y''\} + a\mathcal{L}\{y'\} + b\mathcal{L}\{y\} = \mathcal{L}\{g(x)\}
    \]
    \item Use:
    \[
    \mathcal{L}\{y'\} = sY(s) - y(0), \quad \mathcal{L}\{y''\} = s^2Y(s) - sy(0) - y'(0)
    \]
    \item Solve for \( Y(s) \), then compute \( y(x) = \mathcal{L}^{-1}\{Y(s)\} \).
\end{enumerate}

%-------------------------------------------------
\paragraph*{Nonlinear Second-Order DEs}
No universal method exists. Common approaches:
\begin{itemize}
    \item Substitutions to reduce order (e.g., \( p = y' \))
    \item Exact equations (identify integrable combinations)
    \item Numerical methods (e.g., Runge-Kutta)
\end{itemize}

Homogeneous Equations with constant coefficients have the general
form:
\[
y''+ay'+by=0
\]
 Non-homogeneous Equations: Method of Undetermined Coefficients
\[
Finished
\]


\subsection*{Nth order Differential Equation}

\subsubsection{Solving \( n \)-th Order Differential Equations}  
\paragraph{Linear Homogeneous Equations with Constant Coefficients}  
Consider the equation:  
\[  
a_n y^{(n)} + a_{n-1} y^{(n-1)} + \cdots + a_1 y' + a_0 y = 0  
\]  

\textbf{Solution Steps:}
\begin{enumerate}
    \item \textbf{Form characteristic equation}:
    \[
    a_n r^n + a_{n-1} r^{n-1} + \cdots + a_1 r + a_0 = 0
    \]
    
    \item \textbf{Find roots} \( r_1, r_2, \ldots, r_n \)
    
    \item \textbf{Construct general solution}:
    \begin{itemize}
        \item \textit{Distinct real roots}:
        \[
        y_h = \sum_{i=1}^n C_i e^{r_i x}
        \]
        
        \item \textit{Repeated real root \( r \) with multiplicity \( k \)}:
        \[
        e^{r x}\left(C_1 + C_2 x + \cdots + C_k x^{k-1}\right)
        \]
        
        \item \textit{Complex conjugate pairs \( \alpha \pm \beta i \)}:
        \[
        e^{\alpha x}\left[C_1 \cos(\beta x) + C_2 \sin(\beta x)\right]
        \]
        For repeated pairs (multiplicity \( m \)):
        \[
        x^{m-1}e^{\alpha x}\left[C_1 \cos(\beta x) + C_2 \sin(\beta x)\right]
        \]
    \end{itemize}
\end{enumerate}

\paragraph{Linear Nonhomogeneous Equations}  
For equations:
\[
a_n y^{(n)} + \cdots + a_1 y' + a_0 y = g(x)
\]
\textbf{General Solution}: 
\[
y = y_h + y_p
\]
where \( y_h \) = homogeneous solution, \( y_p \) = particular solution.

\subparagraph{Method of Undetermined Coefficients}  
Use when \( g(x) \) is polynomial, exponential, sine, cosine, or combinations:

\begin{enumerate}
    \item Assume \( y_p \) with same form as \( g(x) \)
    \item If any term matches \( y_h \), multiply by \( x^s \) (\( s \) = smallest integer eliminating duplication)
    \item Substitute \( y_p \) into DE and solve for coefficients
\end{enumerate}

\subparagraph{Variation of Parameters}  
General method for arbitrary \( g(x) \):

\begin{enumerate}
    \item Find fundamental set \( \{y_1, \ldots, y_n\} \) from \( y_h \)
    \item Compute Wronskian:
    \[
    W(y_1, \ldots, y_n) = \begin{vmatrix}
    y_1 & y_2 & \cdots & y_n \\
    y_1' & y_2' & \cdots & y_n' \\
    \vdots & \vdots & \ddots & \vdots \\
    y_1^{(n-1)} & y_2^{(n-1)} & \cdots & y_n^{(n-1)}
    \end{vmatrix}
    \]
    
    \item Find \( u_i' = \frac{W_i}{W} \) where \( W_i \) = Wronskian with \( i \)-th column replaced by \( \begin{bmatrix} 0 \\ \vdots \\ g(x) \end{bmatrix} \)
    
    \item Integrate to get \( u_i \), then:
    \[
    y_p = \sum_{i=1}^n u_i y_i
    \]
\end{enumerate}

\paragraph{Variable Coefficient Equations}  
For \( y^{(n)} + P_{n-1}(x)y^{(n-1)} + \cdots + P_0(x)y = Q(x) \):

\subparagraph{Reduction of Order}  
If solution \( y_1 \) is known, let:
\[
y = y_1 \int v(x) dx
\]
Substitute to reduce equation order by 1.

\subparagraph{Cauchy-Euler Equations}  
Form: \( x^n y^{(n)} + a_{n-1} x^{n-1} y^{(n-1)} + \cdots + a_0 y = 0 \)

\begin{enumerate}
    \item Assume solution \( y = x^m \)
    \item Substitute to get characteristic equation:
    \[
    m(m-1)\cdots(m-n+1) + \sum_{k=0}^{n-1} a_k m(m-1)\cdots(m-k+1) = 0
    \]
    \item Handle roots as with constant coefficient equations
\end{enumerate}

\paragraph{Nonlinear Equations}  
\subparagraph{Order Reduction Techniques}
\begin{itemize}
    \item \textit{Missing \( y \)}: Let \( v = y' \), reduces order by 1
    \item \textit{Missing \( x \)}: Let \( v = y' \), then \( y'' = v \frac{dv}{dy} \), reduces to 1st order in \( v \)
\end{itemize}

\subparagraph{Exact Equations}  
If equation can be written as:
\[
\frac{d}{dx}\left[\text{Lower order expression}\right] = 0
\]
Integrate successively to solve.

\subparagraph{Integrating Factors}  
Find \( \mu(x) \) or \( \mu(y) \) such that:
\[
\mu(x)M(x,y)dx + \mu(x)N(x,y)dy = 0
\]
becomes exact.

\subparagraph{Special Forms}  
\begin{itemize}
    \item \textit{Bernoulli}: \( y' + P(x)y = Q(x)y^n \), use \( z = y^{1-n} \)
    \item \textit{Riccati}: \( y' = P(x)y^2 + Q(x)y + R(x) \), use \( y = y_1 + \frac{1}{v} \) if particular solution \( y_1 \) known
\end{itemize}

\subsection*{Systems of Differential Equations}

\subsubsection*{Solving Systems of Differential Equations}
\paragraph*{Linear Systems with Constant Coefficients}
For the system $\mathbf{x}' = A\mathbf{x}$ where $A$ is an $n \times n$ constant matrix:

\textbf{Solution Method:}
\begin{enumerate}
    \item \textbf{Find eigenvalues} $\lambda$ by solving:
    \[
    \det(A - \lambda I) = 0
    \]
    
    \item \textbf{Find eigenvectors} $\xi$ for each eigenvalue by solving:
    \[
    (A - \lambda I)\xi = 0
    \]
    
    \item \textbf{Construct general solution}:
    \begin{itemize}
        \item \textit{Real distinct eigenvalues}:
        \[
        \mathbf{x}(t) = \sum_{i=1}^n C_i e^{\lambda_i t} \xi_i
        \]
        
        \item \textit{Complex eigenvalues} $\alpha \pm \beta i$:
        \[
        \mathbf{x}(t) = C_1 e^{\alpha t}[\mathbf{a}\cos(\beta t) - \mathbf{b}\sin(\beta t)] + C_2 e^{\alpha t}[\mathbf{a}\sin(\beta t) + \mathbf{b}\cos(\beta t)]
        \]
        where $\mathbf{a} + i\mathbf{b}$ is the complex eigenvector
        
        \item \textit{Repeated eigenvalues}:
        \begin{itemize}
            \item If geometric multiplicity = algebraic multiplicity: proceed as distinct eigenvalues
            \item If deficient eigenvectors: use generalized eigenvectors
        \end{itemize}
    \end{itemize}
\end{enumerate}

\paragraph*{Nonhomogeneous Systems}
For $\mathbf{x}' = A\mathbf{x} + \mathbf{g}(t)$:

\textbf{General Solution}:
\[
\mathbf{x}(t) = \mathbf{x}_h(t) + \mathbf{x}_p(t)
\]

\subparagraph*{Variation of Parameters}
\begin{enumerate}
    \item Find fundamental matrix $\Phi(t)$ from homogeneous solutions
    \item Compute particular solution:
    \[
    \mathbf{x}_p(t) = \Phi(t) \int \Phi^{-1}(t)\mathbf{g}(t) dt
    \]
\end{enumerate}

\subparagraph*{Method of Undetermined Coefficients}
Use when $\mathbf{g}(t)$ contains polynomials, exponentials, or trigonometric functions:
\begin{enumerate}
    \item Assume $\mathbf{x}_p$ with same form as $\mathbf{g}(t)$
    \item Adjust for resonance if any term matches homogeneous solution
    \item Substitute and solve for coefficients
\end{enumerate}

\paragraph*{Matrix Exponential Method}
For $\mathbf{x}' = A\mathbf{x}$:
\[
\mathbf{x}(t) = e^{At}\mathbf{x}_0
\]
where $e^{At}$ can be computed via:
\begin{itemize}
    \item Diagonalization: $A = PDP^{-1} \Rightarrow e^{At} = Pe^{Dt}P^{-1}$
    \item Jordan form for defective matrices
    \item Taylor series expansion for simple cases
\end{itemize}

\paragraph*{Nonlinear Systems}
\subparagraph*{Linearization Near Critical Points}
\begin{enumerate}
    \item Find equilibrium points $\mathbf{x}_0$ where $\mathbf{f}(\mathbf{x}_0) = 0$
    \item Compute Jacobian matrix:
    \[
    J = \begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_n}{\partial x_n}
    \end{bmatrix}_{\mathbf{x}_0}
    \]
    \item Analyze eigenvalues of $J$ to determine stability
\end{enumerate}

\subparagraph*{Phase Plane Analysis (2D Systems)}
\begin{itemize}
    \item Classify critical points: node, spiral, saddle, center
    \item Use nullclines and direction fields
    \item Lyapunov functions for stability (when applicable)
\end{itemize}

\paragraph*{Conversion to First-Order Systems}
Any $n$-th order DE can be converted to a system:
\begin{enumerate}
    \item Let $x_1 = y$, $x_2 = y'$, ..., $x_n = y^{(n-1)}$
    \item Create system:
    \[
    \begin{cases}
        x_1' = x_2 \\
        x_2' = x_3 \\
        \vdots \\
        x_n' = F(t,x_1,\ldots,x_n)
    \end{cases}
    \]
\end{enumerate}

\paragraph*{Important Special Cases}
\subparagraph*{Coupled Oscillators}
\[
\begin{cases}
    m_1 x_1'' = -k_1 x_1 + k_2(x_2 - x_1) \\
    m_2 x_2'' = -k_2(x_2 - x_1)
\end{cases}
\]
Solve by diagonalizing the coefficient matrix

\subparagraph*{Competing Species Model}
\[
\begin{cases}
    x' = x(a - by) \\
    y' = y(c - dx)
\end{cases}
\]
Analyze using linearization and phase plane methods

\subsection*{Special theorems and Problems}

\subsubsection*{Picard--Lindelöf theorem}

\subsubsection*{Picard–Lindelöf Theorem (Existence \& Uniqueness)}
\paragraph*{Theorem Statement}  
Consider the initial value problem (IVP):
\[
\begin{cases}
    y'(t) = f(t, y(t)) \\
    y(t_0) = y_0
\end{cases}
\]
where \( f: D \subset \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n \). If:

\begin{itemize}
    \item \( f \) is \textbf{continuous} in \( t \) on rectangle \( R = [t_0 - a, t_0 + a] \times \overline{B}(y_0, b) \)
    \item \( f \) is \textbf{Lipschitz continuous} in \( y \): 
    \[
    \exists L > 0 \text{ s.t. } \|f(t,y_1) - f(t,y_2)\| \leq L\|y_1 - y_2\|,\; \forall (t,y_1),(t,y_2) \in R
    \]
\end{itemize}

Then \( \exists \tau > 0 \) such that the IVP has a \textbf{unique solution} \( y(t) \) on \( [t_0 - \tau, t_0 + \tau] \).

\paragraph*{Proof Outline (Method of Successive Approximations)}  
\begin{enumerate}
    \item Reformulate IVP as integral equation:
    \[
    y(t) = y_0 + \int_{t_0}^t f(s, y(s)) ds
    \]
    
    \item Define Picard iterations:
    \[
    y_{n+1}(t) = y_0 + \int_{t_0}^t f(s, y_n(s)) ds
    \]
    Starting with \( y_0(t) \equiv y_0 \)
    
    \item Show \( \{y_n\} \) converges uniformly to solution \( y \):
    \begin{itemize}
        \item Use Lipschitz condition to prove \( \|y_{n+1} - y_n\| \leq \frac{M}{L} \frac{(L|t-t_0|)^{n+1}}{(n+1)!} \)
        \item Apply Banach fixed-point theorem in complete metric space
    \end{itemize}
\end{enumerate}

\paragraph*{Implementation Steps}  
To apply the theorem:
\begin{enumerate}
    \item Verify continuity of \( f(t,y) \) in \( t \)
    \item Check Lipschitz condition in \( y \):
    \begin{itemize}
        \item If \( \frac{\partial f}{\partial y} \) exists and bounded \( \Rightarrow \) Lipschitz
        \item For scalar case: \( |f(t,y_1) - f(t,y_2)| \leq L|y_1 - y_2| \)
    \end{itemize}
    
    \item Determine existence interval \( \tau = \min\left(a, \frac{b}{M}\right) \) where:
    \[
    M = \max_{(t,y)\in R} \|f(t,y)\|
    \]
\end{enumerate}

\paragraph*{Example Application}  
For IVP \( y' = y,\; y(0) = 1 \):
\begin{itemize}
    \item Picard iterations:
    \begin{align*}
        y_0(t) &= 1 \\
        y_1(t) &= 1 + \int_0^t y_0(s) ds = 1 + t \\
        y_2(t) &= 1 + \int_0^t (1+s) ds = 1 + t + \frac{t^2}{2} \\
        &\vdots \\
        y_n(t) &= \sum_{k=0}^n \frac{t^k}{k!} \to e^t
    \end{align*}
\end{itemize}

\paragraph*{Important Notes}  
\begin{itemize}
    \item \textbf{Local vs Global}: Theorem guarantees local solution - need additional conditions for global existence
    \item \textbf{Sharpness}: \( \tau \) estimate often conservative
    \item \textbf{Failure Cases}:
    \begin{itemize}
        \item \( f \) not Lipschitz \( \Rightarrow \) possible non-uniqueness (e.g., \( y' = \sqrt{|y|} \))
        \item Discontinuous \( f \) \( \Rightarrow \) solutions may not exist
    \end{itemize}
\end{itemize}

\subsection*{The Question Playbook}

% Series Convergence
\subsubsection*{Series Convergence}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Asymptotic Analysis:} For $a_n$ as $n \to \infty$, use Taylor expansions or logarithms. \\
    Example: $\log(n^\alpha + n^2) - \log(n^2) \sim n^{\alpha-2}$ if $\alpha < 2$.
    \item \textit{Alternating Series (Leibniz):} Verify $|a_n| \searrow 0$.
    \item \textit{Comparison Test:} Bound by $p$-series ($\sum n^{-p}$ converges iff $p>1$) or geometric series.
    \item \textit{Ratio/Root Test:} Preferred for factorials or exponentials.
    \item \textit{Weierstrass M-test (Uniform Convergence):} Find $M_k$ such that $|f_k(x)| \leq M_k$ and $\sum M_k < \infty$. \\
    Non-uniform example: If $\sup_x |f_k(x)| \not\to 0$, uniform convergence fails.
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item Identify dominant term in $a_n$.
    \item Apply convergence test based on asymptotic behavior.
    \item For functional series (e.g., $\sum (-1)^k f_k(x)$):
    \begin{enumerate}
        \item Find pointwise convergence set (e.g., exclude $x=0$ if terms $\not\to 0$).
        \item Check uniform convergence:
        \begin{itemize}
            \item On $\mathbb{R}$: Often fails near points where $f_k \not\to 0$.
            \item On $[a,\infty)$: Use $M_k = \sup_{x \geq a} |f_k(x)|$.
        \end{itemize}
    \end{enumerate}
\end{enumerate}

% Maxima/Minima of Functions
\subsubsection*{Maxima/Minima of Functions}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Critical Points:} Solve $f'(x) = 0$.
    \item \textit{Second Derivative Test:} $f''(x_0) > 0$ (local min), $f''(x_0) < 0$ (local max).
    \item \textit{Boundary/Behavior at Infinity:} Evaluate $\lim_{x \to \pm\infty} f(x)$.
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item Compute $f'$ and solve $f'(x) = 0$.
    \item Evaluate $f$ at critical points and boundary points.
    \item Classify using $f''$ or sign analysis of $f'$. \\
    Example: $f_k(x) = \frac{1 - x}{1 + kx^2}$ $\to$ solve $f_k'(x) = 0$ for critical points.
\end{enumerate}

% Improper Integrals
\subsubsection*{Improper Integrals}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Convergence Tests:}
    \begin{itemize}
        \item \textit{Comparison:} $|f(x)| \leq g(x)$ and $\int g < \infty$.
        \item $p$\textit{-test:} $\int_a^\infty x^{-p}  dx$ converges iff $p > 1$.
    \end{itemize}
    \item \textit{Behavior at Singularities:} Near $x = 0$ or $x = \infty$, use asymptotic expansions.
    \item \textit{Absolute vs. Conditional Convergence:} $|\int f| \leq \int |f|$.
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item Split integral at singularities.
    \item For each interval, find asymptotic equivalence (e.g., $x \to 0^+$: $|\sin x| \sim |x - k\pi|$).
    \item Apply comparison or $p$-test. \\
    Example: $\int_0^{e^2} \frac{dx}{2x - e \ln x}$ $\to$ analyze $x \to 0^+$ and $x \to e^2$.
\end{enumerate}

% ODEs
\subsubsection*{Ordinary Differential Equations (ODEs)}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Separable Equations:} $u' = g(x)h(u) \implies \int \frac{du}{h(u)} = \int g(x)  dx$.
    \item \textit{Linear Equations:} $u' + p(x)u = q(x)$ $\to$ integrating factor $e^{\int p}$.
    \item \textit{Maximal Interval:} Solution exists until blowup or if Lipschitz.
\end{itemize}

\textbf{Steps for Cauchy Problems:}
\begin{enumerate}
    \item Solve ODE explicitly (if separable/linear).
    \item Apply initial condition.
    \item Identify blowup points (e.g., $u' = u^2 - 2u + 2$ $\to$ $u = 1 + \tan(x - 7)$ blows up at $x = 7 \pm \pi/2$).
\end{enumerate}

\textbf{Steps for Boundary Value Problems (BVP):}
\begin{enumerate}
    \item Solve characteristic equation (e.g., $r^2 + 2r + c = 0$).
    \item Apply boundary conditions; solution unique iff homogeneous solution trivial.
\end{enumerate}

% Sequences Defined by Recurrence
\subsubsection*{Sequences Defined by Recurrence}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Monotone + Bounded $\implies$ Convergent:} Show $a_{n+1} \leq a_n$ and bounded below.
    \item \textit{Fixed Points:} Solve $L = f(L)$; if $|f'(L)| < 1$, stable.
    \item \textit{Asymptotic Behavior:} For $\sum a_n x^n$, radius $R = 1/\limsup |a_n|^{1/n}$.
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item Compute initial terms.
    \item Verify monotonicity/boundedness.
    \item If convergent, solve $L = f(L)$. \\
    Example: $a_{n+1} = \sin a_n$, $a_0 = 2024$ $\to$ $a_n \searrow 0$.
\end{enumerate}

% Integral Computation and Estimation
\subsubsection*{Integral Computation and Estimation}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Substitution:} Simplify exponents (e.g., $t = \alpha x^2$).
    \item \textit{Integration by Parts:} $\int u  dv = uv - \int v  du$.
    \item \textit{Gamma Function:} $\int_0^\infty x^n e^{-\alpha x^2}  dx$ $\to$ use $t = \alpha x^2$.
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item Identify substitution to match standard forms.
    \item For $[0, \infty)$, exploit convergence of $\Gamma(z)$. \\
    Example: $\int_0^\infty x^{2n+1} e^{-\alpha x^2}  dx = \frac{n!}{2\alpha^{n+1}}$.
\end{enumerate}

% Lipschitz Functions and ODE Solutions
\subsubsection*{Lipschitz Functions and ODE Solutions}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Lipschitz Proof:} Show $|f'(y)|$ bounded.
    \item \textit{Separable ODEs:} Direct integration.
\end{itemize}

\textbf{Steps for Cauchy Problems:}
\begin{enumerate}
    \item Verify Lipschitz condition (for uniqueness).
    \item Solve ODE (e.g., $u' = |u| + 1$ $\to$ separate variables).
    \item Apply initial condition.
\end{enumerate}

% Area and Region Calculations
\subsubsection*{Area and Region Calculations}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Integration:} Area $= \iint_E dx  dy$.
    \item \textit{Iterated Integrals:} Fix $y$, integrate in $x$ (or vice versa).
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item Sketch region and find intersection points.
    \item Express bounds (e.g., $E: 0 \leq y \leq \arctan x$, $x \leq \cos y$ $\implies x \in [\tan y, \cos y]$ for $y \in [0, y_0]$).
    \item Compute $\int (\text{upper} - \text{lower})  dy$.
\end{enumerate}

% Inequalities and Function Analysis
\subsubsection*{Inequalities and Function Analysis}
\textbf{Key Techniques:}
\begin{itemize}
    \item \textit{Taylor Expansions:} Prove $e^{4x^2} > 1 + 8x^2$ via series.
    \item \textit{Derivative Sign:} Show $g'(x) > 0$ for $g(x) \geq 0$.
\end{itemize}

\textbf{Steps:}
\begin{enumerate}
    \item Define auxiliary function $h(x) = f(x) - g(x)$.
    \item Compute $h'(x)$ and analyze extrema.
\end{enumerate}

% Summary Flowchart
\subsubsection*{Summary Flowchart for Problem-Solving}
\begin{enumerate}
    \item Classify problem type: Series, ODE, Integral, Sequence, Optimization, etc.
    \item Select technique based on classification (reference table above).
    \item Execute steps methodically.
    \item Verify conditions (e.g., continuity for integration by parts).
\end{enumerate}

\subsubsection*{Fin :)}
\end{document}
