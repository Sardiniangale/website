%% LyX 2.4.3 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[american]{article}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[T1]{fontenc}
\usepackage[utf8]{luainputenc}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{stmaryrd}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand{\lyxmathsym}[1]{\ifmmode\begingroup\def\b@ld{bold}
  \text{\ifx\math@version\b@ld\bfseries\fi#1}\endgroup\else#1\fi}


\makeatother

\usepackage{babel}
\begin{document}
\title{Linear Algebra \& Geometry}
\date{May 22, 2025}
\author{Giacomo}

\maketitle
\includegraphics[scale=0.5]{/home/guc/Pictures/Cover}

\newpage{}

\part*{Pre-Det}

\section*{Spaces (Spazi Vettoriali)}

\textquotedbl I have not failed. I've just found 10,000 ways that
won't work.\textquotedbl{}

\subsection*{Basic Definitions}

\paragraph{Vectors (Vettori):}

Vectors are mathematical tools which can be visualized as arrows.
They hold tow primary operations, scaling and addition. 

\subparagraph{Scaling:}

Scaling involves multiplying a scalar quantity $\lambda$ which doesn't
have a direction with the vectors. This either amplifies or reduces
the vector without changing the ``line its on''

\[
\lambda\in\mathbb{R},v=\left(\begin{array}{c}
v_{1}\\
v_{2}
\end{array}\right)\in R^{2}:\lambda\cdot v=\left(\begin{array}{c}
\lambda v_{1}\\
\lambda v_{2}
\end{array}\right)
\]


\subparagraph*{Addition:}

Addition involves adding two vectors to make a new vector

\[
v=\left(\begin{array}{c}
v_{1}\\
v_{2}
\end{array}\right),w=\left(\begin{array}{c}
w_{1}\\
w_{2}
\end{array}\right)\in R^{2}:v+w=\left(\begin{array}{c}
v_{1}+w_{1}\\
v_{2}+w_{2}
\end{array}\right)
\]


\paragraph*{Subsets (Sottoinsiemi):}

This is a set where all the elements are contained within another
set. For linear algebra it could be a collection of vectors within
a vector space e.g $\mathbb{R}^{3}$

\[
S=\left\{ \left(x,y,0\right)|x,y\in\mathbb{R}\right\} 
\]


\subparagraph*{Basic proof}

See 2.210 in proof book

\paragraph*{Proper Subsets (Sottoinsiemi Propri):}

This is a subset that is always smaller than the original set. The
exact definition is: 

\begin{equation}
A\subsetneq B
\end{equation}
if:

1. Containment: ``Every element of A is also a element of B''

\begin{equation}
\forall x(x\in A\Longmapsto x\in B)
\end{equation}

2. Not equal: ``There exists at least one element in B that is not
A''

\begin{equation}
\exists y(y\in B\land y\notin A)
\end{equation}


\paragraph*{Supersets (Soprainsiemi):}

This is the opposite of a subset. If $A\subset B$, then B is a superset
of A

\subsection*{Linear Independence}

Let V be a vector space over a field F. $S\subseteq V$ is linearly
independent if the following conditions are true:

For a collection of vectors $\left\{ v_{1},v_{2},...,v_{n}\right\} \subseteq S$
and scalars $\left\{ a_{1},a_{2},...,a_{n}\right\} \in F$

\begin{equation}
a_{1}v_{1}+a_{2}v_{2}+...+a_{n}v_{n}=0
\end{equation}

i.e every single variant of a up to n has to equal zero. If it does
not, it is linearly dependent

\subsection*{Dimensions of a space}

The dimension (dimensione) of a vector space (spazio vettoriale) is
the number of vectors in any basis (base) for that space. A basis
(base) is a set of vectors that are linearly independent (linearmente
indipendenti) and span (generano) the entire space.

\subsection*{Sum of subspace (Somma di sottospazi)}

Let U and V be subspaces (sottospazi) of a vector space (spazio vettoriale)
V over a field $\mathbb{F}$. The sum (somma) can be defined as:

\begin{equation}
U+V=\left\{ u+v|u\in U,v\in V\right\} 
\end{equation}


\subsection*{Direct Sum (Somma diretta)}

U+V is called a direct sum, if the intersection of U and V is trivial
???- (Further desc required) then:

\[
U\cap V=\left\{ 0\right\} 
\]
 The direct sum can be denoted as $U\oplus V:$--to be expanded upon

\[
z=u+v
\]


\section*{Inner Products}

\subsection*{Definition:}

\subparagraph{For $\mathbb{R}$}

For a vectors $v=(v_{1}....v_{n})$ and $w=(w_{1}....w_{n})$ it is

\begin{equation}
<v,w>=v\cdot w=\sum_{i=1}^{n}v_{i}w_{i}
\end{equation}


\subparagraph*{For $\mathbb{C}$}

For a complex vectors $v=(v_{1}....v_{n})$ and $w=(w_{1}....w_{n})$
it is

\begin{equation}
<v,w>=\sum_{i=1}^{n}v_{i}\overline{w}_{i}
\end{equation}


\subsection*{Specifics in $\mathbb{R}$}

\subsubsection*{Proof:}

Lets take a vector u and a vector v and say that they are orthogonal.
Because of the orthogonality there is some $\lambda$ which allows
the transformation. 

\[
\left(\begin{array}{c}
v_{1}\\
v_{2}
\end{array}\right)=\lambda\left(\begin{array}{c}
-u_{2}\\
u_{1}
\end{array}\right)
\]

(Below the $\leftrightarroweq$ is being used as a substitute for
and). 

\[
u_{1}\cdot v_{1}=-\cancelto{v_{2}}{u_{1}\lambda}u_{2}\leftrightarroweq u_{2}\cdot v_{2}=\cancelto{-v_{1}}{u_{2}\lambda}u_{1}
\]

Since v2 and -v1 equal to those terms above they cancel out the unknown
$\lambda$

\[
u_{1}\cdot v_{1}=-v_{2}\cdot u_{2}\leftrightarroweq u_{2}\cdot v_{2}=-v_{1}u_{1}
\]

Since both sides are now the same formula, we can move them from one
side to the other removing the negative equaling =0

\[
u_{1}v_{1}+u_{2}v_{2}=0
\]

\[
u_{1}v_{1}\Rightarrow<u,v>
\]


\subsubsection*{Length: (Needs some work)}

However the euclidean length of a vector v is from the inner product
of v itself:

\[
\Vert v\Vert=\sqrt{<v,v>}=\sqrt{v_{1}^{2}+v_{2}^{2}+...+v_{n}^{2}}
\]


\subsubsection*{Angle Between Vectors:}

By using the inner product and the euclidean length the angle between
the two vectors is able to be calculated:

\[
<v,w>=\Vert v\Vert\Vert w\Vert cos\theta
\]

If we rearrange for cos$\theta$

\[
cos\theta=\frac{<v,w>}{\Vert v\Vert\Vert w\Vert}
\]

Then to compute the angle use the inverse of cos which is arccos

\begin{equation}
\theta=arccos\left(\frac{<v,w>}{\Vert v\Vert\Vert w\Vert}\right)
\end{equation}


\subparagraph{Small Proof Example:}

Lets make v=(3,0) and w= (0,3). This is because the vectors are perpendicular.

The inner product:

\[
<v,w>=(3)(0)+(0)(3)=0
\]

\textasciicircum{} Currently lines up with normal properties

The norms (Euclidean length) are both 3.

Solving for theta

\[
\theta=arccos(\frac{0}{3\cdot3})=90\lyxmathsym{\textdegree}
\]

$\boxempty$

\subsubsection*{Projection:}

\subsubsection*{Orthogonality:}

\section*{Matrix (Matrice)}

A matrix is one of the most important parts of linear algebra. Generally
it is used as a means to store a array of elements which can be anything
from numbers to functions. These are stored in rows (righe) and columns
(colonne). Its shortened version is generally represented by a capitalized
letter such as A:

\[
A=\left[\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{array}\right]
\]


\paragraph*{Common terms:}

(Will be covered in more detail later)

1. Element (elementi), the individual points such as $a_{ij}$

2. Square matrix (matrice quadrata), a symmetrical matrix where m=n

3. Rectangular matrix (matrice rettangolare), a matrix where m is
not equal to n

4. Transpose (trasposta), denoted as $A^{T}$, is simply swapping
the rows and collums

5. Diagonal (diagonale), the set of elemets where i=j. Self explanatory

\subsubsection*{Basic operations}

\subparagraph*{Addition/Subtraction}

\[
(A\pm B)_{ij}=a_{ij}\pm b_{ij}
\]


\subparagraph*{Scalar Multiplication}

\[
(cA)_{ij}=c\cdot a_{ij}
\]


\subparagraph*{Matrix Multiplication}

\[
c_{ik}=\sum_{j=1}^{p}a_{ij}b_{jk}
\]


\subsubsection*{Specific Matrixes}

\subparagraph*{Identity Matrix (matrice identit√†)}

\subparagraph*{Zero Matrix (matrice nulla)}

\subparagraph*{Diagonal Matrix (matrice diagonale)}

\subparagraph*{Symmetric Matrix (matrice simmetrica)}

\part*{Post-Det}
\end{document}
