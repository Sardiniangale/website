%% LyX 2.4.3 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[british]{article}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[LGR,T1]{fontenc}
\usepackage[utf8]{luainputenc}
\usepackage{color}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{graphicx}
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\newcommand*\LyXbar{\rule[0.585ex]{1.2em}{0.25pt}}
\DeclareRobustCommand{\greektext}{%
  \fontencoding{LGR}\selectfont\def\encodingdefault{LGR}}
\DeclareRobustCommand{\textgreek}[1]{\leavevmode{\greektext #1}}

%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxcode}
	{\par\begin{list}{}{
		\setlength{\rightmargin}{\leftmargin}
		\setlength{\listparindent}{0pt}% needed for AMS classes
		\raggedright
		\setlength{\itemsep}{0pt}
		\setlength{\parsep}{0pt}
		\normalfont\ttfamily}%
	 \item[]}
	{\end{list}}

\makeatother

\usepackage{babel}
\begin{document}
\title{Mathematical Analysis 1}
\date{4 August 2025}
\author{Giacomo}

\maketitle
\includegraphics[scale=0.5]{/home/guc/Pictures/Cover}

\newpage{}

\part*{Pre-Derivatives}

\section*{Theorems, functions and axioms}

``Calvin: You know, I don’t think math is a science, I think it’s
a religion.

Hobbes: A religion?

Calvin: Yeah. All these equations are like miracles. You take two
numbers and when you add them, they magically become one NEW number!
No one can say how it happens. You either believe it or you don’t.
{[}Pointing at his math book{]} This whole book is full of things
that have to be accepted on faith! It’s a religion!''

\subsection*{Logic}

\subsubsection*{Propositional Logic}

A proposition is a statement. Statements in math are either true or
false, if you combined multiple propositions there are multiple outcomes
depending on the validity of each proposition in the statement. I
call them compound statements (but I dont think its the actual name).

\paragraph*{Negation (Negazione):}

Represented by a $\lnot$. Inverts the value of a statement, e.g (Bob
went to the store), the opposite can be represented as $\lnot$(Bob
went to the store).

\paragraph*{Disjunction (Disgiunzione):}

Represented by a $\vee$. Statement is true if at least one of the
propositions is also true.

\paragraph*{Conjunction (Congiunzione):}

Represented by a $\wedge.$ Statement is true only if both propositions
are also true.

\paragraph*{Implication (Implicazione):}

Represented by a $\rightarrow$. Its formally defined as : $\neg P\vee Q$.

\paragraph*{Biconditional (Bicondizionale):}

Represented by a $\leftrightarrow$. Statement is true if both of
the propositions share the same value.

\subsubsection*{Predicate Logic}

\paragraph*{Universal:}

It is represented by $\forall$. ``For every ...''

\paragraph*{Existence:}

It is represented by $\exists$. ``There exists ...''

\subsubsection*{Axioms}

\paragraph*{Axioms (Assiomi):}

An axiom is a postulate, more commonly known as assumption. It is
a statement that is held as always true in regards to the problem
or proof needed to solve. There are different types of axioms which
are briefly stated below:

\subparagraph*{Logical Axioms:}

A universal truth in all of Mathematics applicable in both the Physics
Notes and the Linear Algebra Notes.

\subparagraph*{Non Logical Axioms:}

Domain specific assumptions, such as Axioms only applicable in $\mathbb{R}$(Gross
oversimplification)

\subsection*{Group Theory (Theoria degli insiemi)}

\subsubsection*{Definition of a Group}

A group is a set, that has the following requirements:

\paragraph*{Closure:}

For all $a,b\in G,a*b\in G$

\paragraph*{Associativity:}

$a,b,c\in G,(a*b)*c=a*(c*b)$

\paragraph*{Identity:}

Lets say e exists $e\in G$ such that $e*a=a*e=a$ for all $a\in G$ 

\paragraph*{Inverse:}

If there exists a a, such that $a\in G$ therefore there exists a
inverses

\subsection*{Proofs (Dimostrazione)}

In Math, every theorem and formula needs to be able to be proven in
a proof. There are multiple types of proofs which are used to show
that a theorem and or formula is valid. In these notes, very few proofs
will be done, however in the written blue book. Each proof I have
written in there has a classification. If a proof is referenced it
will have a corresponding code written next to it referencing the
written proof in the blue notebook. 

\paragraph*{Direct Proof (Dimostrazione Diretta).}

By using known definitions, axioms and theorems, a sequence of logical
steps can be used to directly demonstrate whether or not the statement
is correct.

\paragraph{Proof by Contradiction (Dimostrazione per Assurdo).}

Assume that a statement is false and connect it to a logical contradiction. 

\paragraph*{Induction (Induzione).}

Used for statements involving natural numbers

\subparagraph*{Base Case: }

Verify the statement holds for the initial value

\subparagraph*{Inductive Step: }

Assume it holds for a n=k, then prove it holds for n=k+1.

\paragraph*{Constructive Proof (Dimostrazione Costruttiva).}

Make a identity with the exact desired property. More formally `'Demonstrates
the existence of an object by explicitly constructing it'\/'

\subsection*{Functions}

A \textbf{function} (\textit{funzione}) is a very common conecept in math that formalizes the relationship between two sets by assigning each element of the first set to exactly one element of the second set. Formally, a function \( f: A \to B \) consists of:
\begin{itemize}
    \item A \textbf{domain} (\textit{dominio}) \( A \), the set of all possible inputs.
    \item A \textbf{codomain} (\textit{codominio}) \( B \), the set into which all outputs are mapped.
    \item A rule or correspondence that links each element \( x \in A \) to a unique element \( f(x) \in B \).
\end{itemize}

\paragraph*{Formal Definition}
A function \( f \) is a subset of the Cartesian product \( A \times B \) such that for every \( x \in A \), there exists exactly one \( y \in B \) where \( (x, y) \in f \). This is denoted as \( y = f(x) \).

\paragraph*{Key Properties}
\begin{enumerate}
    \item \textbf{Injectivity} (\textit{iniettiva}): A function is injective if distinct inputs map to distinct outputs:
    \[
        \forall x_1, x_2 \in A, \quad f(x_1) = f(x_2) \implies x_1 = x_2.
    \]
    \item \textbf{Surjectivity} (\textit{suriettiva}): A function is surjective if every element in \( B \) is an output for some input:
    \[
        \forall y \in B, \quad \exists x \in A \text{ such that } y = f(x).
    \]
    \item \textbf{Bijectivity} (\textit{biiettiva}): A function is bijective if it is both injective and surjective, establishing a one-to-one correspondence between \( A \) and \( B \).
\end{enumerate}

\subsection*{Natural Numbers}



The \textbf{natural numbers} (\textit{numeri naturali}) are the standard version of numbers in maths, used for counting and ordering. Formally, the set of natural numbers \( \mathbb{N} \) is defined as:
\[
\mathbb{N} = \{1, 2, 3, \ldots\} \quad \text{(sometimes including } 0 \text{ depending on context)}.
\]
They are characterized by their discrete, non-negative integer values and form the foundation for number theory and arithmetic.

\paragraph*{Formal Definition (Peano Axioms)}
The properties of natural numbers are axiomatically defined by the \textbf{Peano axioms} (\textit{assiomi di Peano}):
\begin{enumerate}
    \item \( 1 \) (or \( 0 \)) is a natural number.
    \item Every natural number \( n \) has a unique successor \( S(n) \), which is also a natural number.
    \item \( 1 \) (or \( 0 \)) is not the successor of any natural number.
    \item Distinct natural numbers have distinct successors: \( S(m) = S(n) \implies m = n \).
    \item \textbf{Induction}: If a property holds for \( 1 \) (or \( 0 \)) and holds for \( S(n) \) whenever it holds for \( n \), then it holds for all natural numbers.
\end{enumerate}

\paragraph*{Key Properties}
\begin{itemize}
    \item \textbf{Closure under addition and multiplication}: For all \( a, b \in \mathbb{N} \), \( a + b \in \mathbb{N} \) and \( a \cdot b \in \mathbb{N} \).
    \item \textbf{Non-closure under subtraction and division}: Subtraction \( a - b \) or division \( a / b \) may not result in a natural number.
    \item \textbf{Well-ordering principle} (\textit{principio del buon ordinamento}): Every non-empty subset of \( \mathbb{N} \) has a least element.
    \item \textbf{Infinite cardinality}: \( \mathbb{N} \) is countably infinite.
\end{itemize}

\paragraph*{Number Theory}
Natural numbers are central to number theory, which studies:
\begin{enumerate}
    \item \textbf{Prime numbers} (\textit{numeri primi}): Natural numbers \( > 1 \) with no divisors other than \( 1 \) and themselves:
    \[
        \mathbb{P} = \{2, 3, 5, 7, 11, \ldots\}.
    \]
    \item \textbf{Divisibility}: A number \( a \) divides \( b \) (\( a \mid b \)) if \( \exists k \in \mathbb{N} \) such that \( b = a \cdot k \).
    \item \textbf{Mathematical induction} (\textit{induzione matematica}): A proof technique leveraging the Peano axioms.
    \item \textbf{Modular arithmetic} (\textit{aritmetica modulare}): Operations on residues modulo \( n \), e.g., \( 7 \equiv 2 \mod 5 \).
\end{enumerate}


\subsection*{Whole Numbers}

\textbf{Whole Numbers} (\textit{numeri interi non negativi}) are an extension of the natural numbers that include zero, forming the set \( \mathbb{W} = \{0, 1, 2, 3, \ldots\} \). They are used for counting discrete objects and represent non-negative integers without fractions or decimals. 

\paragraph*{\textbf{Formal Definition}}  
The set \( \mathbb{W} \) satisfies the \textbf{Peano axioms} (\textit{assiomi di Peano}) with zero as the base element:
\begin{itemize}
    \item \( 0 \) is a whole number.
    \item Every whole number \( n \) has a unique successor \( S(n) \in \mathbb{W} \).
    \item \( 0 \) is not the successor of any whole number.
    \item Distinct numbers have distinct successors: \( S(a) = S(b) \implies a = b \).
    \item \textbf{Induction}: If a property holds for \( 0 \) and for \( S(n) \) whenever it holds for \( n \), it holds for all \( \mathbb{W} \).
\end{itemize}

\paragraph*{\textbf{Key Properties}}  
\begin{itemize}
    \item \textbf{Closure under addition and multiplication}: For \( a, b \in \mathbb{W} \), \( a + b \in \mathbb{W} \) and \( a \cdot b \in \mathbb{W} \).
    \item \textbf{Non-closure under subtraction}: \( a - b \in \mathbb{W} \) only if \( a \geq b \).
    \item \textbf{Additive identity}: \( 0 + a = a \) for all \( a \in \mathbb{W} \).
    \item \textbf{Well-ordering principle} (\textit{principio del buon ordinamento}): Every non-empty subset of \( \mathbb{W} \) has a least element.
\end{itemize}

\paragraph*{\textbf{Representation}}  
Whole numbers are represented in numeral systems such as:
\begin{itemize}
    \item \textbf{Decimal}: \( 0, 1, 2, \ldots \)
    \item \textbf{Binary}: \( 0_2 = 0_{10}, 1_2 = 1_{10}, 10_2 = 2_{10} \)
    \item \textbf{Unary}: \( 0 \) (often represented as an absence of marks), \( | = 1, || = 2 \).
\end{itemize}

\paragraph*{\textbf{Differences from Natural Numbers}}  
Unlike natural numbers (\textit{numeri naturali}), which sometimes exclude zero, whole numbers explicitly include \( 0 \). This makes \( \mathbb{W} \) the set \( \mathbb{N} \cup \{0\} \) in contexts where natural numbers start at \( 1 \).


\paragraph*{Below,}

is a image of all the relevant groups of numbers and how they are
related. Not all of them have been stated in this point in the notes,
but they are all relevant for analysis 1

\includegraphics[scale=0.2]{/home/guc/Pictures/NumberSetinC\lyxdot svg}

\subsection*{Exponential Properties}

\subsubsection*{Exponential Functions}
For $a > 0$ and $x \in \mathbb{R}$, the \emph{exponential function} with base $a$ is defined as the function $f(x) = a^x$. This represents continuous growth (when $a > 1$) or decay (when $0 < a < 1$) processes. The fundamental identity $a^x = e^{x \ln a}$ relates all exponentials to the natural base $e \approx 2.71828$, where $\exp(x) = e^x$ is the unique solution to $y' = y$ with $y(0)=1$. Exponentials map additive changes to multiplicative scaling: $a^{x+y} = a^x \cdot a^y$.

\subsubsection*{Exponential Properties}
For $a > 0$, $b > 0$, and $x,y \in \mathbb{R}$:
\begin{itemize}
\renewcommand{\labelitemi}{}
\setlength\itemsep{-0.5em}
\setlength\parsep{-0.5em}
    \item $a^x a^y = a^{x+y}$
    \item $\frac{a^x}{a^y} = a^{x-y}$
    \item $(a^x)^y = a^{xy}$
    \item $a^0 = 1$
    \item $a^{-x} = \frac{1}{a^x}$
    \item $(ab)^x = a^x b^x$
    \item $\left(\frac{a}{b}\right)^x = \frac{a^x}{b^x}$
\end{itemize}

\subsubsection*{Def of Logarithms}
For $a > 0$ ($a \neq 1$), $b > 0$:
\begin{itemize}
\renewcommand{\labelitemi}{}
    \item $\log_a b = x \iff a^x = b$
    \item $\log_a 1 = 0$
    \item $\log_a a = 1$
    \item $a^{\log_a b} = b$
\end{itemize}

\subsubsection*{Logarithm Properties}
For $a > 0$ ($a \neq 1$), $x,y > 0$, $k \in \mathbb{R}$:
\begin{itemize}
\renewcommand{\labelitemi}{}
    \item $\log_a(xy) = \log_a x + \log_a y$
    \item $\log_a\left(\frac{x}{y}\right) = \log_a x - \log_a y$
    \item $\log_a(x^k) = k\log_a x$
    \item $\log_a a^x = x$
\end{itemize}

\subsubsection*{Change of Base}
For $a,b > 0$ ($a,b \neq 1$), $c > 0$:
\[
\log_a c = \frac{\log_b c}{\log_b a}
\]
Special case: $\log_a b = \frac{1}{\log_b a}$

\subsubsection*{Natural Exponential and Logarithm}
\begin{itemize}
\renewcommand{\labelitemi}{}
    \item $e^x = \exp(x)$
    \item $\ln x = \log_e x$
    \item $e^{\ln x} = x$ for $x > 0$
    \item $\ln(e^x) = x$ for $x \in \mathbb{R}$
\end{itemize}

\subsubsection*{Exponential-Logarithmic Equations}
Key solving techniques:
\begin{itemize}
\renewcommand{\labelitemi}{}
    \item If $a^x = a^y$ then $x = y$
    \item If $\log_a x = \log_a y$ then $x = y$
    \item To solve $a^{f(x)} = b$: take logarithms of both sides
    \item To solve $\log_a f(x) = b$: rewrite as $f(x) = a^b$
\end{itemize}

\subsection*{Trigonometric Functions}

\textbf{Trigonometric Functions} (\textit{funzioni trigonometriche}) are periodic functions that relate angles in a right triangle or on the unit circle to ratios of side lengths. The primary trigonometric functions are sine (\(\sin\)), cosine (\(\cos\)), tangent (\(\tan\)), and their reciprocals: cosecant (\(\csc\)), secant (\(\sec\)), and cotangent (\(\cot\)).

\paragraph*{\textbf{Formal Definition (Unit Circle)}}  
For an angle \(\theta\) measured counterclockwise from the positive \(x\)-axis on the unit circle (\(x^2 + y^2 = 1\)):
\[
\sin\theta = y, \quad \cos\theta = x, \quad \tan\theta = \frac{y}{x} \quad (x \neq 0).
\]
The reciprocals are defined as:
\[
\csc\theta = \frac{1}{\sin\theta}, \quad \sec\theta = \frac{1}{\cos\theta}, \quad \cot\theta = \frac{x}{y} \quad (y \neq 0).
\]

\paragraph*{\textbf{Key Properties}}  
\begin{itemize}
    \item \textbf{Periodicity} (\textit{periodicità}): \(\sin\theta\) and \(\cos\theta\) have period \(2\pi\); \(\tan\theta\) and \(\cot\theta\) have period \(\pi\).
    \item \textbf{Range}: 
    \[
    \sin\theta, \cos\theta \in [-1, 1]; \quad \tan\theta \in \mathbb{R} \text{ (excluding asymptotes)}.
    \]
    \item \textbf{Parity}: \(\sin\theta\) and \(\tan\theta\) are odd functions; \(\cos\theta\) is even:
    \[
    \sin(-\theta) = -\sin\theta, \quad \cos(-\theta) = \cos\theta.
    \]
    \item \textbf{Pythagorean Identity} (\textit{identità pitagorica}):
    \[
    \sin^2\theta + \cos^2\theta = 1.
    \]
\end{itemize}

\paragraph*{\textbf{Differences from Other Functions}}  
Unlike polynomial or exponential functions, trigonometric functions:
\begin{itemize}
    \item Are periodic and bounded (except \(\tan\theta\) and \(\cot\theta\)).
    \item Model oscillatory behavior (e.g., waves, circular motion).
    \item Require angular input (radians or degrees) rather than purely scalar quantities.
\end{itemize}

\paragraph*{Unit Circle For Reference:}

\includegraphics[scale=0.3]{pasted1}

\paragraph*{\textbf{Fundamental Identities (\textit{identità fondamentali})}}
\begin{itemize}
    \item \textbf{Reciprocal Relations}:
    \[
    \tan\theta = \frac{\sin\theta}{\cos\theta}, \quad \cot\theta = \frac{\cos\theta}{\sin\theta}, \quad \sec\theta = \frac{1}{\cos\theta}, \quad \csc\theta = \frac{1}{\sin\theta}.
    \]
    \item \textbf{Extended Pythagorean Identities}:
    \[
    1 + \tan^2\theta = \sec^2\theta, \quad 1 + \cot^2\theta = \csc^2\theta.
    \]
    \item \textbf{Co-Function Identities} (\textit{identità complementari}):
    \[
    \sin\left(\frac{\pi}{2} - \theta\right) = \cos\theta, \quad \cos\left(\frac{\pi}{2} - \theta\right) = \sin\theta, \quad \tan\left(\frac{\pi}{2} - \theta\right) = \cot\theta.
    \]
\end{itemize}

\paragraph*{\textbf{Angle Addition \& Subtraction}}  
For any angles \(\alpha\) and \(\beta\):
\begin{itemize}
    \item \(\sin(\alpha \pm \beta) = \sin\alpha\cos\beta \pm \cos\alpha\sin\beta\)
    \item \(\cos(\alpha \pm \beta) = \cos\alpha\cos\beta \mp \sin\alpha\sin\beta\)
    \item \(\tan(\alpha \pm \beta) = \frac{\tan\alpha \pm \tan\beta}{1 \mp \tan\alpha\tan\beta}\)
\end{itemize}

\paragraph*{\textbf{Multiple-Angle \& Half-Angle Identities}}  
\begin{itemize}
    \item \textbf{Double-Angle}:
    \[
    \sin(2\theta) = 2\sin\theta\cos\theta, \quad \cos(2\theta) = \cos^2\theta - \sin^2\theta = 2\cos^2\theta - 1 = 1 - 2\sin^2\theta.
    \]
    \item \textbf{Triple-Angle}:
    \[
    \sin(3\theta) = 3\sin\theta - 4\sin^3\theta, \quad \cos(3\theta) = 4\cos^3\theta - 3\cos\theta.
    \]
    \item \textbf{Half-Angle} (sign depends on quadrant):
    \[
    \sin\left(\frac{\theta}{2}\right) = \pm\sqrt{\frac{1 - \cos\theta}{2}}, \quad \cos\left(\frac{\theta}{2}\right) = \pm\sqrt{\frac{1 + \cos\theta}{2}}.
    \]
\end{itemize}

\paragraph*{\textbf{Product-to-Sum \& Sum-to-Product}}  
\begin{itemize}
    \item \textbf{Product-to-Sum}:
    \[
    \sin\alpha\sin\beta = \frac{1}{2}[\cos(\alpha - \beta) - \cos(\alpha + \beta)], \quad \cos\alpha\cos\beta = \frac{1}{2}[\cos(\alpha - \beta) + \cos(\alpha + \beta)].
    \]
    \item \textbf{Sum-to-Product}:
    \[
    \sin\alpha \pm \sin\beta = 2\sin\left(\frac{\alpha \pm \beta}{2}\right)\cos\left(\frac{\alpha \mp \beta}{2}\right), \quad \cos\alpha + \cos\beta = 2\cos\left(\frac{\alpha + \beta}{2}\right)\cos\left(\frac{\alpha - \beta}{2}\right).
    \]
\end{itemize}

\paragraph*{\textbf{Triangle Relations (Laws)}}  
For any triangle with sides \(a, b, c\) opposite angles \(A, B, C\):
\begin{itemize}
    \item \textbf{Law of Sines}:
    \[
    \frac{a}{\sin A} = \frac{b}{\sin B} = \frac{c}{\sin C} = 2R \quad (R = \text{circumradius}).
    \]
    \item \textbf{Law of Cosines}:
    \[
    c^2 = a^2 + b^2 - 2ab\cos C.
    \]
    \item \textbf{Law of Tangents}:
    \[
    \frac{a - b}{a + b} = \frac{\tan\left(\frac{A - B}{2}\right)}{\tan\left(\frac{A + B}{2}\right)}.
    \]
\end{itemize}

\subsection*{Complex Numbers introduction}

The largest domain covered in these notes: $\mathbb{C}$. Complex
numbers are an extension of real numbers, defined by the imaginary
number i With this strange property where $i^{2}=-1$. They are used
to resolve polynomial equations unsolvable in real numbers, as shown
in the fundamental theorem of algebra (Might need a section on this).
Below is a complex number with iy as the imaginary component and x
as the real component.

\begin{equation}
z=x+iy
\end{equation}
 Imagine $\mathbb{R}$ covering the whole x axis, and $\mathbb{C}$
covering the whole y axis, that's the complex plane.

\subsubsection*{Operations}

Addition

\begin{equation}
z_{1}+z_{2}=(x_{1}+x_{2})+i(y_{1}+y_{2})
\end{equation}
 Subtraction

\begin{equation}
z_{1}-z_{2}=(x_{1}-x_{2})+i(y_{1}-y_{2})
\end{equation}
 Multiplication

\begin{equation}
z_{1}z_{2}=(x_{1}+iy_{1})(x_{2}+iy_{2})=(x_{1}x_{2}-y_{1}y_{2})+i(x_{1}y_{2}+x_{2}y_{1})
\end{equation}
 Complex Conjugate

\begin{equation}
\bar{z}=x-iy
\end{equation}
 Modulus

\begin{equation}
|z|=\sqrt{x^{2}+y^{2}}
\end{equation}
 Inverse

\begin{equation}
z^{-1}=\frac{\bar{z}}{|z|^{2}}=\frac{x-iy}{x^{2}+y^{2}}
\end{equation}

A image of the $\mathbb{C}$ plane for reference, showing the inverse
modulus as well:

\includegraphics[scale=0.2]{/home/guc/Pictures/Complex_conjugate_picture\lyxdot svg}

\subsection*{Polar Coordinates}

Polar coordinates are a alternative to the commonly used Cartesian
coordinate system (x,y). It is mesured using two metrics:

\subparagraph*{Radial Distance (r):}

Which is the distance from the origin which is also the hypotenuse
of the triangle.

\subparagraph*{Angular Coordinate ($\theta$):}

Which is the angle between the radial distance and the +x axis. Which
increases counterclockwise. 

\paragraph*{Basic Conversion between Cartesian and Polar}

For Cartesian to Polar it is:

\[
r=\sqrt{x^{2}+y^{2}},\theta=arctan\left(\frac{y}{x}\right)
\]
 And for Polar to Cartesian it is:

\[
x=rcos\theta,y=rsin\theta
\]


\paragraph*{Basis vectors}

Basis vectors represented in polar coordinates:

\[
\mathbf{\hat{\mathbf{r}}=}cos\theta\hat{\mathbf{i}}+sin\theta\hat{\mathbf{j}},\hat{\mathbf{\theta}}=-sin\theta\hat{\mathbf{i}}+cos\theta\hat{\mathbf{j}}
\]


\paragraph*{Polar Functions}

A polar function is represented as:

\[
r=f(\theta)
\]
 Much like a regular function it has a domain and a range, expressed
below:

Domain: $\theta\in[0,2\pi)$

Range: $r\in\mathbb{R}$

\paragraph*{Common set of Polar Functions:}

-

\includegraphics[scale=0.3]{pasted3}

\includegraphics[scale=0.3]{pasted4}

\includegraphics[scale=0.3]{pasted2}

\pagebreak{}

\section*{Sums and Sequences}

\textquotedbl La situazione è grave ma non è seria.\textquotedbl{}

\subsection*{Limits}

\paragraph*{How its represented.}

Any sequence that converges to a limit is represented as:

\[
lim_{n\rightarrow\infty}a_{n}=L
\]

Where $\left\{ a_{n}\right\} $is the sequence.

\paragraph*{Formal Definition.}

\begin{equation*}
    \lim_{n \to \infty} a_n = L \quad \iff \quad
    \forall \varepsilon > 0,\; \exists N \in \mathbb{N},\; \forall n \geq N,\; |a_n - L| < \varepsilon
\end{equation*}

``For every positive number, there exists a natural number such that,
for all integers, the distance between and L is less than \textgreek{ε}.''

\subsection*{Sequence}

\paragraph{Definition:}

A sequence/sucession is simply a list of $\mathbb{N}$ while following
a set of rules defined by a function. The function then maps each
$\mathbb{N}$ to a corresponding $\mathbb{R}$ following the rules
defined by the function. 

\paragraph*{Representation:}

It is often shown as a random letter (this case a) $a_{n}$ with n
representing the number of the term.

\subparagraph*{The first term, }

$a_{1}$ is called the initial term (termine iniziale)

\subparagraph*{The terms after the first,}

$a_{1+n}$ is called the recursive formula (formula ricorsiva)

\paragraph*{Types:}

There are 2 specific categories which will be covered in more detail.
1. Whether its bounded (limitata) or unbounded (illimitata). 2. Whether
its convergent (convergente), divergent (divergente) or oscillatory
(oscillante).

\subsection*{Bounded Successions (Successioni Limitate)}

\paragraph*{Definition:}

A bounded sequence (successione limitata), is a sequence $a_{n}$
that exists within a range such that if $b\in\mathbb{R}$, b is greater
than $a_{n}$, and there exists $c\in\mathbb{R}$ that is less than
$a_{n}$, it is a bounded sequence. A bounded sequence is may suggest
convergences and can be proven by using the Bolzano-Weierstrass Theorem.

\paragraph*{Intervals:}

\subparagraph*{Open Interval}

\[
(a,b)\coloneqq\left\{ x\in\mathbb{R}|a<x<b\right\} 
\]

A open interval includes all $\mathbb{R}$ numbers between a and b,
excluding the endpoints. Represented by a () and <

\subparagraph*{Closed Interval }

\[
[a,b]\coloneqq\left\{ x\in\mathbb{R}|a\leq x\leq b\right\} 
\]

A closed interval includes all $\mathbb{R}$ numbers between a and
b, including the endpoints. Represented by a {[}{]} and $\leq$. 

\subparagraph*{Empty Interval}

Denoted as $\emptyset$, it contains no numbers.

\subparagraph*{Degenerate Interval}

A single point, $[a,a]=\left\{ a\right\} $. By technicality it is
always closed.

\subparagraph*{Half Intervals}

It is possible to have intervals which are open at one end and closed
at the other, and vice versa. e.g

\[
(a,b]\coloneqq\left\{ x\in\mathbb{R}|a<x\leq b\right\} 
\]


\subparagraph*{Infinite Intervals}

There are also intervals which are infinite on one side and open or
closed on the other. e.g

\[
(a,+\infty)\coloneqq\left\{ x\in\mathbb{R}|a<x\right\} 
\]

The infinite can be negative as well on the other side. (Not sure
if the infinite has to be in a open interval, because I have not seen
any which are not in a open interval)

\paragraph*{Types of bounds: }

\subparagraph*{Upper bound:}

If a upper bound exists we call it bounded from above.

\subparagraph*{Lower bound:}

If a lower bound exists we call it bounded from below.

If both upper and lower bound exist the set is bounded.

\subparagraph*{Bound properties:}

There can be multiple upper and lower bound (As clearly shown in the
diagram below). 

\paragraph*{Supremum and Infimum}

The supremum and infimum can only exist for a interval with at least
one open point. Its the smallest possible upper bound (If its a supremum)
or the largest possible lower bound (If its a infimum). It can NEVER
reach the interval. The Supermum can be written as $supM$ and the
Infium can be written as $infM$

\paragraph*{Minimum and Maximum}

For a Minimum or a Maximum you must have at least one closed interval
point. (as shown in the diagram below). The minimum or maximum is
the point a or b that hold the interval. 

\paragraph*{Diagram of open and closed sequences:}

\includegraphics[scale=0.4]{/home/guc/Pictures/boundedsequences}

\subsection*{Monotone Sequences }

\paragraph*{Definition:}

A sequence that is monotone is either non-decreasing or non-increasing.
Therefore, by extention a constant sequence is simultaneously non-decreasing
and non-increasing. Therefore it is monotone. The strictly increasing/decreasing
are simply subsets.

\paragraph*{Growth of Sequences:}

\subparagraph*{Non-decreasing sequence (successioni crescenti):}

Is a sequence that increases if each term is greater than or equal
to the previous term:
\[
\forall n\in\mathbb{N},a_{n+1}\geq a_{n}
\]


\subparagraph*{Non-increasing sequences (successioni decrescenti):}

Is a sequence that it decreases if each term is less than or equal
to the previous term.
\[
\forall n\in\mathbb{N},a_{n+1}\leq a_{n}
\]


\subparagraph*{Strictly increasing sequences (successioni strettamente crescenti):}

Is a sequence that strictly increasing if each term is strictly greater
than the previous term.
\[
\forall n\in\mathbb{N},a_{n+1}>a_{n}
\]


\subparagraph*{Strictly decreasing sequences (successioni strettamente decrescenti):}

Is a sequence that strictly decreasing if each term is strictly less
than the previous term.
\[
\forall n\in\mathbb{N},a_{n+1}<a_{n}
\]


\subsection*{Convergent and Divergent Sequences}

\subsubsection*{Convergent}

A sequence is convergent if it approaches a finite limit (L) as the
n approaches infinity. Formally this can be defined:

A sequence $\left\{ a_{n}\right\} $ converges to L if 

\[
\forall\varepsilon>0,\exists N\in\mathbb{N}|\forall n\geq N,|a_{n}-L|<\varepsilon
\]


\paragraph*{Accumulation Points Link}

A convergent sequence has exactly one accumulation point which is
its limit. This is different from the divergent sequence that will
have multiple accumulation values or many improper ones.

\paragraph*{Limit sup and inf}

For convergent sequences:

\[
lim_{n\rightarrow\infty}supa_{n}=lim_{n\rightarrow\infty}infa_{n}=L
\]


\paragraph*{Note on Bounded Sequences:}

\uline{While boundedness is a requirement for a sequence to be convergent
it is not the only requirement.}

\paragraph*{Monotone Convergence Theorem:}

Every bounded monotonic sequence converges

\paragraph*{Limit properties for convergence:}

If b is a sequence that converges to M and a is a sequence that converges
to L then
\[
lim_{n\rightarrow\infty}(a_{n}\pm b_{n})=L\pm M
\]
\[
lim_{n\rightarrow\infty}(a_{n}\times b_{n})=L\times M
\]
\[
lim_{n\rightarrow\infty}\frac{a_{n}}{b_{n}}=\frac{L}{M}(M\neq0)
\]


\subsubsection*{Divergent}

A sequence is divergent if it does not converge to any limit L. There
are two primary times of divergence, divergence to infinity(may also
be refered to improper divergence) or oscillatory divergence. The
formal definitions for divergence to infinity is listed below:

\[
\forall M\in\mathbb{R},\exists N\in\mathbb{N}
\]

Such that $\forall n\geq N,a_{n}>M$, and if it diverging to negative
infinity it is $\forall n\geq N,a_{n}<M$

And for Oscillations:

\subparagraph*{Bounded Oscillations:}

Terms change between bounds with the lim sum not equalling the lim
inf

\subparagraph*{Unbounded oscillation:}

Terms grow wile changing and slowly diverging in absolute value but
don't have a directional convergence.

\paragraph*{Unbounded}

While it is not guaranteed that every bounded sequence is convergent,
it is guaranteed that every unbounded sequence is divergent by definition.
However not all divergent sequences are unbounded. 

\paragraph*{Sub sequences}

If a sub sequence of a sequence diverges to infinity then the also
sequence will diverge.

If two sub sequences converge to different limits, the original sequence
diverges.

\paragraph*{Limit properties}

If $lim_{n\rightarrow\infty}a_{n}=+\infty$ and the sequence b is
bounded below then

\[
lim_{n\rightarrow\infty}(a_{n}+b_{n})=+\infty
\]
If $lim_{n\rightarrow\infty}a_{n}=+\infty$ and $lim_{n\rightarrow\infty}b_{n}=c>0$
then

\[
lim_{n\rightarrow\infty}(a_{n}\times b_{n})=+\infty
\]


\subsection*{Cauchy Sequence}

A Cauchy sequence is a sequence whose terms become arbitrarily close
to one another as the sequence progresses, regardless of whether the
sequence converges to a specific limit.

\paragraph{Formal Definition}

Every sequence with this definition is a Cauchy Sequence

\[
\forall\epsilon>0,\exists N\in\mathbb{N}\Rightarrow\forall m,n\geq N,|a_{m}-a_{n}|<\epsilon
\]


\paragraph*{Properties}

\subparagraph{Convergence}

In R every Cauchy sequence by definition must be convergent, due to
the completeness property. 

\subparagraph*{Boundedness}

Every Cauchy Sequence is bounded.

\subparagraph*{Subsequence}

If a Cauchy sequence has a convergent subsequence, the entire sequence
converges to the same limit. 

\subparagraph*{Notes on convergence}

All convergent sequences are Cauchy, not all Cauchy sequences are
convergent IN INCOMPLETE SPACES

\subsection*{Napier's Constant (Costante di Nepero)}

This may also be known as e or exponential or exp. It has 3 separate
definitions: Limit, Series, and integral. However I will only do the
explanation for limit definition here. The integral explanation will
be done in the integral chapter.

\paragraph*{Limit Definition:}

\begin{equation}
e=lim_{n\rightarrow\infty}\left(1+\frac{1}{n}\right)^{n}
\end{equation}

\subparagraph*{Base of the Natural Logarithm} 
\( e \) is the base of the natural logarithm, denoted as \( \ln(x) \).

\subparagraph*{Approximate Value} 
\( e \approx 2.71828 \).

\subparagraph*{Irrational Number} 
\( e \) cannot be expressed as a ratio of two integers.

\subparagraph*{Transcendental Number} 
\( e \) is not a root of any non-zero polynomial with rational coefficients.

\subparagraph*{Limit Definition} 
\[
e = \lim_{n \to \infty} \left(1 + \frac{1}{n}\right)^n
\]

\subparagraph*{Euler's Identity} 
\[
e^{i\pi} + 1 = 0
\]
Connecting \( e \), imaginary numbers, and \( \pi \)

\subsubsection*{Complex Exponential (l'esponenziale Complesso)}

The complex exponential function denoted as $e^{z}$ or exp(z). The
vast majority of definitions of complex exponential are the exact
same as the regular exponential function. Including the proof of their
equivalence. There are several ways to define a complex exponential
function, and I will list them below.

\subparagraph*{Power series expansion}

\paragraph*{
\[
e^{z}=\sum_{k=0}^{\infty}\frac{z^{k}}{k!}
\]
}

\subparagraph*{Limit Definition}

\[
e^{z}=lim_{n\rightarrow\infty}\left(1+\frac{z}{n}\right)^{n}
\]


\subparagraph*{Additive Property (Kinda definition)}

\[
e^{w+z}=e^{w}e^{z}
\]


\subparagraph*{Complex log}
\begin{lyxcode}
\[
log(e^{z})=\left\{ z+2\pi ik|k\in\mathbb{Z}\right\} 
\]
\end{lyxcode}

\paragraph*{Key Properties}

\subparagraph*{Non zero}

$\frac{1}{e^{z}}=e^{-z}$ and e\textasciicircum z does not equal
zero for all C

\subparagraph*{Periodic
\[
e^{z+2\pi}=e^{z}
\]
}

\subparagraph*{Identita di Euler}

For $e^{i\theta}=cos\theta+i*sin\theta$

\[
e^{i\pi}=-1
\]


\subparagraph*{Conjugate}

$\overline{e^{z}}=e^{\overline{z}}$

\subparagraph*{Modulus}

$|e^{z}|=e^{\mathfrak{\mathbb{R}}(z)}$

\subsection*{\textcolor{black}{Handling Infinity}}

\subsubsection*{\textcolor{black}{Limit Superior}}

\textcolor{black}{For a sequence $(a_{n})$, the limit superior which
is written as }

\textcolor{black}{
\[
limsup_{n\rightarrow\infty}a_{n}
\]
 is the supremum of all the accumulation points (cluster points) of
the sequence. For example if a sequence diverges to infinity its only
accumulation point is infinity therefore }

\textcolor{black}{
\[
limsup_{n\rightarrow\infty}a_{n}=\infty
\]
 The main diffrence between the supremum is that the supremum }

\textcolor{black}{Lets say we have a sequence $(a_{n})$ which is
given by $a_{n}=n$, the limit superior of $(a_{n})$ is the largest
accumulation value of $(a_{n})$. Whether it is improper or proper
accumulation value. It is represented as:}

\textcolor{black}{
\[
a=limsup_{n\rightarrow\infty}a_{n}
\]
}

\textcolor{black}{The difference between the limit superior and superior
is that the limit superior is ALWAYS smaller than the }

\paragraph*{\textcolor{black}{Limit Inferior }}

\textcolor{black}{The limit inferior of $(a_{n})$ is the smallest
improper accumulation value of $(a_{n})$, and its represented by
the notation}

\textcolor{black}{
\[
a=liminf_{n\rightarrow\infty}a_{n}
\]
}

\paragraph*{\textcolor{black}{Accumulation Values and Divergence}}

\textcolor{black}{A value $a\in\mathbb{R}\cup\left\{ +\infty,-\infty\right\} $
is called a accumulation value of $a_{n}$, if there exists a sub
sequence $a_{n_{k}}$ of $a_{n}$ such that the limit $k\rightarrow\infty|a_{n_{k}}=a$}

\textcolor{black}{For improper cases:}

\textcolor{black}{+$\infty$ is a accumilation value if the sequences
is unbounded above, meaning for every M$\in$R infinit}

\paragraph*{\textcolor{black}{Improper accumulation values}}

\textcolor{black}{Any sequence that has no accumulation values has
at least one improper accumulation value}

\subsection*{Bolzano-Weierstrass Theorem (Teorema di Bolzano-Weierstrass)}

% BOLZANO-WEIERSTRASS THEOREM
\begin{itemize}
    \item \textbf{Statement (Teorema di Bolzano-Weierstrass):} Every bounded sequence has a convergent subsequence. Formally:
    \[
    \left( \exists K > 0 \text{ such that } \forall n \in \mathbb{N},\, |a_n| \leq K \right) \implies \exists\, \{a_{n_k}\} \subseteq \{a_n\} \text{ and } \exists L \in \mathbb{R} \text{ such that } \lim_{k \to \infty} a_{n_k} = L
    \]
    
    \item \textbf{Key requirements:}
    \begin{itemize}
        \item Bounded sequence ($\exists K > 0 : \forall n \in \mathbb{N},\, |a_n| \leq K$)
    \end{itemize}
    
    \item \textbf{Consequence:} Applies in $\mathbb{R}^n$ (via metodo degli intervalli incapsulati or Heine-Borel)
\end{itemize}

\subsection*{Theorem of Zero}

\subsection*{Trigonometric Functions and $\pi$}

\subsection*{Complex Polynomials}

\subsubsection*{Fundamental Theorem of Algebra}

\pagebreak{}

\section*{Series}

`\/`I'm not lost, I am just taking a scenic route to understand
it'\/'

\subsection*{Series (Serie)}

A series is the sum of the all the terms in a sequence. Formally,
if a is a sequence and we want the infinite sum of the series:

\[
\sum_{n=1}^{\infty}a_{n}=a_{1}+a_{2}+a_{3}+...
\]


\subsubsection*{Partial Sum (Somma Parziale)}

A series does not have to necessarily have to be the sum of infinite
terms. A partial sum, allows just the sum of the k terms:

\[
S_{k}=\sum_{n=1}^{k}a_{n}
\]

If $\sum a_{n}$ converges, then the lim

This series converges to S if lim$_{k\rightarrow\infty}S_{k}=S$ otherwise,
as mentioned in previous sections it diverges.

\subsubsection*{Necessary Condition for Convergence (Divergence Test)}
If \(\sum a_n\) converges, then \(\lim_{n \to \infty} a_n = 0\). \\
Equivalently, if \(\lim_{n \to \infty} a_n \neq 0\) (or the limit does not exist), the series diverges.

\subsection*{Types of Series}

\subsubsection*{Geometric (Serie Geometrica)}

It is a series with the standard form $\sum_{n=0}^{\infty}ar^{n}$
where r is the common ratio. There are 3 primary types of geometric
series:

\subparagraph*{Partial sum (Finite geometric series)}

For a finite number k, the partial sum 
\[
S_{k}=\sum_{n=0}^{n-1}ar^{n}=a+ar+ar^{2}+...+ar^{n-1}
\]

Formula
\[
S_{k}=a\times\frac{1-r^{n}}{1-r}
\]

valid only if $r\cancel{=}1$. If r=1 then

\[
S_{n}=a\times n
\]

There is also a alternate notation for the partial sum formula that
is pretty commonly used.

\[
S_{n+1}=\sum_{k=0}^{n}ar^{k}=\frac{a(1-r^{n+1})}{1-r}
\]


\subparagraph*{Infinite geometric series}

Theorem:

if $\sum_{n=0}^{\infty}ar^{n}$ converges then $|r|<1$ and
\[
\sum_{n=0}^{\infty}ar^{n}=\frac{a}{1-r}
\]


\subparagraph*{Divergence}

The series is divergent if $|r|\geq1,$ and the terms do not approach
zero

\subsubsection*{Telescoping (Serie Telescopica)}

A telescopic series is a series where a lot of the terms cancel out
when written in partial sums
\[
\sum_{n=1}^{\infty}(a_{n}-a_{n+1})
\]

whose partial sum will telescope to
\[
S_{k}=\sum_{n=1}^{k}(a_{n}-a_{n+1})=a_{1}-a_{k+1}
\]

When handling a limit to L
\[
\sum_{n=1}^{\infty}(a_{n}-a_{n+1})=a_{1}-L
\]

\textbf{Note:} This requires \(\lim_{n \to \infty} a_n = L\). \\
\textbf{Example:} \(\sum_{n=1}^{\infty} \frac{1}{n(n+1)} = \sum_{n=1}^{\infty} \left( \frac{1}{n} - \frac{1}{n+1} \right)\) converges to 1.

\subsubsection*{Harmonic Series (Serie armonica)}

A harmonic series is a series that diverges despite its terms tending
to zero, it is represented as:

\[
\sum_{n=1}^{\infty}\frac{1}{n}
\]

A small proof is listed below:

\[
1+\frac{1}{2}+\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+...+\frac{1}{8}\right)+...\geq1+\frac{1}{2}+2\left(\frac{1}{4}\right)+4\left(\frac{1}{8}\right)+...=\sum_{k=0}^{\infty}\frac{1}{2}=\infty
\]

This shows that $a_{n}\rightarrow0$ is necessary for convergence,
but not sufficient to prove it.

\paragraph*{P series}

Harmonic series are a type of p series. P series is often used in
comparison to check if series converge or not. It is represented as

\[
\sum_{n=1}^{\infty}\frac{1}{n^{p}}
\]
 where it is said that if \(p \leq 1\) the series diverges, and if \(p > 1\) the series \textbf{converges}.

\subsubsection*{Associativity of series (Associatività della somma di serie)}

The associativity of series, or the property that grouping terms in
different ways does not affect the sum, holds for convergent series
but not necessarily for divergent ones. 

\subsubsection*{Series with varying signs (Serie a segno variabile)}

% POWER SERIES
\paragraph*{\textbf{Power Series (Serie di potenze)}}
\begin{itemize}
    \item \textbf{Definition:} A series of the form:
    \[
    \sum_{n=0}^{\infty} c_n (x - a)^n = c_0 + c_1(x-a) + c_2(x-a)^2 + \cdots
    \]
    where $c_n$ are coefficients, $a$ is the center, and $x$ is the variable.
    
    \item \textbf{Radius of Convergence (R):} Determined by:
    \[
    R = \frac{1}{L} \quad \text{where} \quad L = \limsup_{n \to \infty} \sqrt[n]{|c_n|}
    \]
    (Cauchy-Hadamard formula) or when it exists:
    \[
    R = \lim_{n \to \infty} \left| \frac{c_n}{c_{n+1}} \right|
    \]
    
    \item \textbf{Interval of Convergence:} The set $x \in (a - R, a + R)$ where:
    \begin{itemize}
        \item Converges absolutely for $|x - a| < R$
        \item May converge or diverge at endpoints $x = a \pm R$
        \item Diverges for $|x - a| > R$
    \end{itemize}
    
    \item \textbf{Properties:}
    \begin{itemize}
        \item \textbf{Continuity:} Continuous on $(a - R, a + R)$
        \item \textbf{Differentiation:} Can differentiate term-by-term:
        \[
        f'(x) = \sum_{n=1}^{\infty} n c_n (x - a)^{n-1}
        \]
        with same radius $R$
        \item \textbf{Integration:} Can integrate term-by-term:
        \[
        \int f(x)  dx = C + \sum_{n=0}^{\infty} \frac{c_n}{n+1} (x - a)^{n+1}
        \]
        with same radius $R$
    \end{itemize}
    
    \item \textbf{Examples:}
    \begin{itemize}
        \item Exponential: $\displaystyle e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} \quad (R = \infty)$
        \item Geometric: $\displaystyle \frac{1}{1-x} = \sum_{n=0}^{\infty} x^n \quad (R = 1)$
        \item Arctangent: $\displaystyle \arctan x = \sum_{n=0}^{\infty} (-1)^n \frac{x^{2n+1}}{2n+1} \quad (R = 1)$
    \end{itemize}
    
    \item \textbf{Important Theorem:} If $\sum c_n (x-a)^n = 0$ for all $x$ in some interval, then $c_n = 0$ for all $n$.
\end{itemize}

% LINEARITY OF SERIES SUM
\paragraph*{\textbf{Linearity of Series Sum (Teorema linearità somma)}}
\begin{itemize}
    \item \textbf{Statement:} Given two convergent series $\sum_{n=1}^{\infty} a_n = A$ and $\sum_{n=1}^{\infty} b_n = B$, and constants $\alpha, \beta \in \mathbb{R}$, then:
    \[
    \sum_{n=1}^{\infty} (\alpha a_n + \beta b_n) = \alpha A + \beta B
    \]
    
    \item \textbf{Proof Outline:}
    \begin{itemize}
        \item Let $s_N = \sum_{n=1}^{N} a_n \to A$ and $t_N = \sum_{n=1}^{N} b_n \to B$
        \item The partial sum of the combined series:
        \[
        \sigma_N = \sum_{n=1}^{N} (\alpha a_n + \beta b_n) = \alpha s_N + \beta t_N
        \]
        \item By limit laws:
        \[
        \lim_{N \to \infty} \sigma_N = \alpha \lim_{N \to \infty} s_N + \beta \lim_{N \to \infty} t_N = \alpha A + \beta B
        \]
    \end{itemize}
    
    \item \textbf{Key Requirements:}
    \begin{itemize}
        \item Both series must converge individually
        \item Linearity fails for divergent series (counterexample: $a_n = (-1)^n$, $b_n = (-1)^{n+1}$)
    \end{itemize}
    
    \item \textbf{Extension:} For finitely many convergent series $\sum a_n^{(k)} = S_k$ and constants $c_k$:
    \[
    \sum_{n=1}^{\infty} \left( \sum_{k=1}^{m} c_k a_n^{(k)} \right) = \sum_{k=1}^{m} c_k S_k
    \]
    
    \item \textbf{Warning:} Does \emph{not} apply to conditionally convergent series rearrangements (Riemann series theorem).
\end{itemize}

\paragraph*{\textbf{Cesàro Summability (Convergenza alla Cesàro)}}
\begin{itemize}
    \item \textbf{Definition:} A sequence $(a_n)$ is Cesàro summable to limit $L$ if the arithmetic mean of its first $n$ partial sums converges to $L$:
    \[
    \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n s_k = L
    \]
    where $s_k = a_1 + a_2 + \cdots + a_k$ are the partial sums.
    
    \item \textbf{Equivalent Form:} For the sequence $(a_n)$ itself:
    \[
    \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n a_k = L
    \]
    \item \textbf{Key Properties:}
    \begin{itemize}
        \item \textbf{Regularity:} If $\lim_{n \to \infty} a_n = L$ exists, then $(a_n)$ is Cesàro summable to $L$
        \item \textbf{Strictly Weaker:} Cesàro summability doesn't imply convergence (e.g., $a_n = (-1)^n$)
        \item \textbf{Linearity:} If $a_n \to_{\text{C}} A$ and $b_n \to_{\text{C}} B$, then:
        \[
        \alpha a_n + \beta b_n \to_{\text{C}} \alpha A + \beta B
        \]
    \end{itemize}
    
    \item \textbf{Example (Divergent Sequence):} Consider $a_n = (-1)^{n+1}$:
    \[
    \text{Sequence: } 1, -1, 1, -1, 1, -1, \dots
    \]
    \[
    \text{Partial sums: } s_n = \begin{cases} 
    1 & n \text{ odd} \\
    0 & n \text{ even}
    \end{cases}
    \]
    \[
    \text{Cesàro mean: } \sigma_n = \frac{1}{n} \sum_{k=1}^n s_k \to \frac{1}{2}
    \]
    Thus $(a_n)$ is Cesàro summable to $1/2$ despite divergence.
\end{itemize}

% WEIERSTRASS M-TEST FOR SERIES
\paragraph*{\textbf{Weierstrass M-Test (Criterio M di Weierstrass)}}
\begin{itemize}
    \item \textbf{Statement:} Let $\{f_n\}$ be a sequence of functions on $E \subseteq \mathbb{R}$. If $\exists \{M_n\} \subset \mathbb{R}$ such that:
    \[
    \text{(i) } |f_n(x)| \leq M_n \quad \forall x \in E,  \quad \forall n \in \mathbb{N}
    \]
    \[
    \text{(ii) } \sum_{n=1}^{\infty} M_n < \infty
    \]
    Then $\sum_{n=1}^{\infty} f_n$ converges uniformly on $E$.
    
    \item \textbf{Key requirements:}
    \begin{itemize}
        \item Dominating series $\sum M_n$ must converge (absolutely)
        \item $M_n$ independent of $x \in E$
        \item $M_n \geq 0$ (non-negative majorants)
    \end{itemize}
    
    \item \textbf{Consequences:}
    \begin{itemize}
        \item Preserves continuity: If $f_n \in \mathcal{C}(E)$, then $\sum f_n \in \mathcal{C}(E)$
        \item Term-by-term integration:
        \[
        \int_a^b \sum_{n=1}^{\infty} f_n(x)  dx = \sum_{n=1}^{\infty} \int_a^b f_n(x)  dx
        \]
        \item Term-by-term differentiation (requires additional convergence of $\sum f_n'$)
    \end{itemize}
\end{itemize}

\paragraph*{\textbf{Special Case: Weierstrass Factorization}}
\begin{itemize}
    \item \textbf{Statement:} Every entire function $f$ can be represented as:
    \[
    f(z) = z^m e^{g(z)} \prod_{n=1}^{\infty} E_p\left(\frac{z}{a_n}\right)
    \]
    where $a_n$ are non-zero zeros, $m$ is zero multiplicity at origin, $g$ entire, and $E_p$ are elementary factors.
\end{itemize}

% LINEARITY OF SERIES SUM
\paragraph*{\textbf{Linearity of Series Sum (Teorema linearità somma)}}
\begin{itemize}
    \item \textbf{Statement:} Given two convergent series $\sum_{n=1}^{\infty} a_n = A$ and $\sum_{n=1}^{\infty} b_n = B$, and constants $\alpha, \beta \in \mathbb{R}$, then:
    \[
    \sum_{n=1}^{\infty} (\alpha a_n + \beta b_n) = \alpha A + \beta B
    \]
    
    \item \textbf{Proof Outline:}
    \begin{itemize}
        \item Let $s_N = \sum_{n=1}^{N} a_n \to A$ and $t_N = \sum_{n=1}^{N} b_n \to B$
        \item The partial sum of the combined series:
        \[
        \sigma_N = \sum_{n=1}^{N} (\alpha a_n + \beta b_n) = \alpha s_N + \beta t_N
        \]
        \item By limit laws:
        \[
        \lim_{N \to \infty} \sigma_N = \alpha \lim_{N \to \infty} s_N + \beta \lim_{N \to \infty} t_N = \alpha A + \beta B
        \]
    \end{itemize}
    
    \item \textbf{Key Requirements:}
    \begin{itemize}
        \item Both series must converge individually
        \item Linearity fails for divergent series (counterexample: $a_n = (-1)^n$, $b_n = (-1)^{n+1}$)
    \end{itemize}
    
    \item \textbf{Extension:} For finitely many convergent series $\sum a_n^{(k)} = S_k$ and constants $c_k$:
    \[
    \sum_{n=1}^{\infty} \left( \sum_{k=1}^{m} c_k a_n^{(k)} \right) = \sum_{k=1}^{m} c_k S_k
    \]
    
    \item \textbf{Warning:} Does \emph{not} apply to conditionally convergent series rearrangements (Riemann series theorem).
\end{itemize}

\subsection*{\sout{Cauchy Criterion}}

\sout{A series a converges if and and only if for every epsilon that
is grater than zero there exists a N such that for all \mbox{$m>n\geq N$}}

\subsubsection*{\\
\sout{\parbox{\linewidth}{
\[
|\sum_{k=n+1}^{m}a_{k}|<\epsilon
\]
}\\
}}

\subsubsection*{\sout{Absolute and Conditional Convergence}}

\subsubsection*{\sout{Using Cauchy criterion in problems}}

\paragraph*{\sout{For a Sequence}}

\sout{There are 4 main steps:}

\sout{1. Assign a epsilon that is grater than zero}

\sout{2. Find a integer N such that for all m, n > N, the inequality
\mbox{$|a_{m}-a_{n}|<\epsilon$} holds.}

\paragraph*{\sout{For a Series}}

\sout{The steps are very similar to a sequence}

\sout{1. Assign a epsilon that is grater than zero}

\sout{2. Find a integer N such that for all m>n>N, the inequality
\mbox{$|\sum_{k=n+1}^{m}a_{k}|<\epsilon$}}

\subsection*{Handling Convergence in Problems}

\subsubsection*{Comparison (Criterio del confronto)}

To find convergence using comparison, there are two primary methods
used. Direct comparison test and the Limit comparison test. 

\subparagraph*{Direct Comparison Test}

For two series $\sum a_{n}$ and $\sum b_{n}$ with $0\leq a_{n}\leq b_{n}$
for all $n\geq N$

If $\sum b_{n}$ converges then $\sum a_{n}$ will converge

If $\sum a_{n}$ diverges then $\sum b_{n}$ will diverge

\subparagraph*{Limit Comparison Test}

For two series $\sum a_{n}$ and $\sum b_{n}$ with $a_{n}>0,b_{n}>0$
\[
L=lim_{n\rightarrow\infty}\frac{a_{n}}{b_{n}}
\]

If L>0 $\sum a_{n}$ and $\sum b_{n}$ share the same convergence
type

If L=0 and $\sum b_{n}$ converges then $\sum a_{n}$ also converges

If L=$\infty$ and $\sum b_{n}$ diverges, then $\sum a_{n}$ also
diverges

\subsubsection*{Ratio (Criterio del rapporto)}

A series $\sum a_{n}$, calculate the limit
\[
L=lim_{n\rightarrow\infty}|\frac{a_{n+1}}{a_{n}}|
\]

If L < 1 the series converges

If L > 1 the series diverges

if L = 1 this test doesn't give enough info do determine a result

\subsubsection*{Root (Criterio della radice)}

A series $\sum a_{n}$, calculate the limit
\[
L=lim_{n\rightarrow\infty}\sqrt[n]{|a_{n}|}
\]

If L < 1 the series converges

If L > 1 the series diverges

if L = 1 this test doesn't give enough info do determine a result

\subsubsection*{Integral (Criterio del integrale)}

A series $\sum_{n=N}^{\infty}a_{n}$ with continuous, positive and
decreasing terms, $a_{n}=f(n)$ for $n\geq N$

If the integral $\int_{N}^{\infty}f(x)dx$ converges, then the series
$\sum a_{n}$ converges

If the integral diverges then $\sum a_{n}$ diverge

\subsection*{Cauchy Condensation Test (it is another way to say the same thing as above, I may remove this)}
If \(a_n \geq 0\) and \(a_n\) is decreasing, then \(\sum_{n=1}^{\infty} a_n\) converges iff \(\sum_{k=1}^{\infty} 2^k a_{2^k}\) converges.

\subsubsection*{\subsubsection*{Alternating Series Test (Leibniz)}
An alternating series \(\sum (-1)^n b_n\) or \(\sum (-1)^{n+1} b_n\) converges if:
\begin{enumerate}
    \item \(b_n \geq 0\) and \textbf{monotonically decreasing}: \(b_{n+1} \leq b_n\)
    \item \(\lim_{n \to \infty} b_n = 0\)
\end{enumerate}
This implies \textbf{conditional convergence} (converges but not absolutely).}

\newpage{}

\part*{Post-Derivatives}

\section*{Differential Quotient/Derivative }

``The Difference Between the Almost Right Word and the Right Word
Is Really a Large Matter---’Tis the Difference Between the Lightning
Bug and the Lightning'' - Mark Twain

\subsection*{Fundamental Formula for derivatives}

The fundamental goal of a derivative is to measure a functions behaviour
at a single point. For a function to be differentiable, it must be
continuous \& ....

\paragraph*{Secant (Retta Secante)}

Take a function $f:\mathbb{R\rightarrow\mathbb{R}}$, and pick two
points on its graph, called x and x1. The direct line that connects
x and x1 is called the secant. It is calculated like so:

\[
\frac{f(x)-f(x_{1})}{x-x_{1}}
\]

Consider that the denominator is the horizontal difference, and the
nominator is the vertical difference. The secant is useful for describing
how a function changes over two set distances, but it cannot be used
to show more precise info regarding the graph. 

\paragraph*{Tangent (Retta Tangente)}

The tangent is almost identical in concept to the secant, with one
key difference. The tangent applies a limit from $x\rightarrow x_{1}$.
So as x moves closer to x1, eventually using a limit they show the
functions behaviour at a single point. More formally its calculated
like so:

\[
lim_{x\rightarrow x_{1}}\frac{f(x)-f(x_{1})}{x-x_{1}}
\]

If this limit does not exist then the function is `'non differentiable'\/',
if instead the limit does exists it defines the slope of the tangent
at the point x. 

\paragraph*{Formal Definition}

The differential quotient is more formally represented as:

\[
f'(x)=lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}
\]

Where h:

\[
h=x-x_{1}
\]

which means:

\[
f'(x)=lim_{x\rightarrow x_{1}}\frac{f(x)-f(x_{1})}{x-x_{1}}
\]


\subsubsection*{Differentiablility implies continuity (Property)}

\[
f(x)=f(x_{1})+f'(x_{1})(x-x_{1})+(x-x_{1})r(x)
\]

where r(x) -> 0 since x -> x1, taking the limits:

\[
lim_{x\rightarrow x_{1}}f(x)=f(x_{1})+0+0=f(x_{1})
\]

$\therefore$f is continuous at x1

WARNING; THE REVERSE IS NOT TRUE, not all functions that are continuous
are differentiable.

\subsection*{Methods for Differentiation}

\paragraph*{Sum Rule
\begin{equation}
\frac{d}{dx}\left(f+g\right)=f'\left(x\right)+g'\left(x\right)
\end{equation}
}

\paragraph*{Product rule}

\begin{equation}
\frac{d}{dx}\left[f(x)g(x)\right]=f'(x)g(x)+f(x)g'(x)
\end{equation}


\paragraph{Quotient Rule}

\begin{equation}
\frac{d}{dx}\left(\frac{f(x)}{g(x)}\right)=\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^{2}}
\end{equation}


\paragraph*{Chain Rule}

\begin{equation}
\frac{d}{dx}f(g(x))=f'(g(x))g'(x)
\end{equation}


\subsection*{Inverse derivatives}

\subsection*{Local Extreme and Rolle’s Theorem}

If a function $f:[a,b]\in\mathbb{R}$ is defined on a closed interval
{[}a,b{]}, we can identify four distinct features: local maximum,
local minimum, global maximum, and global minimum. The local minimum
and local maximum are points within {[}a,b{]} where the function reaches
its lowest or highest value in a small neighborhood around them. They
can be found at tangent = 0, the derivative is undefined, at endpoints
like a,b. The global maximum and global minimum are the absolute highest
and lowest points on the entire interval {[}a,b{]}. These always exist
for continuous functions on closed intervals. 

\subsubsection*{Rolle's Theorem}

Rolle's theorem states that, `'If any differentiable function crosses
the same point on the y axis more than once, there must always be
at least point where the tangent is zero, this point is often known
as a stationary point or as $\hat{x}$. More formally it is defined
as:

$f:\left[a,b\right]\rightarrow\mathbb{R}$is continuous and differentiable
on (a,b) and f(a)=f(b), then there exists $\hat{x}\in(a,b)$ with
$f'(x)=0$

\subparagraph*{Rolle’s Theorem fails for non-differentiable functions}

\subsection*{Mean Value Theorem}

The mean value theorem involves two points on the same function f.
The mean value theorem states, that by taking the secant of those
two points (a,b) you can calculate what the tangent of a point on
the same function and in between a and b. See image below as reference.

\includegraphics[scale=0.5]{/home/guc/Downloads/Mittelwertsatz3}\footnote{By Who2010 - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=51081991}

\paragraph*{Uses:}

Assume that $f:\left[a,b\right]\rightarrow\mathbb{R}$ be differentiable

\subparagraph*{Strictly Monotonically Increasing}

If $f'\left(x\right)>0$ for all $x\in[a,b]$ : then $\hat{x}\in(x_{1},x_{2})$
with $f'(\hat{x})=\frac{f(x_{2})-f(x_{1})}{x_{2}-x_{1}}$

Rewritten then 
\[
f(x_{2})-f(x_{1})=\underset{>0}{\underbrace{f'(\hat{x})}}\cdot\underset{>0}{\underbrace{(x_{2}-x_{1})}}>0
\]

This shows it is strictly monotonically increasing

\subparagraph*{Strictly Monotonically Decreasing}

If $f'\left(x\right)<0$ for all $x\in[a,b]$

\marginpar{Follow similar steps as above to show <0}

This shows it is strictly monotonically decreasing.

\subparagraph*{Monotonically Increasing}

If $f'\left(x\right)\geq0$ for all $x\in[a,b]$

\marginpar{Follow similar steps as above to show >= 0}

This shows monotonically increasing

\subparagraph*{Monotonically Decreasing}

If $f'\left(x\right)\leq0$ for all $x\in[a,b]$

\marginpar{Follow similar steps as above to show =< 0}

This shows monotonically decreasing

\paragraph*{Auxiliary Function for proof}

\[
h(x)=f(x)-\left[f(a)+\underset{slope}{\underbrace{\frac{f(b)-f(a)}{b-a}}}(x-a)\right]
\]

f(a): anchors the secant line to a

(x-a): represents horizontal displacement 

\subsubsection*{Extended Mean Value Theorem}

There are two functions $f,g:[a,b]\rightarrow\mathbb{R}$ which need
to be differentiable and $g'(x)\neq0$ for all $x\in(a,b)$. Then
there exists $\hat{x}\in(a,b)$ 

Giving the following formula:

\[
\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f'(\hat{x})}{g'(\hat{x})}
\]


\paragraph*{EMVT auxiliary function}

\[
h(x)=f(x)-\left[f(a)+\underset{Slope(k)}{\underbrace{\frac{f(b)-f(a)}{g(b)-g(a)}}}(g(x)-g(a))\right]
\]

f(a): anchors the secant line

(g(x)-g(a)): replaces (x-a) with displacement measured by g

\[
\]


\subsection*{L'Hôpital's rule}

Is a theorem, which is primarily used to check limits by comparing
the derivatives of the limits. 

\paragraph*{Definition}

f and g are functions which are differentiable on a open interval.
Assuming that,

$lim_{x\rightarrow a}f(x)=lim_{x\rightarrow a}g(x)=0$, $g'(x)\neq0$
and $lim_{x\rightarrow a}\frac{f'(x)}{g'(x)}$

Then:

\[
lim_{x\rightarrow a}\frac{f(x)}{g(x)}=lim_{x\rightarrow a}\frac{f'(x)}{g'(x)}
\]

\subsubsection*{Higher-Order Derivatives}
The $n$-th derivative is defined recursively:
\[
f^{(n)}(x) = \frac{d}{dx}\left[f^{(n-1)}(x)\right]
\]

Other notations include

\[
f^{n}=\frac{d^{n}f}{dx^{n}}=\frac{d^{n}}{dx^{n}}f
\]


\paragraph*{Min max test}

Assuming that $f'(x_{0})=0$ and that $f''(x_{0})$ exists then

\subparagraph*{Local Minimum}

$f''(x_{0})>0$ Local min at x0 

\subparagraph*{Local Maximum}

$f''(x_{0})<0$ Local max at x0

\marginpar{If $f''(x_{0})=0$ the test fails. Note: Any derivative higher than
2 is valid as well.}

 \subsection*{Taylor's Theorem (Teorema di Taylor)}
  Taylor's theorem provides a way to approximate a function $f$ with a polynomial
  near a point $x_0$, called the expansion point (punto di espansione). For a
  function that is differentiable $(n+1)$ times on an interval $I$, its value at $x
  = x_0 + h$ can be written as:
  \[
  f(x_0 + h) = \underbrace{\sum_{k=0}^{n} \frac{f^{(k)}(x_0)}{k!} h^k}_{T_n(h)} +
  \underbrace{R_n(h)}_{\text{remainder (resto)}}
  \]
  $T_n(h)$ is the $n$-th order Taylor polynomial (polinomio di Taylor), our
  approximation of the function. The term $R_n(h)$ is the remainder, which is the
  error of the approximation, given by:
  \[
  R_n(h) = \frac{f^{(n+1)}(c)}{(n+1)!} h^{n+1}
  \]
  where $c$ is a point between $x_0$ and $x_0 + h$. The theorem can also be
  expressed in terms of $x$:
  \[
  f(x) = \sum_{k=0}^{n} \frac{f^{(k)}(x_0)}{k!} (x - x_0)^k + R_n(x), \quad
  \text{with } R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!} (x - x_0)^{n+1}
  \]


  \subsection*{Key Properties}
  The Taylor polynomial $T_n(h)$ is the unique polynomial of its degree that matches
  the function's value and its first $n$ derivatives at $x_0$. This is expressed as
  $T_n^{(k)}(0) = f^{(k)}(x_0)$ for $k = 0, 1, \dots, n$.


  The remainder $R_n(h)$ becomes very small as $h$ approaches $0$, ensuring the
  approximation grows more accurate the closer we are to $x_0$. Specifically, the
  error decreases faster than $h^n$.


  To estimate the error, we can set a bound. If the absolute value of the $(n+1)$-th
  derivative, $\left|f^{(n+1)}(z)\right|$, is less than or equal to a constant $M$
  for all $z$ between $x_0$ and $x$, then the remainder is bounded by:
  \[
  |R_n(x)| \leq \frac{M}{(n+1)!} |x - x_0|^{n+1}.
  \]


  \subsection*{Application Procedure}
  To approximate $f(x)$ near $x_0$, the process is straightforward. First, compute
  derivatives (calcolare derivate) of $f$ at $x_0$ up to the $n$-th order. Second,
  construct $T_n$ by assembling the polynomial:
  \[
  T_n(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2!}(x - x_0)^2 + \cdots +
  \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n.
  \]
  Third, bound the remainder to check the approximation's accuracy. Find a value $M$
  that is greater than or equal to $\left|f^{(n+1)}(z)\right|$ for all $z$ between
  $x_0$ and $x$. The error is then given by:
  \[
  |f(x) - T_n(x)| \leq \frac{M}{(n+1)!}|x - x_0|^{n+1}.
  \]

\subsubsection*{Exponential/Logarithmic Derivatives}
\begin{itemize}
    \item $\frac{d}{dx}[e^x] = e^x$
    \item $\frac{d}{dx}[a^x] = a^x \ln a$
    \item $\frac{d}{dx}[\ln x] = \frac{1}{x}$
    \item $\frac{d}{dx}[\log_a x] = \frac{1}{x \ln a}$
\end{itemize}

\subsubsection*{Trigonometric Derivatives}
\begin{itemize}
    \item $\frac{d}{dx}[\sin x] = \cos x$
    \item $\frac{d}{dx}[\cos x] = -\sin x$
    \item $\frac{d}{dx}[\tan x] = \sec^2 x$
    \item $\frac{d}{dx}[\cot x] = -\csc^2 x$
    \item $\frac{d}{dx}[\sec x] = \sec x \tan x$
    \item $\frac{d}{dx}[\csc x] = -\csc x \cot x$
\end{itemize}

\subsubsection*{Inverse Trig Derivatives}
\begin{itemize}
    \item $\frac{d}{dx}[\sin^{-1} x] = \frac{1}{\sqrt{1 - x^2}}$
    \item $\frac{d}{dx}[\cos^{-1} x] = -\frac{1}{\sqrt{1 - x^2}}$
    \item $\frac{d}{dx}[\tan^{-1} x] = \frac{1}{1 + x^2}$
    \item $\frac{d}{dx}[\cot^{-1} x] = -\frac{1}{1 + x^2}$
    \item $\frac{d}{dx}[\sec^{-1} x] = \frac{1}{|x|\sqrt{x^2 - 1}}$
    \item $\frac{d}{dx}[\csc^{-1} x] = -\frac{1}{|x|\sqrt{x^2 - 1}}$
\end{itemize}

\subsubsection*{Implicit Differentiation}
Example for $x^2 + y^2 = 1$:
\[
2x + 2y \frac{dy}{dx} = 0 \Rightarrow \frac{dy}{dx} = -\frac{x}{y}
\]

\subsubsection*{Parametric Derivatives}
For $x = x(t)$, $y = y(t)$:
\[
\frac{dy}{dx} = \frac{dy/dt}{dx/dt}, \quad \frac{d^2y}{dx^2} = \frac{d/dt(dy/dx)}{dx/dt}
\]

\subsubsection*{Hyperbolic Derivatives}
\begin{itemize}
    \item $\frac{d}{dx}[\sinh x] = \cosh x$
    \item $\frac{d}{dx}[\cosh x] = \sinh x$
    \item $\frac{d}{dx}[\tanh x] = \text{sech}^2 x$
\end{itemize}

\subsubsection*{Inverse Function Derivative}
If $f$ and $g$ are inverses:
\[
g'(x) = \frac{1}{f'(g(x))}
\]

\pagebreak{}

\section*{Integrals}

“If you're gonna shoot an elephant Mr. Schneider, you better be prepared
to finish the job.”

\LyXbar{} Gary Larson, The Far Side 

\subsection*{The Riemann Integral (Integrale di Riemann)}


  The Riemann integral represents the signed area between a function's graph and the x-axis on an interval \([a, b]\). It is calculated by partitioning the interval and
  summing the areas of rectangles, known as a step function (funzione a gradino).
  \[
  \int_a^b \phi(x) \,dx := \sum_{j=1}^{n} c_j \cdot (x_j - x_{j-1})
  \]


  \subsubsection*{General Riemann Integral}

  For a general function \(f\), the integral is found by approximating it with step functions. A function is Riemann integrable if the approximations from above and below
  meet at the same value. Continuous and monotonic functions are always Riemann integrable.

  \subsubsection*{Properties of the Riemann Integral}


  The integral is a linear and monotonic map.


  For a point \(c\) between \(a\) and \(b\), an integral can be split:
  \[
  \int_a^b f(x) \,dx = \int_a^c f(x) \,dx + \int_c^b f(x) \,dx
  \]
  By convention, reversing the integration bounds negates the value:
  \[
  \int_a^b f(x) \,dx = - \int_b^a f(x) \,dx
  \]


  \subsubsection*{Mean Value Theorem for Integration (Teorema della media integrale)}


  This theorem states that for a continuous function \(f\) on \([a, b]\), there exists a point \(x_{hat} \in [a, b]\) where the function's value is the average value over
  the interval. This means the area under the curve is equal to the area of a rectangle with height \(f(x_{hat})\). This concept is a key step in proving the Fundamental
  Theorem.
  \[
  \int_a^b f(x) \,dx = f(x_{hat}) \cdot (b-a)
  \]


  \subsubsection*{The Fundamental Theorem of Calculus (Teorema fondamentale del calcolo integrale)}

  This theorem connects differentiation and integration, showing they are inverse operations. First, an antiderivative (primitiva) is a function \(F\) whose derivative is
  \(f\), so \(F' = f\).


  \textbf{First Fundamental Theorem of Calculus}: This part shows that the derivative of an integral function is the original function.
  \[
  \frac{d}{dx} \int_a^x f(t) \,dt = f(x)
  \]


  \textbf{Second Fundamental Theorem of Calculus}: This part provides a method to calculate definite integrals using an antiderivative \(F\).
  \[
  \int_a^b f(x) \,dx = F(b) - F(a)
  \]

\subsection*{Integration Rules}

  \subsubsection*{Integration by Substitution (Integrazione per sostituzione)}


  This rule, also known as change of variables (cambiamento di variabili), simplifies integrals of composite functions and is the integral counterpart to the chain rule.
  For a continuous function \(f\) and a continuously differentiable function \(\phi\), the substitution \(x = \phi(t)\) transforms the integral:
  \[
  \int_a^b f(\phi(t)) \phi'(t) \,dt = \int_{\phi(a)}^{\phi(b)} f(x) \,dx
  \]
  The relation \(dx = \phi'(t) \,dt\) is used to change the differential, and the limits of integration are updated from \([a, b]\) to \([\phi(a), \phi(b)]\).


  When applying the rule in reverse, the function \(\phi\) must be invertible (bijective). The substitution \(x = \phi(t)\) transforms the integral as follows:
  \[
  \int_a^b f(x) \,dx = \int_{\phi^{-1}(a)}^{\phi^{-1}(b)} f(\phi(t)) \phi'(t) \,dt
  \]
  Here, the new integration bounds are found using the inverse function, \(\phi^{-1}\).

\subsection*{Partial Fraction Decomposition (Decomposizione di fratti semplici)}

Given a rational function $f(x) = \frac{P(x)}{Q(x)}$ where the degree of $P(x)$ is strictly less than the degree of $Q(x)$. The decomposition of $f(x)$ depends on the nature of the roots of the denominator $Q(x)$.

\subsubsection*{Distinct Real Roots}

This case applies when the denominator $Q(x)$ of degree $n$ has $n$ distinct real roots, denoted $x_1, x_2, \dots, x_n$. The denominator can be factored as:
$$ Q(x) = (x - x_1)(x - x_2)\dots(x - x_n) $$
The partial fraction decomposition is a sum of $n$ fractions:
$$ \frac{P(x)}{Q(x)} = \frac{A_1}{x - x_1} + \frac{A_2}{x - x_2} + \dots + \frac{A_n}{x - x_n} $$
The coefficients $A_1, A_2, \dots, A_n$ are real constants that must be determined.

\subsubsection*{Repeated Real Roots}

This case applies when $Q(x)$ has fewer than $n$ distinct real roots, meaning at least one root has a multiplicity greater than 1. Let the distinct roots be $x_1, \dots, x_k$ with corresponding multiplicities $\alpha_1, \dots, \alpha_k$, such that $\sum_{i=1}^{k} \alpha_i = n$.
A root $x_i$ with multiplicity $\alpha_i$ contributes $\alpha_i$ terms to the decomposition:
$$ \frac{A_{i,1}}{x - x_i} + \frac{A_{i,2}}{(x - x_i)^2} + \dots + \frac{A_{i,\alpha_i}}{(x - x_i)^{\alpha_i}} $$
The full decomposition is the sum of these groups of terms for all distinct roots.

\subsubsection*{Complex Roots}

This case applies when $Q(x)$ has at least one pair of complex conjugate roots. The procedure is analogous to the real root cases, but the roots $x_i$ and coefficients $A_i$ can be complex numbers. The calculations follow the same algebraic rules. For real-valued rational functions, the final expression can be simplified to avoid imaginary units by combining conjugate terms.

\paragraph*{Note on Finding Coefficients}

To find the unknown coefficients (the $A_i$ values), one typically uses the method of equating coefficients. After setting up the decomposition and multiplying both sides by the original denominator $Q(x)$, an identity between two polynomials is formed.
$$ P(x) = \text{Polynomial in } x \text{ with coefficients as functions of } A_i $$
For this identity to hold for all $x$, the coefficients of corresponding powers of $x$ on both sides of the equation must be equal. This creates a system of linear equations which can be solved for the unknown coefficients.

  \subsection*{Improper Integrals (Integrali Impropri)}


  Improper integrals extend the concept of the Riemann integral to cases where the domain of integration is unbounded or the function itself is unbounded within the
  domain. These are evaluated by taking a limit of a definite integral as an endpoint of the integration interval approaches a specific real number or infinity.


  \subsubsection*{Type 1: Unbounded Domains (Domini Illimitati)}


  This type of improper integral applies when the interval of integration extends to infinity. For a function \(f\) that is Riemann integrable on every interval \([a, b]\)
   for \(b > a\), the integral over \([a, \infty)\) is defined as the limit:
  \[
  \int_a^\infty f(x) \,dx := \lim_{b \to \infty} \int_a^b f(x) \,dx
  \]
  The improper integral is said to converge if this limit exists and is finite. A similar definition applies for intervals of the form \((-\infty, b]\) or \((-\infty,
  \infty)\).

  \subsubsection*{Type 2: Unbounded Functions (Funzioni Illimitate)}


  This type addresses functions that have a vertical asymptote, or "pole," at some point within the integration interval. If a function \(f\) is unbounded at the endpoint
  \(a\) of the interval \((a, b]\) but is Riemann integrable on every subinterval \([a+\epsilon, b]\) for \(\epsilon > 0\), the integral is defined as:
  \[
  \int_a^b f(x) \,dx := \lim_{\epsilon \to 0^+} \int_{a+\epsilon}^b f(x) \,dx
  \]
  If the limit exists and is finite, the integral converges. A corresponding definition holds if the function is unbounded at the upper endpoint \(b\).


  \subsubsection*{Comparison Test (Criterio del Confronto)}

  The convergence of an improper integral can often be determined by comparing it to an integral whose convergence properties are known. Let \(f\) and \(g\) be two
  functions integrable on any closed subinterval of \([a, \infty)\).


  If \(0 \le f(x) \le g(x)\) for all \(x \ge a\), then the convergence of the integral of the majorant function \(g\) implies the convergence of the integral of \(f\).
  \[
  \int_a^\infty g(x) \,dx < \infty \implies \int_a^\infty f(x) \,dx < \infty
  \]
  Conversely, the divergence of the integral of the minorant function \(f\) implies the divergence of the integral of \(g\).

  \subsubsection*{Integral Test for Series (Criterio dell'Integrale per le Serie)}


  This test provides a direct link between the convergence of an infinite series and an improper integral. For a function \(f\) that is continuous, positive, and
  monotonically decreasing on the interval \([N, \infty)\), the series \(\sum_{n=N}^{\infty} f(n)\) converges if and only if the improper integral \(\int_N^\infty f(x)
  \,dx\) converges.
  \[
  \sum_{n=N}^{\infty} f(n) < \infty \iff \int_N^\infty f(x) \,dx < \infty
  \]
  This relationship allows the use of integration techniques to analyze the convergence of series.

  \subsubsection*{Cauchy Principal Value (Valore Principale di Cauchy)}


  When a function \(f\) has a discontinuity at a point \(p\) inside the interval \([a, b]\), the integral is typically split into two parts. The integral converges only if
   both limits exist independently:
  \[
  \int_a^b f(x) \,dx = \lim_{\epsilon_1 \to 0^+} \int_a^{p-\epsilon_1} f(x) \,dx + \lim_{\epsilon_2 \to 0^+} \int_{p+\epsilon_2}^b f(x) \,dx
  \]
  However, a weaker form of convergence can be defined using a symmetric limit. The Cauchy Principal Value is defined as:
  \[
  \text{P.V.} \int_a^b f(x) \,dx := \lim_{\epsilon \to 0^+} \left( \int_a^{p-\epsilon} f(x) \,dx + \int_{p+\epsilon}^b f(x) \,dx \right)
  \]
  This value may exist even when the individual limits do not, providing a method for assigning a value to certain divergent integrals.


\subsubsection*{BUFFER ZONE FROM OLD NOTES}

\newpage{}

\subsubsection*{Improper Integral Convergence Criteria}
For $\int_a^\infty f(x)  dx$ or $\int_a^b f(x)  dx$ with singularity at $b$:
\begin{center}
\begin{tabular}{p{0.3\textwidth}p{0.65\textwidth}}
\textbf{Type} & \textbf{Condition} \\
\hline
$\int_1^\infty x^{-p}  dx$ & Converges iff $p > 1$ \\
$\int_0^1 x^{-p}  dx$ & Converges iff $p < 1$ \\
$\int_a^b |x-c|^{-p}  dx$ & Converges iff $p < 1$ \\
Oscillatory $\int_1^\infty g(x)  dx$ & Converges if $\int_1^\infty |g(x)|  dx < \infty$ (absolute conv.) \\
& or by Dirichlet test (bounded antiderivative, monotone $\to 0$)
\end{tabular}
\end{center}

\subsubsection*{Limit Comparison Test Framework}
Given $f(x) \sim g(x)$ as $x \to c$ (singularity or $\infty$):
\begin{enumerate}
\item Compute $L = \lim_{x \to c} \frac{f(x)}{g(x)}$
\item If $0 < L < \infty$, then $\int f$ and $\int g$ converge/diverge together
\item For $L=0$: $\int g < \infty \Rightarrow \int f < \infty$
\item For $L=\infty$: $\int g = \infty \Rightarrow \int f = \infty$
\end{enumerate}

\subsubsection*{Singularity Analysis Toolkit}
Near singular point $c$:
\begin{align*}
|\sin x| &\sim |x - k\pi| \quad \text{near } x = k\pi \\
|\ln x| &\sim |x - 1| \quad \text{near } x = 1 \\
\sqrt{x^2 + a} - x &\sim \frac{a}{2x} \quad \text{as } x \to \infty \\
e^x - 1 - x &\sim \frac{x^2}{2} \quad \text{near } x = 0
\end{align*}

\subsubsection*{Parameter-Dependent Integrals Strategy}
For $\int_a^b h(x,\alpha,\beta)  dx$ with $\alpha,\beta > 0$:
\begin{center}
\begin{tabular}{p{0.4\textwidth}p{0.55\textwidth}}
\textbf{Problem Type} & \textbf{Solution Approach} \\
\hline
$\int_0^1 x^{-\alpha}|\ln x|^\beta  dx$ & 1. Near 0: compare to $x^{-\alpha + \epsilon}$ \\
& 2. Near 1: compare to $|x-1|^\beta$ \\
& \fbox{Converges iff $\alpha < 1$} \\
\hline
$\int_0^\infty \frac{dx}{x^\alpha |\sin x|^\beta}$ & 1. Near $k\pi$: $|\sin x|^{-\beta} \sim |x-k\pi|^{-\beta}$ \\
& 2. At $\infty$: periodic singularities $\Rightarrow$ sum test \\
& \fbox{Finite interval: $\beta < 1$} \\
& \fbox{Infinite interval: $\beta < 1$ and $\alpha > 1$}
\end{tabular}
\end{center}

\subsubsection*{Advanced Convergence Techniques}
\begin{enumerate}
\item \textbf{Integration by Parts:} For $\int f'g$, requires $\lim_{x \to c} f(x)g(x)$ exists and $\int f g' < \infty$
\item \textbf{Series Comparison:} For $\int_1^\infty$, decompose into $\sum \int_{n}^{n+1}$ and compare to series
\item \textbf{Exponential Domination:} $x^p e^{-ax} \to 0$ for all $p$ when $a > 0$
\end{enumerate}

\subsubsection*{Parameter Limit Computations}
For $F(a) = \int_0^\infty g(x,a)  dx$:
\begin{enumerate}
\item Justify limit-interchange via Dominated Convergence or uniform bounds
\item For $F_a(x) = \int_0^{x^a} h(t)  dt$:
\begin{align*}
\lim_{x \to 0^+} F_a(x) &= \begin{cases} 
0 & a > 0 \\
\int_0^\infty h(t)  dt & a < 0 
\end{cases} \\
F_a'(0) &= \begin{cases} 
0 & a > 1 \\
\text{DNE} & a \leq 1 
\end{cases}
\end{align*}
\end{enumerate}

\subsubsection*{Theoretical Result Template}
Given $f \in C^1((0,1])$ with $\int_0^1 \sqrt{x} |f'(x)|  dx < \infty$:
\begin{enumerate}
\item $\sqrt{\epsilon} f(\epsilon) \to 0$ as $\epsilon \to 0^+$ by:
\[
\left| \sqrt{\epsilon} f(\epsilon) \right| \leq \sqrt{\epsilon} |f(1)| + \int_\epsilon^1 \sqrt{t} |f'(t)| \frac{\sqrt{\epsilon}}{\sqrt{t}}  dt
\]
\item $\int_0^1 \frac{f(x)}{\sqrt{x}}  dx < \infty$ via integration by parts:
\[
\int_\epsilon^1 \frac{f(x)}{\sqrt{x}}  dx = \left[2\sqrt{x}f(x)\right]_\epsilon^1 - 2\int_\epsilon^1 \sqrt{x} f'(x)  dx
\]
\end{enumerate}

\pagebreak{}


\section*{Differential Equations}

\textquotedbl Would you tell me, please, which way I ought to go
from here?\textquotedbl{}

\textquotedbl That depends a good deal on where you want to get to,\textquotedbl{}
said the Cat.

\subsubsection*{ODEs and PDEs}

Ordinary Differential Equations (ODEs) are a differential equation
which has a single variable. ODEs have a general form:

\begin{equation}
F\left(x,y,\frac{dy}{dx},\frac{d^{2}y}{dx^{2}},...,\frac{d^{n}y}{dx^{n}}\right)=0
\end{equation}

where 

- x independent

- y dependent

Partial Differential Equations (PDEs) are a Differential equation
which has multiple independent variables. Instead of using the standard
d, they use partial derivatives (\ensuremath{\partial}) to show the
change with respect for multiple variables. The general form is:

\begin{equation}
F\left(x,y,u,\frac{\partial u}{\partial x},\frac{\partial u}{\partial y},\frac{\partial^{2}u}{\partial x^{2}},....\right)=0
\end{equation}

where

- x,y independent

- u(x,y) dependent

Fundamentally image a ODE as a means to track a single car, while
PDE track all the traffic in the city.

\subsubsection*{Types of ODEs}

ODEs are usually classified by 2 primary things. their order, aka
the degree of their derivative, and by whether they are linear or
non-linear. 

\paragraph*{First order ODEs}

are pretty self explanatory, they involve only the 1st derivative.
Here is a basic first order ODE:

\paragraph*{
\begin{equation}
\frac{dy}{dx}+y=x
\end{equation}
Second order ODEs}

involve UP to the 2nd derivative. Here is a example:

\paragraph*{
\begin{equation}
\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}+y=0
\end{equation}
Higher order ODEs}

involve everything 3rd derivative or higher. It is unlikely to ever
appear in a 1st year analysis exam, but you never know. 

\paragraph*{Liniar and Non-liniear ODEs.}

An ODE is linear if the dependent variable and the derivatives are
in a linear form. Basically: they are not multiplied together. Anything
else is considered non-liniear. A linear ODE can be written in the
form:

\begin{equation}
a_{n}(x)\frac{d^{n}y}{dx^{n}}+a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}}+...+a_{1}(x)\frac{dy}{dx}+a_{0}(x)y=f(x)
\end{equation}

- a(x) is a function of x

Here are some basic examples. We will go in much more detail when
solving ODEs.

$\frac{dy}{dx}+3y=x$, and $\frac{d^{2}y}{dx^{2}}+x\frac{dy}{dx}+y=sinx$

A non-linear ODE is any ordinary differential equation that cannot
be written in the linear form shown earlier. This is because the dependent
variable or its derivatives are not linear. I will cover this more
later on in the chapter.

\subsubsection*{The weird classifications of ODEs}

\paragraph{A Homogeneous ODE,}

is a differential equation where L is a linear differential operator.

\paragraph{
\[
L[y]=0
\]
 A Non-Homogeneous ODE,}

is a differential equation where f(x) \ensuremath{\neq} 0

\paragraph{
\[
L[y]=f(x)
\]
 A Autonomous ODE,}

is a differential equation where the independent variable (usually
x or t) does not appear in the equation.

\paragraph*{
\[
\frac{d^{n}y}{dx^{n}}=F\left(y,y',...,y^{(n-1)}\right)
\]
A Non-Autonomous ODE,}

is a differential equation if the independent variable appears

{*}You can use multiple types at once, just use common sense to make
sure its right

\subsection*{Dealing with ODEs}

\paragraph*{Separable ODE,}

can be written as 
\[
\frac{dy}{dx}=f(x)g(y)
\]
 Divide both sides by g(y), and multiply both sides by dx
\[
\frac{dy}{g(y)}=f(x)dx
\]
 Integrate both sides, make sure to keep the constant on the RHS
\[
\int\frac{dy}{g(y)}=\int f(x)dx
\]


\paragraph*{Integrating Method.}

Format the equation to fit the following before using the method
\[
\frac{dy}{dx}+P(x)y=Q(x)
\]
 Find the function $\mu(x)$ that will help simplify the problem.
(Symplify as much as possible here it will help a lot later on)
\[
\mu(x)=e^{\int P(x)dx}
\]
Multiply every term by the function $\mu(x)$ and using the product
rule calculate the derivative of the LHS
\[
\frac{d}{dx}(\mu(x)y)=\mu(x)Q(x)
\]
 Integrate both sides, remember the constant!
\[
y=\frac{1}{\mu(x)}\int\mu(x)Q(x)dx+C
\]

Quick note on integrating factor. A integrating factor is the method
used to solve derivative equation above. The \textgreek{μ}(x) also
called the integrating factor, works for any, first-order linear differential
equations. A function is derived by multiplying the equation with
\textgreek{μ}(x), which makes the left-hand side a derivative of \textgreek{μ}(x)y.

\paragraph*{Exact Method}

If a DE is exact, which can be found if it is in this form

\[
M(x,y)dx+N(x,y)dy=0
\]
 After this, calculate the partial derivative of M in respect to y
and the partial derivative of N with respect to x. If these are equivalent
the DE is exact.

\[
\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}
\]

Lets make a function that we will call $\Psi$ such that, $\varPsi_{x}=M(x,y)$
and $\Psi_{y}=N(x,y)$

Therefore we can write this now as

\[
\Psi_{x}+\Psi_{y}\frac{dy}{dx}=0
\]
we can start to find this function $\Psi$. So we will start to integrate
M with respect to x. (h(y) is a function of y)

\[
\Psi(x,y)=\int Mdx+h(y)
\]
 We can now differentiate $\Psi$ with respect to y

\[
\frac{\partial\Psi}{\partial y}=\frac{\partial}{\partial y}\left(\int Mdx\right)+h'(y)=N
\]
 Solve the integral for h(y)

\subsection*{Types of Problems}

\subsubsection*{Initial Value Problem}

Generally, IVPs are a DE and a \uline{initial condition} or condition's
which when used in unison they can be used to solve a function, that
will also fit the DE. The steps are pretty straight forward. 

1. Solve the DE

\[
y(x)=\int f(x)dx+C
\]

2. Use the initial condition, lets say that $y(x_{0})=y_{0}$

\[
y_{0}=\int f(x_{0})dx+C
\]

Where C is:

\[
C=y_{0}-V
\]
 {*}V is the value of the integral at $x_{0}$, therefore if we replace
C, the final answer is

\paragraph*{
\[
y(x)=\protect\int f(x)dx+(y_{0}-V)
\]
}

\paragraph*{Proving existence and uniqueness}

\subparagraph*{Theorem Definition:}

If f(x,y) and the partial derivative $\frac{\partial f}{\partial y}$
are continuous in:

\[
D=\left\{ (x,y)||x-x_{0}|\leq a,|y-y_{0}|\leq b\right\} 
\]
around the point $(x_{0},y_{0})$ therefore, there exits a interval
between x and $x_{0}$where there is at least one solution of y(x).
And it proves that this solution is unique on the interval.

\subparagraph*{Steps:}

Write the ODE in standard form, aka:

\[
\frac{dy}{dx}=f(x,y)
\]

use intuition to check that the function f is continuous between the
points you want. Then if there are any points where the function is
non continuous, then be sure to mark it such that it is clear, using
$\leq\geq><$. This is the first rule.

Now, do a partial derivative of y such that:

\[
\frac{\partial f}{\partial y}=f(x,y)
\]

Remember to treat x as a constant in this case!!!

If the result is continuous in D between x and $x_{0}$ then this
proves that there is at least a solution. 

\subsection*{Interpreting answers}

\paragraph{Interval of validity for Linear DE.}

The interval of validity for a Linear DE is largest around $x_{0}$where
$p(x)$ and $q(x)$ are continuous. Make sure to exclude discontinuities.

\paragraph{Interval of validity for Non-Linear DE,}

Solutions may be exponential to infinity or become undefined despite
$f(x,y)$ being smooth. Therefore check all the points where the solution
becomes undefined. Extend the interval on both sides of $x_{0}$ till
it is no longer possible.

\subsubsection*{Interpreting answers}

(this whole section is a bit useless, I may remove it if I don't find
any use for it son)

\paragraph*{Explicit Solution,}

is when the dependent variable is isolated and in terms of the independent
variable. e.g

\[
y=x^{2}+C
\]

(soluzione esplicita)

\paragraph*{Implicit Solution,}

is when the dependent variable is not explicitly isolated from the
independent. e.g

\[
x^{2}+y^{2}=C
\]

(soluzione implicita)

\paragraph{General Solution,}

is the solution containing all the possible solutions for the differential
equation, ie it keeps the constants, its the trivial form. (soluzione
generale)

\paragraph*{Particular Solution,}

is the solution which is a specific solution by locking the constants
by using the initial conditions, ie the C has a fixed value. (soluzione
particolare)

\paragraph*{Equilibrium Solution,}

is a solution which is constant because the dependent variable does
not change and therefore the derivative is zero. (soluzione di equilibrio)

\paragraph*{Parametric Solution,}

is a solution represented using a parameter like (t,u,z..) instead
of using x and y. eg.

\[
y(t)=\sqrt{t^{2}+C}
\]

(soluzione parametrica)

Homogeneous Equations with constant coefficients have the general
form:
\[
y''+ay'+by=0
\]
 Non-homogeneous Equations: Method of Undetermined Coefficients
\[
Finished
\]

\subsubsection*{Challenges to Uniqueness and Global Existence (Esistenza e Unicità)}

Standard assumptions like continuity of the vector field \(V\) are not sufficient to guarantee that a solution is unique or that it exists for all time.

\paragraph*{Finite-Time Blow-up}

A solution to an IVP may not be defined for all time \(t \in \mathbb{R}\). It is possible for a solution to approach infinity in a finite amount of time, a phenomenon known as blow-up. This restricts the maximal domain of definition of the solution to a finite interval.

\paragraph*{Non-Uniqueness of Solutions (Non Unicità delle Soluzioni)}

For a given IVP, it is possible for more than one distinct solution to exist. This typically occurs when the vector field \(V\) is not sufficiently regular at certain points. Geometrically, this corresponds to having multiple solution curves (orbits) passing through the same point in the state space.

\subsubsection*{Lipschitz Continuity (Continuità di Lipschitz)}

To ensure the uniqueness of solutions, a stronger condition than mere continuity is required for the vector field \(V\). This condition is known as Lipschitz continuity, which serves as an intermediate property between continuity and continuous differentiability. The hierarchy is as follows: a continuously differentiable function is locally Lipschitz continuous, which in turn is continuous.

\paragraph*{Local Lipschitz Continuity (Continuità di Lipschitz Locale)}

A function \(V: U \subseteq \mathbb{R}^n \to \mathbb{R}^n\) is called locally Lipschitz continuous if for every point \(x \in U\), there exists a neighborhood and a constant \(L > 0\), known as the Lipschitz constant (costante di Lipschitz), such that for any two points \(y, z\) in this neighborhood, the following inequality holds:
\[
\|V(y) - V(z)\| \le L \|y - z\|
\]

\paragraph*{Properties of Lipschitz Functions (Proprietà delle Funzioni di Lipschitz)}

A direct consequence of the definition is that any locally Lipschitz continuous function is also continuous. The condition fundamentally imposes a bound on the rate of change of the function; the magnitude of the slopes of secant lines connecting any two nearby points is bounded by the Lipschitz constant \(L\).

\paragraph*{Relation to Differentiability (Relazione con la Derivabilità)}

Any continuously differentiable function (a \(C^1\) function) is locally Lipschitz continuous. This can be shown using the Mean Value Theorem, which relates the difference \(V(y) - V(z)\) to the derivative of \(V\) at an intermediate point. The continuity of the derivative ensures it is bounded on any compact neighborhood, providing a valid Lipschitz constant.

\subsubsection*{The Existence and Uniqueness Theorem (Teorema di Esistenza e Unicità)}

The Picard–Lindelöf theorem states that if the vector field \(V\) is locally Lipschitz continuous, then for any initial condition \(x(t_0) = x_0\), there exists a unique solution to the initial value problem on some time interval containing \(t_0\). This condition on \(V\) is sufficient to prevent issues like non-uniqueness and ensures well-behaved local solutions.

    \subsubsection*{Integral Formulation and Fixed-Point Theory}
    
     The differential equation of an IVP can be transformed into an integral equation. A function \(x(t)\) is a solution to the IVP \(x' = V(x)\) with \(x(t_0) =
      x_0\) if and only if it satisfies:
    \[
    x(t) = x_0 + \int_{t_0}^{t} V(x(s)) \,ds
     \]
    This recasts the problem of finding a solution as finding a fixed point of an integral operator \(\Phi\), where \(\Phi(x) = x\).
     
     \paragraph*{The Banach Fixed-Point Theorem (Teorema del Punto Fisso di Banach)}
    
    This theorem provides a general condition for the existence and uniqueness of fixed points. It states that if \((X, d)\) is a non-empty complete metric space
      and \(\Phi: X \to X\) is a \textbf{contraction mapping (contrazione)}—that is, for some constant \(q \in [0, 1)\), \(d(\Phi(x), \Phi(y)) \le q \cdot d(x, y)\)
      for all \(x, y \in X\)—then \(\Phi\) has a unique fixed point in \(X\).
    
    \paragraph*{Application to ODEs: The Picard-Lindelöf Theorem}
    
    The existence of a solution can be proven by applying the Banach Fixed-Point Theorem. The setup involves:
    \begin{itemize}
        \item \textbf{Metric Space:} The space of continuous, bounded functions on a small interval \([- \epsilon, \epsilon]\) around \(t_0\) that satisfy the
      initial condition. The metric is the supremum norm, which makes the space complete.
        \item \textbf{Integral Operator:} The map \(\Phi(x)(t) = x_0 + \int_{t_0}^{t} V(x(s)) \,ds\).
    \end{itemize}
    If \(V\) is locally Lipschitz, it can be shown that for a sufficiently small time interval, the operator \(\Phi\) is a contraction. The Banach theorem then
      guarantees the existence of a unique fixed point, which is the unique local solution to the IVP.
    
    \paragraph*{The Picard Iteration (Iterazione di Picard)}
    
    The proof of the Banach Fixed-Point Theorem is constructive and provides an iterative method to find the solution, known as the Picard iteration. Starting with
      an initial guess \(\alpha_0(t)\) (often the constant function \(\alpha_0(t) = x_0\)), one generates a sequence of functions:
    \[
    \alpha_{n+1}(t) = \Phi(\alpha_n)(t) = x_0 + \int_{t_0}^{t} V(\alpha_n(s)) \,ds
    \]
    The theorem guarantees that this sequence converges uniformly to the unique solution of the IVP on the local interval.
    
    \subsubsection*{Maximal and Global Solutions}
    
    A local solution is defined on a small time interval around the initial time \(t_0\). This solution can often be extended to a larger domain.
    
    \paragraph*{Maximal Solutions (Soluzioni Massimali)}
    
    A solution is called a \textbf{maximal solution (soluzione massimale)} if it cannot be extended to any larger time interval. For any IVP with a locally
      Lipschitz continuous vector field, there exists a unique maximal solution defined on a \textbf{maximal interval of existence (intervallo massimale di
      esistenza)}, which is always an open interval. This uniqueness is proven by showing that any two solutions to the same IVP must agree on the intersection of
      their domains, allowing them to be "glued" together.
    
    \paragraph*{Global Solutions (Soluzioni Globali)}
    
    A maximal solution is called a \textbf{global solution (soluzione globale)} if its maximal interval of existence is the entire real line, \(\mathbb{R}\).
      Global existence is not guaranteed in general, as solutions can exhibit finite-time blow-up.
    
    \subsubsection*{The Phase Portrait}
    
   The geometric representation of all solutions of a dynamical system provides qualitative insight into its behavior.
    
    \paragraph*{Orbits and the Phase Portrait (Orbite e Ritratto di Fase)}
   
    An \textbf{orbit (orbita)} is the set of points in the state space traced by a maximal solution. The collection of all orbits for a given system constitutes
      its \textbf{phase portrait (ritratto di fase)}. For a system with a locally Lipschitz continuous vector field, the orbits form a partition of the state space;
      that is, for every point in the state space, there is exactly one orbit passing through it. Consequently, orbits cannot cross.
    
    \paragraph*{Special Orbits}
   
    \begin{itemize}
        \item \textbf{Fixed Points (Punti Fissi):} A point \(x_0\) where \(V(x_0) = 0\). This corresponds to a constant solution, and its orbit is a single point.
       \item \textbf{Periodic Orbits (Orbite Periodiche):} An orbit corresponding to a non-constant solution \(\alpha(t)\) that returns to its starting point
      after some time \(T > 0\), i.e., \(\alpha(t+T) = \alpha(t)\). The orbit is a closed curve.
    \end{itemize}
   
    \subsubsection*{Non-Autonomous and Linear Systems}
    
    The theory can be extended to more general classes of differential equations.
   
   \paragraph*{Non-Autonomous Systems (Sistemi Non Autonomi)}
   
    For a non-autonomous system \(x' = W(t, x)\), the Picard-Lindelöf theorem holds if \(W\) is continuous and locally Lipschitz continuous with respect to its
      second argument, \(x\). A stronger, global Lipschitz condition on \(x\) (uniformly on compact time intervals) is sufficient to guarantee the existence of a
      unique global solution.
    
   \paragraph*{Systems of Linear ODEs (Sistemi di EDO Lineari)}
    
    A system is linear if it can be written as \(x' = A(t)x + b(t)\), where \(A(t)\) and \(b(t)\) are continuous matrix and vector-valued functions, respectively.
    \begin{itemize}
       \item \textbf{Homogeneous (Omogeneo):} The system is homogeneous if \(b(t) = 0\).
       \item \textbf{Autonomous (Autonomo):} The system is autonomous if \(A(t)\) and \(b(t)\) are constant.
   \end{itemize}
   Any linear system satisfies a global Lipschitz condition with respect to \(x\) on any compact time interval. As a result, an IVP for a linear system always has
      a unique solution defined on the entire interval where \(A(t)\) and \(b(t)\) are continuous.
    
   \subsubsection*{Structure of Solutions for Linear Systems}
    
   Linear systems possess a highly structured solution set due to the properties of linearity.
   
   \paragraph*{The Homogeneous Case and the Superposition Principle}
   
   For the homogeneous linear system \(x' = A(t)x\), the principle of superposition holds: any linear combination of solutions is also a solution. This implies 
      that the set of all solutions to the homogeneous system, denoted \(S_0\), forms a vector space.
   
   \paragraph*{Dimension of the Solution Space (Dimensione dello Spazio delle Soluzioni)}
   
 A fundamental result states that for an n-dimensional homogeneous linear system, the \textbf{solution space (spazio delle soluzioni)} \(S_0\) is an 
      n-dimensional real vector space. This is proven by constructing a linear map \(L: S_0 \to \mathbb{R}^n\) given by \(L(\alpha) = \alpha(t_0)\), which evaluates
      a solution at a fixed time \(t_0\). The existence and uniqueness theorem guarantees that this map is an isomorphism, meaning the dimension of \(S_0\) is the
      same as the dimension of \(\mathbb{R}^n\), which is \(n\). This result is crucial, as it implies that the general solution can be constructed from a basis of
      just \(n\) linearly independent solutions.

\subsection*{Nth order Differential Equation}

\subsection*{Systems of Differential Equations}

\subsection*{Special theorems and Problems}

\subsubsection*{Picard--Lindelöf theorem}

\section*{\sffamily Function Analysis}

These questions test your ability to analyze specific function properties, often without requiring a full graphical study.

\subsection*{\sffamily Strategy for Max/Min}
\begin{enumerate}
    \item \textbf{Identify Domain:} Determine the function's domain. Pay close attention to the boundaries.
    \item \textbf{First Derivative:} Calculate the first derivative, $f'(x)$.
    \item \textbf{Critical Points:} Find the critical points by solving $f'(x) = 0$ and identifying where $f'(x)$ is undefined.
    \item \textbf{Monotonicity:} Study the sign of $f'(x)$ to find intervals where the function is increasing or decreasing.
    \item \textbf{Evaluate:} Calculate the value of $f(x)$ at the critical points and at the boundaries of the domain.
    \item \textbf{Conclude:} The largest value found is the absolute maximum, and the smallest is the absolute minimum.
\end{enumerate}
\textbf{Common Pitfall:} Forgetting to evaluate the function at the domain's boundaries.

\subsection*{\sffamily Strategy for Inequalities (e.g., prove $f(x) > g(x)$)}
\begin{enumerate}
    \item \textbf{Define Auxiliary Function:} Create a new function $h(x) = f(x) - g(x)$. The goal is now to prove $h(x) > 0$.
    \item \textbf{Find Minimum:} Analyze the sign of $h'(x)$ to find the absolute minimum of $h(x)$ on the given interval.
    \item \textbf{Check Minimum's Value:} If the minimum value, $h_{min}$, is greater than 0, the inequality is proven.
    \item \textbf{Tangent Point Case:} Often, the minimum is a point $x_0$ where $h(x_0) = 0$. In this case, you must show that $x_0$ is a minimum (e.g., by checking that $h'(x_0)=0$ and $h''(x_0)>0$, or by studying the sign of $h'(x)$ around $x_0$). This proves that $h(x) \ge 0$, and is only 0 at that single point.
\end{enumerate}

\subsection*{\sffamily Strategy for Number of Solutions to $f(x) = k$}
\begin{enumerate}
    \item \textbf{Graphical Mindset:} This is a graphical problem. You are looking for the number of intersections between the graph of $y=f(x)$ and the horizontal line $y=k$.
    \item \textbf{Analyze Variation:}
        \begin{itemize}
            \item Calculate $f'(x)$ and study its sign to determine monotonicity.
            \item Find all local maximum and minimum values.
            \item Calculate the limits of $f(x)$ at the boundaries of its domain (e.g., at $\pm\infty$).
        \end{itemize}
    \item \textbf{Conclude:} Based on the values of the local extrema and the limits, you can determine how many times the line $y=k$ intersects the graph for different ranges of $k$. For example, if a local max has value $M$ and a local min has value $m$, for any $k$ such that $m < k < M$, there might be multiple solutions.
\end{enumerate}

\section{\sffamily Convergence of Series $\sum a_n$}

\begin{enumerate}
    \item \textbf{Necessary Condition:} Calculate $\lim_{n \to \infty} a_n$. If the limit is NOT 0, the series \textbf{diverges}.
    \item \textbf{Check for Positive Terms ($a_n \ge 0$):}
        \begin{itemize}
            \item \textbf{Asymptotic Comparison Test (Most Common):} Find a simpler series $b_n$ (typically $1/n^p$) and evaluate $L = \lim_{n \to \infty} \frac{a_n}{b_n}$. If $L$ is a finite, non-zero number, then $\sum a_n$ behaves exactly like $\sum b_n$.
            \item \textit{How to find $b_n$?} Use Taylor expansions for terms like $\sin(1/n), \log(1+1/n), e^{1/n}-1$, etc., around 0.
            \item \textbf{Ratio Test:} Best for factorials and powers. If $\lim_{n \to \infty} \frac{a_{n+1}}{a_n} < 1$, it converges. If $> 1$, it diverges. If $= 1$, it's inconclusive.
        \end{itemize}
    \item \textbf{Check for Alternating Terms ($\sum (-1)^n a_n$):}
        \begin{itemize}
            \item \textbf{Leibniz Test:} If $a_n > 0$, $a_n$ is decreasing, and $\lim_{n \to \infty} a_n = 0$, the series converges.
            \item \textbf{Absolute Convergence:} Check if $\sum |a_n|$ converges (using the tests for positive series). If it does, the original series converges absolutely.
        \end{itemize}
\end{enumerate}

\section{\sffamily Improper Integrals $\int_a^b f(x) dx$}

\begin{enumerate}
    \item \textbf{Identify Problem Points:} Find the points where the integral is improper. This occurs if a bound is $\pm\infty$ or if $f(x)$ is unbounded at some point $c \in [a,b]$.
    \item \textbf{Analyze Each Point Separately:}
        \begin{itemize}
            \item \textbf{At $\infty$:} Use asymptotic comparison with $g(x) = 1/x^p$. The integral $\int_a^\infty f(x)dx$ converges if the integral of $g(x)$ converges, which happens for $p > 1$.
            \item \textbf{At a point $c$:} Use asymptotic comparison with $g(x) = 1/|x-c|^p$. The integral converges if $p < 1$.
        \end{itemize}
    \item \textbf{Key Tool:} To find the asymptotic behavior, use Taylor expansions. For a problem at $x=c$, expand $f(x)$ for $x \to c$.
\end{enumerate}

\section{\sffamily Limits}

\begin{enumerate}
    \item \textbf{Indeterminate Form:} First, try direct substitution. If you get an indeterminate form ($0/0, \infty/\infty, 1^\infty$, etc.), proceed.
    \item \textbf{Taylor Expansions (for $x \to 0$):} This is the most reliable method.
        \begin{itemize}
            \item Replace functions with their expansions: $e^x \approx 1+x+\frac{x^2}{2!}$, $\sin x \approx x-\frac{x^3}{3!}$, $\cos x \approx 1-\frac{x^2}{2!}$, $\log(1+x) \approx x-\frac{x^2}{2}$.
            \item Keep enough terms to ensure the lowest-order terms in the numerator and denominator do not cancel to zero.
        \end{itemize}
    \item \textbf{For $1^\infty$ forms:} Rewrite $f(x)^{g(x)}$ as $e^{g(x) \ln(f(x))}$. Calculate the limit of the exponent, $L = \lim g(x) \ln(f(x))$. The final result is $e^L$. This often simplifies by using $\ln(1+t) \sim t$ for $t \to 0$.
\end{enumerate}

\section{\sffamily Sequences by Recurrence ($a_{n+1} = f(a_n)$)}

\begin{enumerate}
    \item \textbf{Find Fixed Points:} These are the candidates for the limit. Solve the equation $f(x) = x$.
    \item \textbf{Study Monotonicity:}
        \begin{itemize}
            \item Analyze the sign of $f(x) - x$.
            \item If $f(x) > x$, the sequence is increasing ($a_{n+1} > a_n$).
            \item If $f(x) < x$, the sequence is decreasing ($a_{n+1} < a_n$).
        \end{itemize}
    \item \textbf{Check for Boundedness:}
        \begin{itemize}
            \item Find an interval $I$ such that if $a_n \in I$, then $a_{n+1} \in I$. This is called an invariant interval.
            \item If the initial term $a_0$ is in $I$, all subsequent terms will be as well, proving the sequence is bounded.
        \end{itemize}
    \item \textbf{Conclude:} A monotonic and bounded sequence always converges to a limit $L$. This limit $L$ must be one of the fixed points found in step 1. Use the bounds and monotonicity to select the correct one.
\end{enumerate}
\section{\sffamily Differential Equations: A Detailed Guide}

This section provides a step-by-step guide for the types of differential equations that appear in the exams. The key is to first identify the type of the equation and then apply the corresponding standard method.

\subsection*{\sffamily Type 1: First-Order Separable Equations}
\begin{itemize}
    \item \textbf{How to Identify:} The equation can be written in the form $\frac{dy}{dx} = f(x) \cdot g(y)$. You can algebraically separate all $x$ terms from all $y$ terms.
    \item \textbf{Step-by-Step Solution:}
    \begin{enumerate}
        \item Rewrite $y'$ as $\frac{dy}{dx}$.
        \item Separate the variables: move all $y$ terms to one side and all $x$ terms to the other. This yields: $$\frac{1}{g(y)} dy = f(x) dx$$
        \item Integrate both sides: $$\int \frac{1}{g(y)} dy = \int f(x) dx + C$$ Remember to add the constant of integration, $C$, on the side with the independent variable ($x$).
        \item Solve for $y$ explicitly, if possible.
    \end{enumerate}
    \item \textbf{Important Note on Constant Solutions:} Before separating, check if there are any values $y_c$ for which $g(y_c) = 0$. If so, then $y(x) = y_c$ is a constant (or stationary) solution. These are important and can sometimes be the specific solution to a Cauchy problem.
\end{itemize}

\subsection*{\sffamily Type 2: Second-Order Linear Homogeneous with Constant Coefficients}
\begin{itemize}
    \item \textbf{How to Identify:} The equation has the form $ay'' + by' + cy = 0$, where $a, b, c$ are real numbers.
    \item \textbf{Step-by-Step Solution:}
    \begin{enumerate}
        \item Write the \textbf{Characteristic Equation}: This is a quadratic equation formed by replacing $y''$ with $\lambda^2$, $y'$ with $\lambda$, and $y$ with 1: $$a\lambda^2 + b\lambda + c = 0$$
        \item Solve for $\lambda$. The form of the general solution depends on the roots ($\lambda_1, \lambda_2$):
        \begin{itemize}
            \item[\textbf{Case A:}] Two distinct real roots, $\lambda_1 \neq \lambda_2$. The general solution is: $$y(x) = C_1 e^{\lambda_1 x} + C_2 e^{\lambda_2 x}$$
            \item[\textbf{Case B:}] One repeated real root, $\lambda_1 = \lambda_2 = \lambda$. The general solution is: $$y(x) = C_1 e^{\lambda x} + C_2 x e^{\lambda x}$$ (Notice the extra $x$ in the second term).
            \item[\textbf{Case C:}] Two complex conjugate roots, $\lambda = \alpha \pm i\beta$. The general solution is: $$y(x) = e^{\alpha x} (C_1 \cos(\beta x) + C_2 \sin(\beta x))$$
        \end{itemize}
    \end{enumerate}
\end{itemize}

\subsection*{\sffamily Type 3: Second-Order Linear Non-Homogeneous}
\begin{itemize}
    \item \textbf{How to Identify:} The equation has the form $ay'' + by' + cy = f(x)$, where $f(x)$ is not zero.
    \item \textbf{General Principle:} The final solution is the sum of two parts: $$y(x) = y_h(x) + y_p(x)$$
    \begin{itemize}
        \item $y_h(x)$ is the solution to the corresponding \textbf{homogeneous} equation ($ay'' + by' + cy = 0$), found using the method from Type 2.
        \item $y_p(x)$ is a \textbf{particular solution} that depends on the form of $f(x)$.
    \end{itemize}
    \item \textbf{Finding the Particular Solution $y_p(x)$ (Method of Undetermined Coefficients):}
    \begin{enumerate}
        \item First, find the homogeneous solution $y_h(x)$.
        \item Make a guess for $y_p(x)$ based on the form of $f(x)$:
        \begin{itemize}
            \item If $f(x)$ is a polynomial of degree $n$, guess a polynomial of degree $n$: $y_p(x) = Ax^n + ... + D$.
            \item If $f(x) = K e^{kx}$, guess $y_p(x) = A e^{kx}$.
            \item If $f(x) = K \sin(kx)$ or $K \cos(kx)$, guess $y_p(x) = A\sin(kx) + B\cos(kx)$.
        \end{itemize}
        \item \textbf{CRITICAL - The Resonance Rule:} If your guess for $y_p(x)$ (or any part of it) is already present in the homogeneous solution $y_h(x)$, you must multiply your entire guess by $x$. If it's still present, multiply by $x$ again.
        \item Differentiate your guess to find $y_p'$ and $y_p''$.
        \item Substitute $y_p, y_p', y_p''$ into the original non-homogeneous equation.
        \item Solve for the coefficients (e.g., $A, B$) by equating the coefficients on both sides.
        \item Your final solution is $y(x) = y_h(x) + y_p(x)$.
    \end{enumerate}
\end{itemize}

\subsection*{\sffamily The Final Step for All DEs: Solving the Cauchy Problem}
\begin{itemize}
    \item \textbf{What it is:} A Cauchy Problem gives you the differential equation plus one or more initial conditions, like $y(x_0)=y_0$ or $y'(x_0)=y_1$. Your task is to find the values of the constants of integration ($C$, or $C_1$ and $C_2$) in your general solution.
    \item \textbf{The Universal Method:}
    \begin{enumerate}
        \item \textbf{Find the General Solution First:} Follow the methods for the appropriate DE type (Separable, 2nd Order Homogeneous, etc.) to find the solution with the constants still in it. Let's call it $y_{gen}(x)$.
        \item \textbf{Apply the First Condition ($y(x_0)=y_0$):}
        \begin{itemize}
            \item Take your general solution $y_{gen}(x)$.
            \item Substitute $x_0$ for every $x$ and set the entire expression equal to $y_0$.
            \item This gives you your first equation involving the constants. For a first-order equation, you can solve for $C$ directly.
        \end{itemize}
        \item \textbf{Apply the Second Condition ($y'(x_0)=y_1$) (for 2nd order DEs):}
        \begin{itemize}
            \item Take your general solution $y_{gen}(x)$ and calculate its derivative, $y_{gen}'(x)$.\
            \textbf{Important:} Differentiate first, then substitute.
            \item Now substitute $x_0$ for every $x$ in the derivative and set the expression equal to $y_1$.
            \item This gives you your second equation involving the constants.
        \end{itemize}
        \item \textbf{Solve for the Constants:} You now have a system of one or two linear equations for your constants ($C_1, C_2$). Solve this system to find their specific numerical values.
        \item \textbf{Write the Final Solution:} Substitute the values you found for the constants back into your general solution. This final expression is the unique solution to the Cauchy Problem.
    \end{enumerate}
\end{itemize}

\subsubsection*{Fin :)}
\end{document}
